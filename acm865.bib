@proceedings{10.1145/1653771,
	abstract = {These proceedings contain the papers selected for publication and presentation at the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL GIS) in Seattle, Washington, USA, from November 4th to 6th, 2009.ACM SIGSPATIAL GIS 2009 brings together researchers, developers, users, and practitioners carrying out research and development in novel systems based on geo-spatial data and knowledge. The conference fosters interdisciplinary discussions and research in all aspects of Geographic Information Systems and Science (GIS) and provides a forum for original research contributions covering all conceptual, design, and implementation aspects of GIS and ranging from applications, user interface considerations, and visualization down to storage management and indexing issues. This is the second time that the conference is being held under the auspices of the ACM Special Interest Group on Spatial Information (SIGSPATIAL). This year, we had four conference tracks, namely, regular research papers, industry papers, demonstration track, and a Ph.D. showcase track.},
	address = {New York, NY, USA},
	isbn = {9781605586496},
	location = {Seattle, Washington},
	publisher = {Association for Computing Machinery},
	title = {GIS '09: Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	year = {2009},
}

@proceedings{10.1145/2442810,
	abstract = {These proceedings contain the papers selected for presentation at the 1st ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems (MobiGIS 2012) which is held in conjunction with the 20th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (GIS 2012) on November 6, 2012 in Redondo Beach, California, USA.},
	address = {New York, NY, USA},
	isbn = {9781450316996},
	location = {Redondo Beach, California},
	publisher = {Association for Computing Machinery},
	title = {MobiGIS '12: Proceedings of the First ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
	year = {2012},
}

@article{10.1145/2803172,
	abstract = {Accumulated cost surfaces are used in a variety of fields that employ spatial analysis. Several algorithms have been suggested in the past for solving them efficiently or with minimal errors. Meanwhile, a new wave on the technological frontier has brought about general-purpose computing on GPUs. In this article, we describe how accumulated cost surfaces can be solved with CUDA. To verify the performance of our solution, we performed an experimental comparison against implementations run on a CPU. Our results with realistic cost models indicate that the move to GPUs can engender a speed-up of an order of magnitude.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Kovanen, Janne and Sarjakoski, Tapani},
	doi = {10.1145/2803172},
	issn = {2374-0353},
	issue_date = {November 2015},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {CUDA, Spatial analysis, accumulated cost surface, raster data, single-source shortest-path},
	month = {aug},
	number = {2},
	numpages = {27},
	publisher = {Association for Computing Machinery},
	title = {Tilewise Accumulated Cost Surface Computation with Graphics Processing Units},
	url = {https://doi.org/10.1145/2803172},
	volume = {1},
	year = {2015},
}

@inproceedings{10.1145/2837689.2837692,
	abstract = {We argue that Geographic Information Retrieval has a lot to gain from the creation of contextualized models of spatial relations that capture how human usage of spatial relational expressions is affected by contextual factors. We propose a framework to develop such models and discuss challenges.},
	address = {New York, NY, USA},
	articleno = {14},
	author = {Wallgr\"{u}n, Jan Oliver and Klippel, Alexander and Karimzadeh, Morteza},
	booktitle = {Proceedings of the 9th Workshop on Geographic Information Retrieval},
	doi = {10.1145/2837689.2837692},
	isbn = {9781450339377},
	keywords = {geographic information retrieval, relations, spatial language},
	location = {Paris, France},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {GIR '15},
	title = {Towards contextualized models of spatial relations},
	url = {https://doi.org/10.1145/2837689.2837692},
	year = {2015},
}

@inproceedings{10.1145/2837689.2837698,
	abstract = {Many gazetteers contain only a small amount of historic place name information and spelling variants of places. Even more focused historic gazetteers are far from being complete and often specialize on certain geographic regions or particular time periods. On the other hand, there are huge amounts of historic route descriptions, so called itineraries. They represent massive knowledge sources from which historic place names and spelling variants can be deduced. The analysis and geocoding of those route descriptions---often done by hand---is an important task in the humanities. To cope with these problems, we present preliminary ideas how to automatically deduce historic place names and thus travel routes from historic route descriptions.},
	address = {New York, NY, USA},
	articleno = {9},
	author = {Blank, Daniel and Henrich, Andreas},
	booktitle = {Proceedings of the 9th Workshop on Geographic Information Retrieval},
	doi = {10.1145/2837689.2837698},
	isbn = {9781450339377},
	keywords = {gazetteer enrichment, geocoding, itinerary resolution},
	location = {Paris, France},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {GIR '15},
	title = {Geocoding place names from historic route descriptions},
	url = {https://doi.org/10.1145/2837689.2837698},
	year = {2015},
}

@inproceedings{10.1145/2837689.2837701,
	abstract = {We investigate tagging figure and table captions in scientific articles from geology to support visualization of research findings on maps and time-lines. Our proposed approach comprises identifying geological time expressions and geographic and geologic locations without requiring large pre-annotated data. Different tagging approaches are tested and evaluated on a corpus of captions extracted from scientific geological articles. Our baseline method builds on geologic timescale ontologies and GeoNames as gazetteers to facilitate lookup of times and location names. The baseline is evaluated on a development set of captions from 20 documents and the results are analyzed manually to identify causes for tagging errors. We found that the poor performance of the baseline approach is mainly due to i) lack of coverage in the gazetteers, ii) incorrect tagging of person names as location names, and iii) a simplistic gazetteer lookup for capitalized words. We augmented the baseline approach by extending the gazetteers, by adding reference identification to block person names being tagged as locations, by filtering trivial matches, and by augmenting the lookup by correcting capitalization using true casing of words. The different configurations of our extended approach were evaluated on a test set of 80 documents, achieving an improved precision and recall of more than 90\%.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Leveling, Johannes},
	booktitle = {Proceedings of the 9th Workshop on Geographic Information Retrieval},
	doi = {10.1145/2837689.2837701},
	isbn = {9781450339377},
	keywords = {geo-tagging, geo-temporal IR, temporal tagging},
	location = {Paris, France},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {GIR '15},
	title = {Tagging of temporal expressions and geological features in scientific articles},
	url = {https://doi.org/10.1145/2837689.2837701},
	year = {2015},
}

@inproceedings{10.1145/2837689.2837703,
	abstract = {In newspapers or scholar manuals, numerous texts are accompanied by maps. In these map/text couples, maps give a spatial portrayal of the text issues, thus they make the spatial issues easier to understand. TEXTOMAP aims to design the geographical window of the text, based on the notion of important toponyms according to text issues. The important toponym selection is based on indicators which may be spatial, linguistic or semantic. Examples of geographical window calculation are shown and compared with the corresponding CLAVIN geographical focus. The work is in progress and perspectives are offered.},
	address = {New York, NY, USA},
	articleno = {17},
	author = {Brun, Geoffrey and Domingu\`{e}s, Catherine and Van Damme, M.-D.},
	booktitle = {Proceedings of the 9th Workshop on Geographic Information Retrieval},
	doi = {10.1145/2837689.2837703},
	isbn = {9781450339377},
	keywords = {gazetteer, geographic tagging, geographical window, information retrieval, natural language processing},
	location = {Paris, France},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {GIR '15},
	title = {TEXTOMAP: determining geographical window for texts},
	url = {https://doi.org/10.1145/2837689.2837703},
	year = {2015},
}

@inproceedings{10.1145/2851613.2851709,
	abstract = {User trajectories contain a wealth of implicit information. The places that people visit, provide us with information about their preferences and needs. Furthermore, it provides us with information about the popularity of places, for example at which time of the year or day these places are frequently visited. The potential for behavioral analysis of trajectories is widely discussed in literature, but all of these methods need a pre-processing step: the geometric trajectory data needs to be transformed into a semantic collection or sequence of visited points-of-interest that is more suitable for data mining. Especially indoor activities in urban areas are challenging to detect from raw trajectory data. In this paper, we propose a new algorithm for the automated detection of visited points-of-interest. This algorithm extracts the actual visited points-of-interest well, both in terms of precision and recall, even for the challenging urban indoor activity detection. We demonstrate the strength of the algorithm by comparing it to three existing and widely used algorithms, using annotated trajectory data, collected through an experiment with students in the city of Hengelo, The Netherlands. Our algorithm, which combines multiple trajectory pre-processing techniques from existing work with several novel ones, shows significant improvements.},
	address = {New York, NY, USA},
	author = {de Graaff, Victor and de By, Rolf A. and van Keulen, Maurice},
	booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
	doi = {10.1145/2851613.2851709},
	isbn = {9781450337397},
	keywords = {GPS, algorithm, point-of-interest, smartphone, trajectory analysis},
	location = {Pisa, Italy},
	numpages = {8},
	pages = {552–559},
	publisher = {Association for Computing Machinery},
	series = {SAC '16},
	title = {Automated semantic trajectory annotation with indoor point-of-interest visits in urban areas},
	url = {https://doi.org/10.1145/2851613.2851709},
	year = {2016},
}

@inproceedings{10.1145/2851613.2851712,
	abstract = {This paper proposes a new method called Virtual Run for extracting the representative trajectories from a set of similar trajectories by readjusting the sequence of movements of vehicles. This method uses convex hull and Euclidean minimum spanning tree to move the trajectory sets, so that it can generate the representative trajectories. Those representative trajectories are used to generate the digital road map which is the final goal of this research. Generally, the well-known Fr\'{e}chet distance measure is applied to estimate the similarity between a pair of trajectories. However, in this paper, we present that our new method is more effective than the previous methods on the basis of an experiment and evaluation. Also we show the application of generating the digital road map using the presented method. For the evaluation, we collected a set of actual trajectory data from 3,000 taxis in the Gangnam and Seongnam areas. The experimental results show that our approach can solve the problems associated with the special case of loops, which are not clearly solved by the previous approaches. The results also show that our method is capable of generating a digital road map from a set of trajectories.},
	address = {New York, NY, USA},
	author = {Park, Jinkwan and Kim, Taeyong and Park, Bokuk and Cho, Hwan-Gue},
	booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
	doi = {10.1145/2851613.2851712},
	isbn = {9781450337397},
	keywords = {GPS data analysis, geographic information system, road map generation, vehicle trajectory},
	location = {Pisa, Italy},
	numpages = {8},
	pages = {572–579},
	publisher = {Association for Computing Machinery},
	series = {SAC '16},
	title = {Virtual running of vehicle trajectories for automatic map generation},
	url = {https://doi.org/10.1145/2851613.2851712},
	year = {2016},
}

@proceedings{10.1145/2855680,
	address = {New York, NY, USA},
	isbn = {9781450339803},
	location = {Bellevue, Washington},
	publisher = {Association for Computing Machinery},
	title = {SIGSPATIAL PhD '15: Proceedings of the 2nd ACM SIGSPATIAL PhD Workshop},
	year = {2015},
}

@inproceedings{10.1145/2872427.2883016,
	abstract = {Maps have long played a crucial role in enabling people to conceptualize and navigate the world around them. However, maps also encode the world-views of their creators. Disputed international borders are one example of this: governments may mandate that cartographers produce maps that conform to their view of a territorial dispute. Today, online maps maintained by private corporations have become the norm. However, these new maps are still subject to old debates. Companies like Google and Bing resolve these disputes by localizing their maps to meet government requirements and user preferences, i.e., users in different locations are shown maps with different international boundaries. We argue that this non-transparent personalization of maps may exacerbate nationalistic disputes by promoting divergent views of geopolitical realities. To address this problem, we present MapWatch, our system for detecting and cataloging personalization of international borders in online maps. Our system continuously crawls all map tiles from Google and Bing maps, and leverages crowdworkers to identify border personalization. In this paper, we present the architecture of MapWatch, and analyze the instances of border personalization on Google and Bing, including one border change that MapWatch identified live, as Google was rolling out the update.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Soeller, Gary and Karahalios, Karrie and Sandvig, Christian and Wilson, Christo},
	booktitle = {Proceedings of the 25th International Conference on World Wide Web},
	doi = {10.1145/2872427.2883016},
	isbn = {9781450341431},
	keywords = {maps, measurement, personalization},
	location = {Montr\'{e}al, Qu\'{e}bec, Canada},
	numpages = {12},
	pages = {867–878},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '16},
	title = {MapWatch: Detecting and Monitoring International Border Personalization on Online Maps},
	url = {https://doi.org/10.1145/2872427.2883016},
	year = {2016},
}

@inproceedings{10.1145/2872518.2890084,
	abstract = {During the last years, researchers explored the geographic and environmental factors that affect happiness. More recently, location-sharing services provided by the social media has given an unprecedented access to geo-located data for studying the interplay between these factors on a much bigger scale. Do location-sharing services help in turn at distinguishing emotions in places within a city? Which aspects contribute better at understanding happier places? To answer these questions, we use data from Foursquare location-sharing service to identify areas within a major US metropolitan area with many check-ins, i.e., areas that people like to use. We then use data from the Twitter microblogging platform to analyze the properties of these areas. Specifically, we have extracted a large corpus of geo-tagged messages, called tweets, from a major metropolitan area and linked them US Census data through their locations. This allows us to measure the sentiment expressed in tweets that are posted from a specific area, and also use that area's demographic properties in analysis. Our results reveal that areas with many check-ins are different from other areas within the metropolitan region. In particular, these areas have happier tweets, which also encourage people living in it or from other areas to commute longer distances to these places. These findings shed light on the influence certain places play within a city regarding people's emotions and mobility, which in turn can be used for city planners for designing happier and more equitable cities.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Gallegos, Luciano and Lerman, Kristina and Huang, Arhur and Garcia, David},
	booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
	doi = {10.1145/2872518.2890084},
	isbn = {9781450341448},
	keywords = {location-sharing services, location-sharing services; mobility; sentiment analysis; social media, mobility, sentiment analysis, social media},
	location = {Montr\'{e}al, Qu\'{e}bec, Canada},
	numpages = {6},
	pages = {569–574},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '16 Companion},
	title = {Geography of Emotion: Where in a City are People Happier?},
	url = {https://doi.org/10.1145/2872518.2890084},
	year = {2016},
}

@inproceedings{10.1145/2872518.2890483,
	abstract = {There exist many popular crowdsourcing and social services (Volunteered Geographic Information (VGI)) to share information and documents such as Flickr, Foursquare, Twitter , Facebook, etc. They all use metadata, folksonomy and more importantly a geographic axis with GPS coordinates and/or geographic tags. Using this available folksonomy in VGI services we propose a logical approach to highlight and possibly discover the characteristics of geographic places. The approach is based on the notion of spatial coverage and a model of tags categorization and on their semantic identification, using semantic services such as GeoNames, OpenStreetMap or WordNet. We illustrate our model with Flickr to retrieve the characteristics (function, usage?) of places even if those places have a small number of related photos. Those found characteristics allow tag disambiguation and can be use to complete the semantic gap on places and POIs such as the function of buildings, which can exist in geographic services.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Tardy, Camille and Moccozet, Laurent and Falquet, Gilles},
	booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
	doi = {10.1145/2872518.2890483},
	isbn = {9781450341448},
	keywords = {coverage, folksonomy, geographic information, semantic, tag, vgi},
	location = {Montr\'{e}al, Qu\'{e}bec, Canada},
	numpages = {4},
	pages = {657–660},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '16 Companion},
	title = {A Simple Tags Categorization Framework Using Spatial Coverage to Discover Geospatial Semantics},
	url = {https://doi.org/10.1145/2872518.2890483},
	year = {2016},
}

@inproceedings{10.1145/2872518.2890484,
	abstract = {Geocoding is the process of converting addresses to geocoordinates. It is widely used in several fields such as public health to monitor socioeconomic inequalities for example or in Geographical Information Systems (GIS) to be able to use with its provided features. In this work, we describe a method to create an address geocoder from a free and open government street lines data source. The address geocoder transforms a street address into a location typically measured in latitude-longitude coordinates. The address geocoder is used in a search engine to relate spatial data to search results and improve accuracy.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Peterman, Michael and Benomar, Omar and Mechedou, Hacene and Bachand, Felix-Herve},
	booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
	doi = {10.1145/2872518.2890484},
	isbn = {9781450341448},
	keywords = {geocoding, local search, street profiles},
	location = {Montr\'{e}al, Qu\'{e}bec, Canada},
	numpages = {2},
	pages = {655–656},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '16 Companion},
	title = {Address Geocoding using Street Profiles for Local Search},
	url = {https://doi.org/10.1145/2872518.2890484},
	year = {2016},
}

@inproceedings{10.1145/2882903.2882962,
	abstract = {Users in many domains, including urban planning, transportation, and environmental science want to execute analytical queries over continuously updated spatial datasets. Current solutions for large-scale spatial query processing either rely on extensions to RDBMS, which entails expensive loading and indexing phases when the data changes, or distributed map/reduce frameworks, running on resource-hungry compute clusters. Both solutions struggle with the sequential bottleneck of parsing complex, hierarchical spatial data formats, which frequently dominates query execution time. Our goal is to fully exploit the parallelism offered by modern multi-core CPUs for parsing and query execution, thus providing the performance of a cluster with the resources of a single machine. We describe AT-GIS, a highly-parallel spatial query processing system that scales linearly to a large number of CPU cores. AT-GIS integrates the parsing and querying of spatial data using a new computational abstraction called associative transducers (ATs). ATs can form a single data-parallel pipeline for computation without requiring the spatial input data to be split into logically independent blocks. Using ATs, AT-GIS can execute, in parallel, spatial query operators on the raw input data in multiple formats, without any pre-processing. On a single 64-core machine, AT-GIT provides 3x the performance of an 8-node Hadoop cluster with 192 cores for containment queries, and 10x for aggregation queries.},
	address = {New York, NY, USA},
	author = {Ogden, Peter and Thomas, David and Pietzuch, Peter},
	booktitle = {Proceedings of the 2016 International Conference on Management of Data},
	doi = {10.1145/2882903.2882962},
	isbn = {9781450335317},
	keywords = {JSON, NODB, XML, multi-core CPUs, parallel automata, spatial query processing},
	location = {San Francisco, California, USA},
	numpages = {14},
	pages = {1041–1054},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '16},
	title = {AT-GIS: Highly Parallel Spatial Query Processing with Associative Transducers},
	url = {https://doi.org/10.1145/2882903.2882962},
	year = {2016},
}

@inproceedings{10.1145/2882903.2914833,
	abstract = {R-tree is a data structure used for multidimensional indexing. Essentially, it is a balanced tree consisting of nested hyper-rectangles which are used to locate the data. One of the most performance sensitive parts of this data structure is its split algorithm, which runs during node overflows. The split can be performed in multiple ways, according to many different criteria and in general the problem of finding an optimal solution is NP-hard. There are many heuristic split algorithms. In this paper we study an existing k-means node split algorithm. We describe a number of serious issues in its theoretical foundation, which made us to re-design k-means split. We propose several well-grounded solutions to the re-emerged problem of k-means split. Finally, we report the comparison results using PostgreSQL and contemporary benchmark for multidimensional structures.},
	address = {New York, NY, USA},
	author = {Grigorev, Valentin and Chernishev, George},
	booktitle = {Proceedings of the 2016 International Conference on Management of Data},
	doi = {10.1145/2882903.2914833},
	isbn = {9781450335317},
	keywords = {k-means, multidimensional indexing, r-tree, r-tree split},
	location = {San Francisco, California, USA},
	numpages = {2},
	pages = {2251–2252},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '16},
	title = {K-means Split Revisited: Well-grounded Approach and Experimental Evaluation},
	url = {https://doi.org/10.1145/2882903.2914833},
	year = {2016},
}

@inproceedings{10.1145/2910017.2910617,
	abstract = {When analyzing and processing videos, it has become increasingly important in many applications to also consider contextual information, in addition to the content. With the ubiquity of sensor-rich smartphones, acquiring a continuous stream of geo-spatial metadata that includes the location and orientation of a camera together with the video frames has become practical. However, no such detailed dataset is publicly available. In this paper we present an extensive geo-tagged video dataset named GeoUGV that has been collected as part of the MediaQ [3] and GeoVid [1] projects. The key features of the dataset are that each video file is accompanied by a metadata sequence of geo-tags consisting of GPS locations, compass directions, and spatial keywords at fine-grained intervals. The GeoUGV dataset has been collected by volunteer users and its statistics can be summarized as follows: 2,397 videos containing 208,976 video frames that are geo-tagged, collected by 289 users in more than 20 cities across the world over a period of 10 years (2007-2016). We hope that this dataset will be useful for researchers, scientists and practitioners alike in their work.},
	address = {New York, NY, USA},
	articleno = {43},
	author = {Lu, Ying and To, Hien and Alfarrarjeh, Abdullah and Kim, Seon Ho and Yin, Yifang and Zimmermann, Roger and Shahabi, Cyrus},
	booktitle = {Proceedings of the 7th International Conference on Multimedia Systems},
	doi = {10.1145/2910017.2910617},
	isbn = {9781450342971},
	keywords = {dataset, geo-tagging, metadata, mobile video, multimedia},
	location = {Klagenfurt, Austria},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {MMSys '16},
	title = {GeoUGV: user-generated mobile video dataset with fine granularity spatial metadata},
	url = {https://doi.org/10.1145/2910017.2910617},
	year = {2016},
}

@inproceedings{10.1145/2911996.2912022,
	abstract = {Where in the world are pictures of cute animals or ancient architecture most shared from? And are they equally sentimentally perceived across different languages? We demonstrate a series of visualization tools, that we collectively call SentiCart, for answering such questions and navigating the landscape of how sentiment-biased images are shared around the world in multiple languages. We present visualizations using a large-scale, self-gathered geodata corpus of &gt;1.54M geo-references coming from over 235 countries mined from &gt;15K visual concepts over 12 languages. We also highlight several compelling data-driven findings about multilingual visual sentiment in geo-social interactions.},
	address = {New York, NY, USA},
	author = {Jou, Brendan and Qian, Margaret Yuying and Chang, Shih-Fu},
	booktitle = {Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval},
	doi = {10.1145/2911996.2912022},
	isbn = {9781450343596},
	keywords = {GIS, affective computing, geodata, multilingual, ontology, sentiment, visual affect, visualization},
	location = {New York, New York, USA},
	numpages = {4},
	pages = {389–392},
	publisher = {Association for Computing Machinery},
	series = {ICMR '16},
	title = {SentiCart: Cartography and Geo-contextualization for Multilingual Visual Sentiment},
	url = {https://doi.org/10.1145/2911996.2912022},
	year = {2016},
}

@inproceedings{10.1145/2914586.2914588,
	abstract = {If you were to open your own cafe, would you not want to effortlessly identify the most suitable location to set up your shop? Choosing an optimal physical location is a critical decision for numerous businesses, as many factors contribute to the final choice of the location. In this paper, we seek to address the issue by investigating the use of publicly available Facebook Pages data-which include user "check-ins", types of business, and business locations-to evaluate a user-selected physical location with respect to a type of business. Using a dataset of 20,877 food businesses in Singapore, we conduct analysis of several key factors including business categories, locations, and neighboring businesses. From these factors, we extract a set of relevant features and develop a robust predictive model to estimate the popularity of a business location. Our experiments have shown that the popularity of neighboring business contributes the key features to perform accurate prediction. We finally illustrate the practical usage of our proposed approach via an interactive web application system.},
	address = {New York, NY, USA},
	author = {Lin, Jovian and Oentaryo, Richard and Lim, Ee-Peng and Vu, Casey and Vu, Adrian and Kwee, Agus},
	booktitle = {Proceedings of the 27th ACM Conference on Hypertext and Social Media},
	doi = {10.1145/2914586.2914588},
	isbn = {9781450342476},
	keywords = {facebook, feature extraction, location analytics, machine learning},
	location = {Halifax, Nova Scotia, Canada},
	numpages = {10},
	pages = {93–102},
	publisher = {Association for Computing Machinery},
	series = {HT '16},
	title = {Where is the Goldmine? Finding Promising Business Locations through Facebook Data Analytics},
	url = {https://doi.org/10.1145/2914586.2914588},
	year = {2016},
}

@inproceedings{10.1145/2914586.2914621,
	abstract = {Crowd-mapping is a form of collaborative work that empowers users to gather and share geographic knowledge. OpenStreetMap is one of the most successful examples of such paradigm, where the goal of building a global map of the world is collectively performed by over 2M contributors. Despite geographic information being intrinsically evolving, little research has so far gone into analysing maintenance practices in these domains. In this paper, we perform a preliminary exploration to quantitatively capture maintenance dynamics in geographic crowd-sourced datasets, in terms of: the extent to which different maintenance actions are taking place, the type of spatial information that is being maintained, and who engages in these practices. We apply this method to 117 countries in OSM, over one year of mapping activity. Our findings reveal that, although maintenance practices vary substantially from country to country in terms of how widespread they are, strong commonalities exist in terms of what metadata is being maintained and by whom.},
	address = {New York, NY, USA},
	author = {Quattrone, Giovanni and Dittus, Martin and Capra, Licia},
	booktitle = {Proceedings of the 27th ACM Conference on Hypertext and Social Media},
	doi = {10.1145/2914586.2914621},
	isbn = {9781450342476},
	keywords = {collaboration practices, crowd-sourcing, maintenance work, openstreetmap, volunteered geographic information},
	location = {Halifax, Nova Scotia, Canada},
	numpages = {6},
	pages = {285–290},
	publisher = {Association for Computing Machinery},
	series = {HT '16},
	title = {Exploring Maintenance Practices in Crowd-Mapping},
	url = {https://doi.org/10.1145/2914586.2914621},
	year = {2016},
}

@inproceedings{10.1145/2933267.2933293,
	abstract = {Existing commercial database systems provide spatial libraries that support functions on static non-moving spatial objects, e.g., points, linestrings and polygons. Examples of these spatial functions include intersection, distance, buffer, and convex hull computation. The RxSpatial, or Reactive Spatial, library provides the same functionality support in the context of moving objects, and addresses the spatial computation challenges over high-frequency, low-latency, real-time moving objects. This demo presents the RxSpatial library that enables developers to instantly compute spatio-temporal operations in an incremental and streaming fashion. The demo scenarios show the applicability of the library in two real-world applications: spatio-temporal social networks and family locators.},
	address = {New York, NY, USA},
	author = {Hendawi, Abdeltawab M. and Shi, Youying and Fattah, Hossam and Karwa, Jumana and Ali, Mohamed},
	booktitle = {Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems},
	doi = {10.1145/2933267.2933293},
	isbn = {9781450340212},
	keywords = {RUM-tree, SQL server, moving objects, query processing},
	location = {Irvine, California},
	numpages = {2},
	pages = {366–367},
	publisher = {Association for Computing Machinery},
	series = {DEBS '16},
	title = {RxSpatial: a framework for real-time spatio-temporal operations: demo},
	url = {https://doi.org/10.1145/2933267.2933293},
	year = {2016},
}

@inproceedings{10.1145/2938559.2948850,
	abstract = {There are a number of index structure has been developed for spatial keyword query. Geographic query is composed of query keywords and a location. Given a location and a set of keywords, spatial keyword query returns the objects that are relevant to the text describing the objects to the query keywords. Most of the spatial queries are considered for English keyword. Myanmar native language is also necessary for the users who are not familiar with English language. This paper proposed an index structure for Myanmar language to efficiently search the desired location with Myanmar language on the mobile devices. Mobile devices own a limited memory and a low computational capacity than the Personal Computer (PC). So, spatial index is one of the problems in the mobile devices. This index structure reduces the using space and searching time. In this index structure, two dimensional coordinate points is converted to the single value by using the Hilbert curve. Then, B-tree that combine the inverted file with Myanmar language is constructed according to the value from the Hilbert curve. Myanmar 3 Unicode is used for keyword search.},
	address = {New York, NY, USA},
	author = {Khine, Myat Thiri and Sein, Myint Myint},
	booktitle = {Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services Companion},
	doi = {10.1145/2938559.2948850},
	isbn = {9781450344166},
	keywords = {b-tree, hilbert curve, myanmar language, spatial keyword queries},
	location = {Singapore, Singapore},
	numpages = {1},
	pages = {43},
	publisher = {Association for Computing Machinery},
	series = {MobiSys '16 Companion},
	title = {Poster: Index Structure for Spatial Keyword Query with Myanmar Language on the Mobile Devices},
	url = {https://doi.org/10.1145/2938559.2948850},
	year = {2016},
}

@inproceedings{10.1145/2938559.2948851,
	abstract = {This paper intends to develop the optimal path finding for emergency cases on mobile devices. According to the weak road network infrastructure of Myanmar, there are some difficulties for Emergency Vehicles. In some townships, there are narrowed roads which are not wide enough to enter the vehicles and closed roads which are not passed through the other streets. In the emergency cases (e.g. Accident or Fire), the drivers mistakenly choose these roads, it can cause problems and delays. The main objective of this system is to find the optimal routes between incident site and emergency services without delay caused by closed and narrowed roads. The system uses Multiple Sources Single-Destination (MSSD) Algorithm using node exclusion to calculate optimal route. Our proposed system significantly solves to find the accident location and locate the closest emergency services by using the real-time technology (GPS/GSM).},
	address = {New York, NY, USA},
	author = {Phyo, K-zin and Sein, Myint Myint},
	booktitle = {Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services Companion},
	doi = {10.1145/2938559.2948851},
	isbn = {9781450344166},
	keywords = {gps, gsm: mssd, optimal path finding},
	location = {Singapore, Singapore},
	numpages = {1},
	pages = {71},
	publisher = {Association for Computing Machinery},
	series = {MobiSys '16 Companion},
	title = {Poster: Optimal Path Finding for Emergency Cases on Android},
	url = {https://doi.org/10.1145/2938559.2948851},
	year = {2016},
}

@inproceedings{10.1145/2939672.2939682,
	abstract = {The Atlanta Fire Rescue Department (AFRD), like many municipal fire departments, actively works to reduce fire risk by inspecting commercial properties for potential hazards and fire code violations. However, AFRD's fire inspection practices relied on tradition and intuition, with no existing data-driven process for prioritizing fire inspections or identifying new properties requiring inspection. In collaboration with AFRD, we developed the Firebird framework to help municipal fire departments identify and prioritize commercial property fire inspections, using machine learning, geocoding, and information visualization. Firebird computes fire risk scores for over 5,000 buildings in the city, with true positive rates of up to 71\% in predicting fires. It has identified 6,096 new potential commercial properties to inspect, based on AFRD's criteria for inspection. Furthermore, through an interactive map, Firebird integrates and visualizes fire incidents, property information and risk scores to help AFRD make informed decisions about fire inspections. Firebird has already begun to make positive impact at both local and national levels. It is improving AFRD's inspection processes and Atlanta residents' safety, and was highlighted by National Fire Protection Association (NFPA) as a best practice for using data to inform fire inspections.},
	address = {New York, NY, USA},
	author = {Madaio, Michael and Chen, Shang-Tse and Haimson, Oliver L. and Zhang, Wenwen and Cheng, Xiang and Hinds-Aldrich, Matthew and Chau, Duen Horng and Dilkina, Bistra},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/2939672.2939682},
	isbn = {9781450342322},
	keywords = {data science, fire risk, government innovation, interactive visualization, predictive modeling},
	location = {San Francisco, California, USA},
	numpages = {10},
	pages = {185–194},
	publisher = {Association for Computing Machinery},
	series = {KDD '16},
	title = {Firebird: Predicting Fire Risk and Prioritizing Fire Inspections in Atlanta},
	url = {https://doi.org/10.1145/2939672.2939682},
	year = {2016},
}

@inproceedings{10.1145/2939672.2939734,
	abstract = {We present a secondary ranking system to find and remove erroneous suggestions from a geospatial recommendation system. We discover such anomalous links by "double checking" the recommendation system's output to ensure that it is both structurally cohesive, and semantically consistent.Our approach is designed for the Google Related Places Graph, a geographic recommendation system which provides results for hundreds of millions of queries a day. We model the quality of a recommendation between two geographic entities as a function of their structure in the Related Places Graph, and their semantic relationship in the Google Knowledge Graph.To evaluate our approach, we perform a large scale human evaluation of such an anomalous link detection system. For the long tail of unpopular entities, our models can predict the recommendations users will consider poor with up to 42\% higher mean precision (29 raw points) than the live system.Results from our study reveal that structural and semantic features capture different facets of relatedness to human judges. We characterize our performance with a qualitative analysis detailing the categories of real-world anomalies our system is able to detect, and provide a discussion of additional applications of our method.},
	address = {New York, NY, USA},
	author = {Perozzi, Bryan and Schueppert, Michael and Saalweachter, Jack and Thakur, Mayur},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/2939672.2939734},
	isbn = {9781450342322},
	keywords = {anomaly detection, knowledge graph, link prediction, recommendation systems},
	location = {San Francisco, California, USA},
	numpages = {10},
	pages = {569–578},
	publisher = {Association for Computing Machinery},
	series = {KDD '16},
	title = {When Recommendation Goes Wrong: Anomalous Link Discovery in Recommendation Networks},
	url = {https://doi.org/10.1145/2939672.2939734},
	year = {2016},
}

@inproceedings{10.1145/2939672.2939736,
	abstract = {Crime is one of the most important social problems in the country, affecting public safety, children development, and adult socioeconomic status. Understanding what factors cause higher crime is critical for policy makers in their efforts to reduce crime and increase citizens' life quality. We tackle a fundamental problem in our paper: crime rate inference at the neighborhood level. Traditional approaches have used demographics and geographical influences to estimate crime rates in a region. With the fast development of positioning technology and prevalence of mobile devices, a large amount of modern urban data have been collected and such big data can provide new perspectives for understanding crime. In this paper, we used large-scale Point-Of-Interest data and taxi flow data in the city of Chicago, IL in the USA. We observed significantly improved performance in crime rate inference compared to using traditional features. Such an improvement is consistent over multiple years. We also show that these new features are significant in the feature importance analysis.},
	address = {New York, NY, USA},
	author = {Wang, Hongjian and Kifer, Daniel and Graif, Corina and Li, Zhenhui},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/2939672.2939736},
	isbn = {9781450342322},
	keywords = {big data, crime inference, heterogeneous data, spatial-temporal data},
	location = {San Francisco, California, USA},
	numpages = {10},
	pages = {635–644},
	publisher = {Association for Computing Machinery},
	series = {KDD '16},
	title = {Crime Rate Inference with Big Data},
	url = {https://doi.org/10.1145/2939672.2939736},
	year = {2016},
}

@inproceedings{10.1145/2939672.2939773,
	abstract = {Point-of-interest (POI) recommendation, which helps mobile users explore new places, has become an important location-based service. Existing approaches for POI recommendation have been mainly focused on exploiting the information about user preferences, social influence, and geographical influence. However, these approaches cannot handle the scenario where users are expecting to have POI recommendation for a specific time period. To this end, in this paper, we propose a unified recommender system, named the 'Where and When to gO' (WWO) recommender system, to integrate the user interests and their evolving sequential preferences with temporal interval assessment. As a result, the WWO system can make recommendations dynamically for a specific time period and the traditional POI recommender system can be treated as the special case of the WWO system by setting this time period long enough. Specifically, to quantify users' sequential preferences, we consider the distributions of the temporal intervals between dependent POIs in the historical check-in sequences. Then, to estimate the distributions with only sparse observations, we develop the low-rank graph construction model, which identifies a set of bi-weighted graph bases so as to learn the static user preferences and the dynamic sequential preferences in a coherent way. Finally, we evaluate the proposed approach using real-world data sets from several location-based social networks (LBSNs). The experimental results show that our method outperforms the state-of-the-art approaches for POI recommendation in terms of various metrics, such as F-measure and NDCG, with a significant margin.},
	address = {New York, NY, USA},
	author = {Liu, Yanchi and Liu, Chuanren and Liu, Bin and Qu, Meng and Xiong, Hui},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/2939672.2939773},
	isbn = {9781450342322},
	keywords = {poi recommendation, sequential preference},
	location = {San Francisco, California, USA},
	numpages = {10},
	pages = {1015–1024},
	publisher = {Association for Computing Machinery},
	series = {KDD '16},
	title = {Unified Point-of-Interest Recommendation with Temporal Interval Assessment},
	url = {https://doi.org/10.1145/2939672.2939773},
	year = {2016},
}

@inproceedings{10.1145/2939672.2939830,
	abstract = {The rapid urbanization has motivated extensive research on urban computing. It is critical for urban computing tasks to unlock the power of the diversity of data modalities generated by different sources in urban spaces, such as vehicles and humans. However, we are more likely to encounter the label scarcity problem and the data insufficiency problem when solving an urban computing task in a city where services and infrastructures are not ready or just built. In this paper, we propose a FLexible multimOdal tRAnsfer Learning (FLORAL) method to transfer knowledge from a city where there exist sufficient multimodal data and labels, to this kind of cities to fully alleviate the two problems. FLORAL learns semantically related dictionaries for multiple modalities from a source domain, and simultaneously transfers the dictionaries and labelled instances from the source into a target domain. We evaluate the proposed method with a case study of air quality prediction.},
	address = {New York, NY, USA},
	author = {Wei, Ying and Zheng, Yu and Yang, Qiang},
	booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/2939672.2939830},
	isbn = {9781450342322},
	keywords = {multi-modality, transfer learning, urban computing},
	location = {San Francisco, California, USA},
	numpages = {10},
	pages = {1905–1914},
	publisher = {Association for Computing Machinery},
	series = {KDD '16},
	title = {Transfer Knowledge between Cities},
	url = {https://doi.org/10.1145/2939672.2939830},
	year = {2016},
}

@inproceedings{10.1145/2945292.2945294,
	abstract = {This paper presents an immersive geo-spatial social media system for virtual and augmented reality environments. With the rapid growth of photo-sharing social media sites such as Flickr, Pinterest, and Instagram, geo-tagged photographs are now ubiquitous. However, the current systems for their navigation are unsatisfyingly one- or two-dimensional. In this paper, we present our prototype system, Social Street View, which renders the geo-tagged social media in its natural geo-spatial context provided by immersive maps, such as Google Street View. This paper presents new algorithms for fusing and laying out the social media in an aesthetically pleasing manner with geospatial renderings, validates them with respect to visual saliency metrics, suggests spatio-temporal filters, and presents a system architecture that is able to stream geo-tagged social media and render it across a range of display platforms spanning tablets, desktops, head-mounted displays, and large-area room-sized curved tiled displays. The paper concludes by exploring several potential use cases including immersive social storytelling, learning about culture and crowd-sourced tourism.},
	address = {New York, NY, USA},
	author = {Du, Ruofei and Varshney, Amitabh},
	booktitle = {Proceedings of the 21st International Conference on Web3D Technology},
	doi = {10.1145/2945292.2945294},
	isbn = {9781450344289},
	keywords = {WebGL, geographical information systems, mixed reality, social media, spatial-temporal virtual reality, street view},
	location = {Anaheim, California},
	numpages = {9},
	pages = {77–85},
	publisher = {Association for Computing Machinery},
	series = {Web3D '16},
	title = {Social street view: blending immersive street views with geo-tagged social media},
	url = {https://doi.org/10.1145/2945292.2945294},
	year = {2016},
}

@inproceedings{10.1145/2948649.2948653,
	abstract = {The automatic classification of patent applications into a particular patent classification system remains a challenge with many practical applications. From a computer science point of view, the task is a multi-label hierarchical classification problem, i.e. each patent application might belong to multiple classes within the class hierarchy. The problem is still especially difficult for purely text-based classifiers because patents and patent applications are often formulated in a rather generic way. Thus, additional sources of information should be used to improve class prediction. In our approach, we propose the use of location information contained in the meta data of a patent application in combination with text-based patent classification. We argue that certain technological areas often cluster in geographic regions. For example, space travel technology is often collocated at Houston, Texas due to the NASA facilities in this area. In many cases, the addresses of the inventors are correlated to the technological area of a given patent. Thus, the addresses can be exploited to provide additional information about the technological area. We present a geo-enriched classifier joining established methods for text-based classification with location-based topic prediction. Since the location-based prediction is not applicable to all cases, we provide a method to regulate the impact of the spatial predictor for these cases. Our experiments indicate that spatial prediction is applicable to a considerable amount of patent applications and that the combination of spatial prediction and text-based classification significantly improves the prediction accuracy.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Stutzki, Jan and Schubert, Matthias},
	booktitle = {Proceedings of the Third International ACM SIGMOD Workshop on Managing and Mining Enriched Geo-Spatial Data},
	doi = {10.1145/2948649.2948653},
	isbn = {9781450343091},
	keywords = {ensemble classification, geospatial data, patent classification, support vector machines},
	location = {San Francisco, California},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {GeoRich '16},
	title = {Geodata supported classification of patent applications},
	url = {https://doi.org/10.1145/2948649.2948653},
	year = {2016},
}

@article{10.1145/2963147,
	abstract = {The field of Geographical Information Systems (GIS) has experienced a rapid and ongoing growth of available sources for geospatial data. This growth has demanded more data integration in order to explore the benefits of these data further. However, many data providers implies many points of view for the same phenomena: geospatial features. We need sophisticated procedures aiming to find the correspondences between two vector datasets, a process named geospatial data matching. Similarity measures are key-tools for matching methods, so it is interesting to review these concepts together. This article provides a survey of 30 years of research into the measures and methods facing geospatial data matching. Our survey presents related work and develops a common taxonomy that permits us to compare measures and methods. This study points out relevant issues that may help to discover the potential of these approaches in many applications, like data integration, conflation, quality evaluation, and data management.},
	address = {New York, NY, USA},
	articleno = {39},
	author = {Xavier, Emerson M. A. and Ariza-L\'{o}pez, Francisco J. and Ure\~{n}a-C\'{a}mara, Manuel A.},
	doi = {10.1145/2963147},
	issn = {0360-0300},
	issue_date = {June 2017},
	journal = {ACM Comput. Surv.},
	keywords = {Spatial data integration, geometric algorithms, map conflation},
	month = {aug},
	number = {2},
	numpages = {34},
	publisher = {Association for Computing Machinery},
	title = {A Survey of Measures and Methods for Matching Geospatial Vector Datasets},
	url = {https://doi.org/10.1145/2963147},
	volume = {49},
	year = {2016},
}

@inproceedings{10.1145/2964284.2986911,
	abstract = {The world is a big place. At any given instant something is happening somewhere, but even when nothing is going on people still find ways to generate multimedia data, ranging from social media posts, to photos and videos. A substantial number of these media objects is associated with a location, and in an increasingly mobile and connected world (both in terms of people and devices), this number is only bound to get larger. Yet, in the multimedia literature we observe that many researchers often unwittingly treat the geospatial dimension as if it were a regular feature dimension, despite it requiring special attention. In order to avoid pitfalls and to steer clear of erroneous conclusions, this tutorial aims to teach researchers and students how geotagged multimedia data differs from regular data and to educate them on best practices when dealing with such data. We will cover the lifecycle of geotagged data in multimedia research, where the topics range from how this kind of data is represented, processed, analyzed, and visualized. The tutorial requires both passive and active involvement, where we not only present the material, but the attendees also get the opportunity to interact with it using a variety of open source data and tools that we have prepared using a virtual machine.},
	address = {New York, NY, USA},
	author = {Schifanella, Rossano and Thom\'{e}e, Bart},
	booktitle = {Proceedings of the 24th ACM International Conference on Multimedia},
	doi = {10.1145/2964284.2986911},
	isbn = {9781450336031},
	keywords = {geospatial, geotagged, multimedia, visualization},
	location = {Amsterdam, The Netherlands},
	numpages = {2},
	pages = {1471–1472},
	publisher = {Association for Computing Machinery},
	series = {MM '16},
	title = {The Lifecycle of Geotagged Multimedia Data},
	url = {https://doi.org/10.1145/2964284.2986911},
	year = {2016},
}

@inproceedings{10.1145/2976796.2976854,
	abstract = {Trajectories of moving objects have been an active research topic for over a decade. Classical approaches to analyze the trajectories of these objects are mainly based on large amounts of data acquired from positioning devices, such GPS receivers. GPS data has the advantage of describing the trajectory of an object with a great level of detail and high accuracy, but the data do not carry any kind of semantic information. On the other hand, nowadays it is growing the number of interactions in social networks with some kind of information about the location of the user. Georeferenced social interactions are an important source of semantic about user's trajectories and activities. This work proposes a solution for reconstructing travel histories using heterogeneous social track sources posts in social networks, GPS positioning data, location history data generated by cloud services or any digital footprint with an associated geographic position. The solution encompasses a conceptual model; a methodology to reconstruct travel histories based on heterogeneous social tracks sources; and an application to present the reconstructed travel itinerary in a graphical and interactive fashion. An experiment conducted with real travel experiences showed that the proposed solution is a reasonable way to reconstruct travels histories, geographically and semantically, in an automatic fashion.},
	address = {New York, NY, USA},
	author = {Santana, Amon Veiga and Campos, Jorge},
	booktitle = {Proceedings of the 22nd Brazilian Symposium on Multimedia and the Web},
	doi = {10.1145/2976796.2976854},
	isbn = {9781450345125},
	keywords = {travel history, trajectory semantic enrichment, social networks, moving object trajectories, geographic information systems, crowdsourcing},
	location = {Teresina, Piau\'{\i} State, Brazil},
	numpages = {8},
	pages = {311–318},
	publisher = {Association for Computing Machinery},
	series = {Webmedia '16},
	title = {Travel History: Reconstructing Semantic Trajectories Based on Heterogeneous Social Tracks Sources},
	url = {https://doi.org/10.1145/2976796.2976854},
	year = {2016},
}

@inproceedings{10.1145/2978192.2978240,
	abstract = {This research and software development project, is carried out based on a problem about the clinical context in Bogot\'{a} D.C., Colombia. It seeks through a web and mobile application, to inform citizens who are looking for, or moving to an emergency room, about the population status in which it is at the moment, in order to reduce the number of patients present in there by proposing some less saturated centers to the user. The latter, in order to expose a possible solution to the health crisis that exists in the city and the overcrowding that occurs especially in its medical centers.},
	address = {New York, NY, USA},
	author = {Cruz, Andr\'{e}s F. and Ospina, Sebasti\'{a}n and Feij\'{o}o, Pedro G. and Suarez, Daniel R.},
	booktitle = {Proceedings of the 17th Annual Conference on Information Technology Education},
	doi = {10.1145/2978192.2978240},
	isbn = {9781450344524},
	keywords = {crowd computing, human centered computing, social computing, validated learning, web and mobile technology},
	location = {Boston, Massachusetts, USA},
	numpages = {1},
	pages = {166},
	publisher = {Association for Computing Machinery},
	series = {SIGITE '16},
	title = {GALILEO: Emergency Rooms and Crowd Computing},
	url = {https://doi.org/10.1145/2978192.2978240},
	year = {2016},
}

@inproceedings{10.1145/2983310.2985747,
	abstract = {This paper explores the design space for interacting with maps on Optical (See-Through) Head-Mounted Displays (OHMDs). The resulting interactions were evaluated in a comprehensive experiment involving 31 participants. More precisely, novel head-based interactions were compared with well-known haptic interactions on an OHMD regarding efficiency, effectiveness, user experience and perceived cognitive workload. The tasks involved navigating on maps by panning, zooming, and both panning and zooming. The results suggest that interaction methods exploiting congruent spatial relationships, i.e., mappings between the same axis in the control and display space, outperform others. In particular, the head-based interactions incorporating such mappings, significantly outperformed the haptic interactions for tasks involving panning, and combined tasks of panning and zooming.},
	address = {New York, NY, USA},
	author = {Rudi, David and Giannopoulos, Ioannis and Kiefer, Peter and Peier, Christian and Raubal, Martin},
	booktitle = {Proceedings of the 2016 Symposium on Spatial User Interaction},
	doi = {10.1145/2983310.2985747},
	isbn = {9781450340687},
	keywords = {interaction design, maps, optical head-mounted display},
	location = {Tokyo, Japan},
	numpages = {10},
	pages = {3–12},
	publisher = {Association for Computing Machinery},
	series = {SUI '16},
	title = {Interacting with Maps on Optical Head-Mounted Displays},
	url = {https://doi.org/10.1145/2983310.2985747},
	year = {2016},
}

@inproceedings{10.1145/2983323.2983337,
	abstract = {The typical Internet user has data spread over several devices and across several online systems. We demonstrate an open-source system for integrating user's data from different sources into a single Knowledge Base. Our system integrates data of different kinds into a coherent whole, starting with email messages, calendar, contacts, and location history. It is able to detect event periods in the user's location data and align them with calendar events. We will demonstrate how to query the system within and across different dimensions, and perform analytics over emails, events, and locations.},
	address = {New York, NY, USA},
	author = {Montoya, David and Pellissier Tanon, Thomas and Abiteboul, Serge and Suchanek, Fabian M.},
	booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
	doi = {10.1145/2983323.2983337},
	isbn = {9781450340731},
	keywords = {data integration, open-source, personal information, querying},
	location = {Indianapolis, Indiana, USA},
	numpages = {4},
	pages = {2477–2480},
	publisher = {Association for Computing Machinery},
	series = {CIKM '16},
	title = {Thymeflow, A Personal Knowledge Base with Spatio-temporal Data},
	url = {https://doi.org/10.1145/2983323.2983337},
	year = {2016},
}

@inproceedings{10.1145/2983323.2983757,
	abstract = {With the proliferation of geo-positioning techniques that enable users to acquire their geographical positions, there has been increasing popularity of online location-based services. This development has generated a large volume of points of interest labeled with category features (e.g., hotel, resort, stores, stations, and tourist attractions). It gives prominence to various types of spatial-keyword queries, which are employed to provide fundamental querying functionality for location-based services.We study the Location-aware Group Preference (LGP) query that aims to find a destination place for a group of users. The group of users want to go to a place labeled with a specified category feature (e.g., hotel) together, and each of them has a location and a set of additional preferences. It is expected that the result place of the query belongs to the specified category feature, and it is close to places satisfying the preferences of each user. We develop a novel framework for answering the LGP query, which can be used to compute both exact query result and approximate result with a proven approximation ratio. The efficiency and efficacy of the proposed algorithms for answering the LGP query are verified by extensive experiments on two real datasets.},
	address = {New York, NY, USA},
	author = {Li, Miao and Chen, Lisi and Cong, Gao and Gu, Yu and Yu, Ge},
	booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
	doi = {10.1145/2983323.2983757},
	isbn = {9781450340731},
	keywords = {group, location, preference, query processing},
	location = {Indianapolis, Indiana, USA},
	numpages = {10},
	pages = {559–568},
	publisher = {Association for Computing Machinery},
	series = {CIKM '16},
	title = {Efficient Processing of Location-Aware Group Preference Queries},
	url = {https://doi.org/10.1145/2983323.2983757},
	year = {2016},
}

@inproceedings{10.1145/2983323.2983797,
	abstract = {With the popularity of mobile GPS devices such as on-board navigation systems and smart phones, users can contribute their GPS trajectory data for creating geo-volunteered road maps. However, the quality of these road maps cannot be guaranteed due to the lack of expertise among contributing users. Therefore, important challenges are (i) to automatically generate accurate roads from GPS traces and (ii) to validate the correctness of existing road maps. To address these challenges, we propose a novel Spatial-Linear Clustering (SLC) technique to infer road segments from GPS traces. In our algorithm, we propose the use of spatial-linear clusters to appropriately represent the linear nature of GPS points collected from the same road segment. Through inferring road segments our algorithm can detect missing roads and checking the correctness of existing road network. For our evaluation, we conduct extensive experiments that compare our method to the state-of-the-art methods on two real data sets. The experimental results show that the F1 score of our algorithm is on average 10.7\% higher than the best state-of-the-art method.},
	address = {New York, NY, USA},
	author = {Li, Hengfeng and Kulik, Lars and Ramamohanarao, Kotagiri},
	booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
	doi = {10.1145/2983323.2983797},
	isbn = {9781450340731},
	keywords = {gps traces, map inference, spatial-linear clustering},
	location = {Indianapolis, Indiana, USA},
	numpages = {10},
	pages = {1523–1532},
	publisher = {Association for Computing Machinery},
	series = {CIKM '16},
	title = {Automatic Generation and Validation of Road Maps from GPS Trajectory Data Sets},
	url = {https://doi.org/10.1145/2983323.2983797},
	year = {2016},
}

@inproceedings{10.1145/2983323.2983850,
	abstract = {Microblogging services like Twitter contain abundant of user generated content covering a wide range of topics. Many of the tweets can be associated to real-world entities for providing additional information for the latter. In this paper, we aim to associate tweets that are semantically related to real-world locations or Points of Interest (POIs). Tweets contain dynamic and real-time information while POIs contain relatively static information. The tweets associated with POIs provide complementary information for many applications like opinion mining and POI recommendation; the associated POIs can also be used as POI tags in Twitter. We define the research problem of annotating POIs with tweets and propose a novel supervised Bayesian Model (sBM). The model takes into account the textual, spatial features and user behaviors together with the supervised information of whether a tweet is POI-related. It is able to capture user interests in latent regions for the prediction of whether a tweet is POI-related and the association between the tweet and its most semantically related POI. On tweets and POIs collected for two cities (New York City and Singapore), we demonstrate the effectiveness of our models against baseline methods.},
	address = {New York, NY, USA},
	author = {Zhao, Kaiqi and Cong, Gao and Sun, Aixin},
	booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
	doi = {10.1145/2983323.2983850},
	isbn = {9781450340731},
	keywords = {POI annotation, bayesian model, regression},
	location = {Indianapolis, Indiana, USA},
	numpages = {10},
	pages = {417–426},
	publisher = {Association for Computing Machinery},
	series = {CIKM '16},
	title = {Annotating Points of Interest with Geo-tagged Tweets},
	url = {https://doi.org/10.1145/2983323.2983850},
	year = {2016},
}

@inproceedings{10.1145/2983323.2983887,
	abstract = {Previous research on content-based geolocation in general has developed prediction methods via conducting pre-partitioning and applying classification methods. The input of these methods is the concatenation of individual tweets during a period of time. But unfortunately, these methods have some drawbacks. They discard the natural real-values properties of latitude and longitude as well as fail to capture geolocation in near real-time. In this work, we develop a novel generative content-based regression model via a matrix factorization technique to tackle the near real-time geolocation prediction problem. With this model, we aim to address a couple of un-answered questions. First, we prove that near real-time geolocation prediction can be accomplished if we leave out the concatenation. Second, we account the real-values properties of physical coordinates within a regression solution. We apply our model on Twitter datasets as an example to prove the effectiveness and generality. Our experimental results show that the proposed model, in the best scenario, outperforms a set of state-of-the-art regression models including Support Vector Machines and Factorization Machines by a reduction of the median localization error up to 79\%.},
	address = {New York, NY, USA},
	author = {Duong-Trung, Nghia and Schilling, Nicolas and Schmidt-Thieme, Lars},
	booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
	doi = {10.1145/2983323.2983887},
	isbn = {9781450340731},
	keywords = {geolocation, matrix factorization, regression, twitter},
	location = {Indianapolis, Indiana, USA},
	numpages = {4},
	pages = {1973–1976},
	publisher = {Association for Computing Machinery},
	series = {CIKM '16},
	title = {Near Real-time Geolocation Prediction in Twitter Streams via Matrix Factorization Based Regression},
	url = {https://doi.org/10.1145/2983323.2983887},
	year = {2016},
}

@inproceedings{10.1145/2983323.2983902,
	abstract = {Nowadays, geosensor data, such as air quality and traffic flow, have become more and more essential in people's daily life. However, installing geosensors or hiring volunteers at every location and every time is so expensive. Some organizations may have only few facilities or limited budget to sense these data. Moreover, people usually tend to know the forecast instead of ongoing observations, but the number of sensors (or volunteers) will be a hurdle to make precise prediction. In this paper, we propose a novel concept to forecast geosensor data with participatory sensing. Given a limited number of sensors or volunteers, participatory sensing assumes each of them can observe and collect data at different locations and at different time. By aggregating these sparse data observations in the past time, we propose a neural network based approach to forecast the future geosensor data in any location of an urban area. The extensive experiments have been conducted with large-scale datasets of the air quality in three cities and the traffic of bike sharing systems in two cities. Experimental results show that our predictive model can precisely forecast the air quality and the bike rentle traffic as geosensor data.},
	address = {New York, NY, USA},
	author = {Jiang, Jyun-Yu and Li, Cheng-Te},
	booktitle = {Proceedings of the 25th ACM International on Conference on Information and Knowledge Management},
	doi = {10.1145/2983323.2983902},
	isbn = {9781450340731},
	keywords = {geo-sensor data forecasting, participatory sensing, urban computing},
	location = {Indianapolis, Indiana, USA},
	numpages = {4},
	pages = {2033–2036},
	publisher = {Association for Computing Machinery},
	series = {CIKM '16},
	title = {Forecasting Geo-sensor Data with Participatory Sensing Based on Dropout Neural Network},
	url = {https://doi.org/10.1145/2983323.2983902},
	year = {2016},
}

@article{10.1145/2983645,
	abstract = {In location-based social Q8A services, people ask a question with a high expectation that local residents who have local knowledge will answer the question. However, little is known about the locality of user activities in location-based social Q8A services. This study aims to deepen our understanding of location-based knowledge sharing by investigating the following: general behavioral characteristics of users, the topical and typological patterns related to geographic characteristics, geographic locality of user activities, and motivations of local knowledge sharing. To this end, we analyzed a 12-month period Q8A dataset from Naver KiN “Here,” a location-based social Q8A mobile app, in addition to a supplementary survey dataset obtained from 285 mobile users. Our results reveal several unique characteristics of location-based social Q8A. When compared with conventional social Q8A sites, users ask and answer different topical/typological questions. In addition, those who answer have a strong spatial locality wherein they primarily have local knowledge in a few regions, in areas such as their home and work. We also find unique motivators such as ownership of local knowledge and a sense of local community. The findings reported in the article have significant implications for the design of Q8A systems, especially location-based social Q8A systems.},
	address = {New York, NY, USA},
	articleno = {16},
	author = {Park, Sangkeun and Ackerman, Mark S. and Lee, Uichin},
	doi = {10.1145/2983645},
	issn = {1559-1131},
	issue_date = {August 2018},
	journal = {ACM Trans. Web},
	keywords = {mobile applications, Knowledge sharing},
	month = {jul},
	number = {3},
	numpages = {33},
	publisher = {Association for Computing Machinery},
	title = {Localness of Location-based Knowledge Sharing: A Study of Naver KiN “Here”},
	url = {https://doi.org/10.1145/2983645},
	volume = {12},
	year = {2018},
}

@article{10.1145/2994598,
	abstract = {In this article, we present an algorithmic system for determining the proper correspondence between place markers and their labels in historical maps. We assume that the locations of place markers (usually pictographs) and labels (pieces of text) have already been determined -- either algorithmically or by hand -- and we want to match the labels to the markers. This time-consuming step in the digitization process of historical maps is nontrivial even for humans but provides valuable metadata (e.g., when subsequently georeferencing the map). To speed up this process, we model the problem in terms of combinatorial optimization, solve that problem efficiently, and show how user interaction can be used to improve the quality of the results. We also consider a version of the model where we are given label fragments and additionally have to decide which fragments go together. We show that this problem is NP-hard. However, we give a polynomial-time algorithm for a restricted version of this fragment assignment problem. We have implemented the algorithm for the main problem and tested it on a manually extracted ground truth for eight historical maps with a combined total of more than 12,800 markers and labels. On average, the algorithm correctly matches 96\% of the labels and is robust against noisy input. It furthermore performs a sensitivity analysis and in this way computes a measure of confidence for each of the matches. We use this as the basis for an interactive system where the user’s effort is directed to checking those parts of the map where the algorithm is unsure; any corrections the user makes are propagated by the algorithm. We discuss a prototype of this system and statistically confirm that it successfully locates those areas on the map where the algorithm needs help.},
	address = {New York, NY, USA},
	articleno = {13},
	author = {Budig, Benedikt and Dijk, Thomas C. Van and Wolff, Alexander},
	doi = {10.1145/2994598},
	issn = {2374-0353},
	issue_date = {November 2016},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {sensitivity analysis, metadata extraction, interaction, algorithms, Historical maps},
	month = {nov},
	number = {4},
	numpages = {24},
	publisher = {Association for Computing Machinery},
	title = {Matching Labels and Markers in Historical Maps: An Algorithm with Interactive Postprocessing},
	url = {https://doi.org/10.1145/2994598},
	volume = {2},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996929,
	abstract = {Mid-level disasters that frequently occur, such as typhoons and earthquakes, heavily affect human activities in urban areas by causing severe congestion and economic loss. Predicting the irregular movement of individuals following such disasters is crucial for managing urban systems. Past survey results show that mid-level disasters do not force many individuals to evacuate away from their homes, but do cause irregular movement by significantly delaying the movement timings, resulting in severe congestion in urban transportation. We propose a novel method that predicts such irregularity of individuals' movements in several mid-level disasters using various types of features including the victims' usual movement patterns, disaster information, and geospatial information of victims' locations. Using real GPS data of 1 million people in Tokyo, we show that our method can predict mobility delay with high accuracy,},
	address = {New York, NY, USA},
	articleno = {54},
	author = {Yabe, Takahiro and Tsubouchi, Kota and Sudo, Akihito and Sekimoto, Yoshihide},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996929},
	isbn = {9781450345897},
	keywords = {GPS data, L1-regularized logistic regression, disaster alert, frequent disasters, urban dynamics},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Predicting irregular individual movement following frequent mid-level disasters using location data from smartphones},
	url = {https://doi.org/10.1145/2996913.2996929},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996931,
	abstract = {Geographic information systems (GIS) are important for decision support based on spatial data. Due to technical and economical progress an ever increasing number of data sources are available leading to a rapidly growing fast and unreliable amount of data that can be beneficial (1) in the approximation of multivariate and causal predictions of future values as well as (2) in robust and proactive decision-making processes. However, today's GIS are not designed for such big data demands and require new methodologies to effectively model uncertainty and generate meaningful knowledge. As a consequence, we introduce BigGIS, a predictive and prescriptive spatio-temporal analytics platform, that symbiotically combines big data analytics, semantic web technologies and visual analytics methodologies. We present a novel continuous refinement model and show future challenges as an intermediate result of a collaborative research project into big data methodologies for spatio-temporal analysis and design for a big data enabled GIS.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Wiener, Patrick and Stein, Manuel and Seebacher, Daniel and Bruns, Julian and Frank, Matthias and Simko, Viliam and Zander, Stefan and Nimis, Jens},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996931},
	isbn = {9781450345897},
	keywords = {big data analytics, data architecture, knowledge generation},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {BigGIS: a continuous refinement approach to master heterogeneity and uncertainty in spatio-temporal big data (vision paper)},
	url = {https://doi.org/10.1145/2996913.2996931},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996936,
	abstract = {Understanding human mobility is important for planning and delivering various services in urban area. An important element for mobility understanding is to understand the context in which the movement takes place. In this direction, we propose a method for identifying significant locations and time periods along a bus route. The significance is based on special characteristics that locations have during specific time periods as determined from their effect of these locations on the movement of the bus. The method extracts discriminative features from the space, time and other selected attributes and then classifies locations and time periods into 5 significance classes. The classes are then rendered in different views for discovering and understanding patterns. The novelty of the method is an explicit consideration of the time dimension at different granularity levels and a visualization that facilitates comparison across the space and time dimensions while avoiding a visual clutter. We demonstrate the applicability of our approach by applying it on a large set of bus trajectories.},
	address = {New York, NY, USA},
	articleno = {56},
	author = {Mazimpaka, Jean Damasc\`{e}ne and Timpf, Sabine},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996936},
	isbn = {9781450345897},
	keywords = {bus stop, classification, data mining, spatiotemporal analysis, spatiotemporal context, trajectory, visual analysis, visualization},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {A visual and computational analysis approach for exploring significant locations and time periods along a bus route},
	url = {https://doi.org/10.1145/2996913.2996936},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996937,
	abstract = {It is estimated that 50\% of the global population lives in urban areas occupying just 0.4\% of the Earth's surface. Understanding urban activity constitutes monitoring population density and its changes over time, in urban environments. Currently, there are limited mechanisms to non-intrusively monitor population density in real-time. The pervasive use of cellular phones in urban areas is one such mechanism that provides a unique opportunity to study population density by monitoring the mobility patterns in near real-time. Cellular carriers such as AT&amp;T harvest such data through their cell towers; however, this data is proprietary and the carriers restrict access, due to privacy concerns. In this work, we propose a system that passively senses the population density and infers mobility patterns in an urban area by monitoring power spectral density in cellular frequency bands using periodic beacons from each cellphone without knowing who and where they are located. A wireless sensor network platform is being developed to perform spectral monitoring along with environmental measurements. Algorithms are developed to generate real-time fine-resolution population estimates.},
	address = {New York, NY, USA},
	articleno = {57},
	author = {Thakur, Gautam S. and Kuruganti, Teja and Bobrek, Miljko and Killough, Stephen and Nutaro, James and Liu, Cheng and Lu, Wei},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996937},
	isbn = {9781450345897},
	keywords = {population dynamics, sensors, spectral monitoring, transportation},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Real-time urban population monitoring using pervasive sensor network},
	url = {https://doi.org/10.1145/2996913.2996937},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996945,
	abstract = {Today's 'Big' spatial computing and analytics are largely processed in-memory. Still, evaluation in prominent spatial query engines is neither fully optimized for modern-class platforms nor taking full advantage of compilation (i.e., generating low-level query code). Query compilation has been rapidly rising inside in-memory relational database management systems (RDBMSs) achieving remarkable speedups; how can we bring similar benefits to spatial query engines?In this research, we bring in proven Programming Languages (PL) approaches e.g., partial evaluation, generative programming, etc. and leverage the power of modern hardware to extend query compilation inside spatial query engines. We envision a fully compiled spatial query engine that is efficient and feasible to implement in a high-level language. We describe LB2-Spatial; a prototype for a fully compiled spatial query engine that employs generative and multi-stage programming to realize query compilation. Furthermore, we discuss challenges, and conduct a preliminary experiment to highlight potential gains of compilation. Finally, we sketch potential avenues for supporting spatial query compilation in Postgres/ PostGIS; a traditional RDBMS and Spark/ Spark SQL; a main-memory cluster computing framework.},
	address = {New York, NY, USA},
	articleno = {9},
	author = {Tahboub, Ruby Y. and Rompf, Tiark},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996945},
	isbn = {9781450345897},
	keywords = {spatial query compilation},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {On supporting compilation in spatial query engines: (vision paper)},
	url = {https://doi.org/10.1145/2996913.2996945},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996949,
	abstract = {We propose a method for modeling the topology of swarm behavior in a manner which facilitates the application of machine learning techniques such as clustering. This is achieved by modeling the persistence of topological features, such as connected components and holes, of the swarm with respect to time using zig-zag persistent homology. The output of this model is subsequently transformed into a representation known as a persistence landscape. This representation forms a vector space and therefore facilitates the application of machine learning techniques. The proposed model is validated using a real data set corresponding to a swarm of 300 fish. We demonstrate that it may be used to perform clustering of swarm behavior with respect to topological features.},
	address = {New York, NY, USA},
	articleno = {65},
	author = {Corcoran, Padraig and Jones, Christopher B.},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996949},
	isbn = {9781450345897},
	keywords = {persistence landscape, spatio-temporal, swarm, topology},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Spatio-temporal modeling of the topology of swarm behavior with persistence landscapes},
	url = {https://doi.org/10.1145/2996913.2996949},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996951,
	abstract = {Over the course of three years, the New York Public Library has run a crowdsourcing project to extract polygonal representation of the building footprints from insurance atlases of the 19th and early-20th century. As is common in crowd-sourcing projects, the overall problem was decomposed into small user tasks and each task was given to multiple users. In the case of polygons representing building footprints, it is unclear how best to integrate the answers into a majority vote: given a set of polygons ostensibly describing the same footprint, what is the consensus? We discuss desirable properties of such a "consensus polygon" and arrive at an efficient algorithm. We have manually evaluated the algorithm on approximately 3,000 polygons corresponding to 200 footprints and observe that our algorithmic consensus polygons are correct for 96\% of the footprints whereas only 85\% of the (input) crowd polygons are correct.},
	address = {New York, NY, USA},
	articleno = {66},
	author = {Budig, Benedikt and van Dijk, Thomas C. and Feitsch, Fabian and Arteaga, Mauricio Giraldo},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996951},
	isbn = {9781450345897},
	keywords = {deep georeferencing, historical maps, polygon consensus, smart crowdsourcing},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Polygon consensus: smart crowdsourcing for extracting building footprints from historical maps},
	url = {https://doi.org/10.1145/2996913.2996951},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996952,
	abstract = {We propose parallel algorithms in the massively parallel communication (MPC) model (e.g. MapReduce) for processing large terrain elevation data (represented as a 3D point cloud) that are too big to fit on one machine. In particular, given a set S of 3D points that is distributed across multiple machines, we present a simple randomized algorithm to construct a TIN DEM of S by computing the Delaunay triangulation of the xy-projections of points in S, which is also stored across multiple machines. With high probability, the algorithm works in O(1) rounds and the total work performed is O(n log n). Next, we describe an efficient algorithm in the MPC model for computing the contour tree of the resulting DEM. Under some assumptions on the input, the algorithm works in O(1) rounds and the total work performed is O(n log n).},
	address = {New York, NY, USA},
	articleno = {25},
	author = {Nath, Abhinandan and Fox, Kyle and Agarwal, Pankaj K. and Munagala, Kamesh},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996952},
	isbn = {9781450345897},
	keywords = {contour trees, delaunay triangulation, mapreduce, massively parallel communication},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Massively parallel algorithms for computing TIN DEMs and contour trees for large terrains},
	url = {https://doi.org/10.1145/2996913.2996952},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996957,
	abstract = {The increased availability of interactive maps on the Internet and on personal mobile devices has created new challenges in computational cartography and, in particular, for label placement in maps. Operations like rotation, zoom, and translation dynamically change the map over time and make a consistent adaptation of the map labeling necessary.In this paper, we consider map labeling for the case that a map undergoes a sequence of operations over a specified time span. We unify and generalize several preceding models for dynamic map labeling into one versatile and flexible model. In contrast to previous research, we completely abstract from the particular operations (e.g., zoom, rotation, etc.) and express the labeling problem as a set of time intervals representing the labels' presences, activities, and conflicts. The model's strength is manifested in its simplicity and broad range of applications. In particular, it supports label selection both for map features with fixed position as well as for moving entities (e.g., for tracking vehicles in logistics or air traffic control).Through extensive experiments on OpenStreetMap data, we evaluate our model using algorithms of varying complexity as a case study for navigation systems. Our experiments show that even simple (and thus, fast) algorithms achieve near-optimal solutions in our model with respect to an intuitive objective function.},
	address = {New York, NY, USA},
	articleno = {23},
	author = {Barth, Lukas and Niedermann, Benjamin and N\"{o}llenburg, Martin and Strash, Darren},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996957},
	isbn = {9781450345897},
	keywords = {dynamic maps, label selection, unified labeling models},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Temporal map labeling: a new unified framework with experiments},
	url = {https://doi.org/10.1145/2996913.2996957},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996958,
	abstract = {The Delaunay triangulation is the standard choice for building triangulated irregular networks (TINs) to represent terrain surfaces. However, the Delaunay triangulation is based only on the 2D coordinates of the data points, ignoring their elevation. It has long been recognized that sometimes it may be beneficial to use other, non-Delaunay, criteria to build TINs. Data-dependent triangulations were introduced decades ago to address this. However, they are rarely used in practice, mostly because the optimization of data- dependent criteria often results in triangulations with many thin and elongated triangles. Recently, in the field of computational geometry, higher order Delaunay triangulations (HODTs) were introduced, trying to tackle both issues at the same time-data-dependent criteria and good triangle shape. Nevertheless, most previous studies about them have been limited to theoretical aspects. In this work we present the first extensive experimental study on the practical use of HODTs, as a tool to build data-dependent TINs. We present experiments with two USGS terrains that show that HODTs can give significant improvements over the Delaunay triangulation for the criteria identified as most important for data-dependent triangulations. The resulting triangulations have data-dependent values comparable to those obtained with pure data-dependent approaches, without compromising the shape of the triangles, and are faster to compute.},
	address = {New York, NY, USA},
	articleno = {26},
	author = {Rodr\'{\i}guez, Natalia and Silveira, Rodrigo I.},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996958},
	isbn = {9781450345897},
	keywords = {Delaunay triangulation, data-dependent triangulations, triangulated irregular networks},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Implementing data-dependent triangulations with higher order Delaunay triangulations},
	url = {https://doi.org/10.1145/2996913.2996958},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996973,
	abstract = {The Pyspatiotemporalgeom library is a pure-python library implementing spatial data types, spatiotemporal data types for moving regions, and operations to create and analyze those types. The library is available on the Python Package Index (PyPI) and has been downloaded over 18,000 times since its release. In this paper, we demonstrate mechanisms to create random spatial data and perform operations over them. We then show how to create moving regions from existing data, and demonstrate aggregate operations over moving regions.},
	address = {New York, NY, USA},
	articleno = {93},
	author = {McKenney, Mark and Nyalakonda, Niharika and McEvers, Jarrod and Shipton, Mitchell},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996973},
	isbn = {9781450345897},
	keywords = {aggregate operations, component moving regions, moving regions, pyspatiotemporalgeom library},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Pyspatiotemporalgeom: a python library for spatiotemporal types and operations},
	url = {https://doi.org/10.1145/2996913.2996973},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996975,
	abstract = {Crowd-sourced and volunteered information, social media, and participatory sensors are capable of providing real-time activity data. Monitoring these sources in time of relevance and then using them to gather operational knowledge is important during crisis management. Beyond that, it's important to curate this information for geo-spatial research purposes, including land use classification and population occupancy analysis. In this demonstration, we will showcase PlanetSense - a geo-spatial research platform built to harness the existing power of archived data and add to that, the dynamics of heterogeneous real-time streaming data from social media and volunteered sources, seamlessly integrated with sophisticated machine learning algorithms and visualization tools. A demonstration will focus on - 1) Recent initiative emphasizing the need to harness crowd-sources, volunteered, and social media data at scale; 2) Anatomy and insight into data collection workflow. We will show the ability to harvest and process several terabytes of raw data in real-time; 3) A detailed discussion with insight into more than 20 sources of data will be given. These sources include text, sensors, as well as imagery data; 4) PlanetSense's end to end distributed architecture will be discussed with focus on collecting and processing high-volumes of streaming data in a Geo-Data Cloud. Data fusion methods and algorithms for integrating disparate data sources with existing legacy products. Data analytics and machine learning methods for generating operational intelligence on the fly; 5) In addition, PlanetSense "App" platform will be shown with hands-on application enabling interested audience to quickly develop and deploy solutions. 6) Several case studies will be discussed relevant to, land use classification, monitoring transient population, high-resolution occupancy analysis, mapping special events population, ability to uncover global breaking events and reactions in near-real time, ability to track protest, unrest, and monitor other societal turbulences as they happen, and real-time monitoring of infrastructure outages.},
	address = {New York, NY, USA},
	articleno = {94},
	author = {Thakur, Gautam S. and Sparks, Kevin and Li, Roger and Stewart, Robert N. and Urban, Marie L.},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996975},
	isbn = {9781450345897},
	keywords = {big data, social media, volunteered geographic information},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Demonstrating PlanetSense: gathering geo-spatial intelligence from crowd-sourced and social-media data},
	url = {https://doi.org/10.1145/2996913.2996975},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996976,
	abstract = {We provide an efficient algorithm for determining how a road network has evolved over time, given two snapshot instances from different dates. To allow for such determinations across different databases and even against hand-drawn maps, we take a strictly topological approach in this paper, so that we compare road networks based strictly on graph-theoretic properties. Given two road networks of same region from two different dates, our approach allows one to match road network portions that remain intact and also point out added or removed portions. We analyze our algorithm both theoretically, showing that it runs in polynomial time for non-degenerate road networks even though a related problem is NP-complete, and experimentally, using dated road networks from the TIGER/Line archive of the U.S. Census Bureau.},
	address = {New York, NY, USA},
	articleno = {31},
	author = {Goodrich, Michael T. and Gupta, Siddharth and Torres, Manuel R.},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996976},
	isbn = {9781450345897},
	keywords = {conformal matching, isomorphism, map evolution},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {A topological algorithm for determining how road networks evolve over time},
	url = {https://doi.org/10.1145/2996913.2996976},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996980,
	abstract = {In this paper, we investigate state-of-the-art computer vision techniques to perform large scale geolocalization of overhead imagery through image matching. We consider two types of features: scale invariant feature transform and region-based shape features. Since these features can be high dimensional and an image can contain many of them, using them to perform image matching can be computationally expensive. Therefore, we also investigate two methods for performing efficient matching: aggregating the features at the image level using a bag of words framework and using hashing to perform multiple, efficient matches and then aggregating the results. We show that hashing performs better in terms of accuracy but is expensive computationally compared to bag of words. We also show that shape features may be accurate and efficient for small data sets, but they do not scale well to large data sets.},
	address = {New York, NY, USA},
	articleno = {32},
	author = {Divecha, Mehul and Newsam, Shawn},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996980},
	isbn = {9781450345897},
	keywords = {geolocalization, image matching, overhead imagery},
	location = {Burlingame, California},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Large-scale geolocalization of overhead imagery},
	url = {https://doi.org/10.1145/2996913.2996980},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996987,
	abstract = {The popularity of GPS-enabled cellular devices introduced numerous applications, e.g., social networks, micro-blogs, and crowd-powered reviews. These applications produce large amounts of geo-tagged textual data that need to be processed and queried. Nowadays, many complex spatio-textual operators and their matching complex indexing structures are being proposed in the literature to process this spatio-textual data. For example, there exist several complex variations of the spatio-textual group queries that retrieve groups of objects that collectively satisfy certain spatial and textual criteria. However, having complex operators is against the spirit of SQL and relational algebra. In contrast to these complex spatio-textual operators, in relational algebra, simple relational operators are offered, e.g., relational selects, projects, order by, and group by, that are composable to form more complex queries. In this paper, we introduce Atlas, an SQL extension to express complex spatial-keyword group queries. Atlas follows the philosophy of SQL and relational algebra in that it uses simple declarative spatial and textual building-block operators and predicates to extend SQL. Not only that Atlas can represent spatio-textual group queries from the literature, but also it can compose other important queries, e.g., retrieve spatio-textual groups from subsets of object datasets where the selected subset satisfies user-defined relational predicates and the groups of close-by objects contain miss-spelled keywords. We demonstrate that Atlas is able to represent a wide range of spatial-keyword queries that existing indexes and algorithms would not be able to address. The building- block paradigm adopted by Atlas creates room for query optimization, where multiple query execution plans can be formed.},
	address = {New York, NY, USA},
	articleno = {45},
	author = {Mahmood, Ahmed R. and Aref, Walid G. and Aly, Ahmed M. and Tang, Mingjie},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996987},
	isbn = {9781450345897},
	keywords = {query language, relational databases, spatial-keyword},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Atlas: on the expression of spatial-keyword group queries using extended relational constructs},
	url = {https://doi.org/10.1145/2996913.2996987},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996990,
	abstract = {This paper presents a large-scale strip adjustment method for LiDAR mobile mapping data, yielding highly precise maps. It uses several concepts to achieve scalability. First, an efficient graph-based pre-segmentation is used, which directly operates on LiDAR scan strip data, rather than on point clouds. Second, observation equations are obtained from a dense matching, which is formulated in terms of an estimation of a latent map. As a result of this formulation, the number of observation equations is not quadratic, but rather linear in the number of scan strips. Third, the dynamic Bayes network, which results from all observation and condition equations, is partitioned into two sub-networks. Consequently, the estimation matrices for all position and orientation corrections are linear instead of quadratic in the number of unknowns and can be solved very efficiently using an alternating least squares approach.It is shown how this approach can be mapped to a standard key/value MapReduce implementation, where each of the processing nodes operates independently on small chunks of data, leading to essentially linear scalability. Results are demonstrated for a dataset of one billion measured LiDAR points and 278,000 unknowns, leading to maps with a precision of a few millimeters.},
	address = {New York, NY, USA},
	articleno = {27},
	author = {Brenner, Claus},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996990},
	isbn = {9781450345897},
	keywords = {LiDAR, MapReduce, least squares adjustment, mobile mapping},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Scalable estimation of precision maps in a MapReduce framework},
	url = {https://doi.org/10.1145/2996913.2996990},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996997,
	abstract = {Geotagged imagery, from satellite, aerial, and ground-level cameras, provides a rich record of how the appearance of scenes and objects differ across the globe. Modern web- based mapping software makes it easy to see how different places around the world look, both from satellite and ground-level views. Unfortunately, interfaces for exploring how the appearance of objects depend on geographic location are quite limited. In this work, we focus on a particularly common object, the human face, and propose learning generative models that relate facial appearance and geographic location. We train these models using a novel dataset of geotagged face imagery we constructed for this task. We present qualitative and quantitative results that demonstrate that these models capture meaningful trends in appearance. We also describe a framework for constructing a web-based visualization that captures the geospatial distribution of human facial appearance.},
	address = {New York, NY, USA},
	articleno = {49},
	author = {Bessinger, Zachary and Stauffer, Chris and Jacobs, Nathan},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996997},
	isbn = {9781450345897},
	keywords = {facial appearance modeling, geotagged imagery, web-based mapping},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Who goes there? approaches to mapping facial appearance diversity},
	url = {https://doi.org/10.1145/2996913.2996997},
	year = {2016},
}

@inproceedings{10.1145/2996913.2996998,
	abstract = {Given a spatial field and the traffic flow between neighboring locations, the early detection of gathering events (edge) problem aims to discover and localize a set of most likely gathering events. It is important for city planners to identify emerging gathering events which might cause public safety or sustainability concerns. However, it is challenging to solve the edge problem due to numerous candidate gathering footprints in a spatial field and the non-trivial task to balance pattern quality and computational efficiency. Prior solutions to model the edge problem lack the ability to describe the dynamic flow of traffic and the potential gathering destinations because they rely on static or undirected footprints. In contrast, in this paper, we model the footprint of a gathering event as a Gathering directed acyclic Graph (G-Graph), where the root of the G-Graph is the potential destination and the directed edges represent the most likely paths traffic takes to move towards the destination. We also proposed an efficient algorithm called SmartEdge to discover the most likely non-overlapping G-Graphs in the given spatial field. Our analysis shows that the proposed G-Graph model and the SmartEdge algorithm have the ability to efficiently and effectively capture important gathering events from real-world human mobility data. Our experimental evaluations show that SmartEdge saves 50\% computation time over the baseline algorithm.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Zhou, Xun and Khezerlou, Amin Vahedian and Liu, Alex and Shafiq, Zubair and Zhang, Fan},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2996998},
	isbn = {9781450345897},
	keywords = {early detection, gathering event, spatial data mining},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {A traffic flow approach to early detection of gathering events},
	url = {https://doi.org/10.1145/2996913.2996998},
	year = {2016},
}

@inproceedings{10.1145/2996913.2997000,
	abstract = {Real-time estimation of human mobility following a massive disaster will play a crucial role in disaster relief. Because human mobility in massive disasters is quite different from their usual mobility, real-time human location data is necessary for precise estimation. Due to privacy concerns, real-time data is anonymized and a popular form of anonymization is population distribution. In this paper, we aim to estimate human mobility following an unprecedented disaster using such population distribution data. To overcome technical obstacles including high dimensionality, we propose novel particle filter by devising proposal distribution. Our proposal distribution provides states considering both prediction model and acquired observation. Therefore, particles maintain high likelihood. In the experiments, our methods realized more accurate estimation than the baselines, and its estimated mobility was consistent with the survey researches. The computational cost is significantly low enough for real-time operations. The GPS data collected on the day of the Great East Japan Earthquake is used for the evaluation.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Sudo, Akihito and Kashiyama, Takehiro and Yabe, Takahiro and Kanasugi, Hiroshi and Song, Xuan and Higuchi, Tomoyuki and Nakano, Shin'ya and Saito, Masaya and Sekimoto, Yoshihide},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2997000},
	isbn = {9781450345897},
	keywords = {GPS data, bayesian inference, disaster management, human mobility prediction},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Particle filter for real-time human mobility prediction following unprecedented disaster},
	url = {https://doi.org/10.1145/2996913.2997000},
	year = {2016},
}

@inproceedings{10.1145/2996913.2997006,
	abstract = {Pursuing criminal activity is tied with understanding illegal or unlawful actions taken on opportunity within a geographic location. Mapping such activities can aid significantly in determining the health of a region, and the vicissitudes of civilian life. Methods to track crime and criminal activity after the fact by mapping news reports of it to geographic locations using the NewsStand system are discussed. NewsStand provides a map-query interface to monitor over 10,000 RSS news sources and making them available within minutes after publication. NewsStand was designed to collect event data given keywords centered on locations specified textually and mapping these locations to their spatial representation, a procedure called geotagging. The goal is to demonstrate how to detect and classify criminal activity by geotagging keywords pertaining to crime, and, in effect, to enhance the capabilities of NewsStand to explicitly show this category of news. The resulting system is named "CrimeStand".},
	address = {New York, NY, USA},
	articleno = {81},
	author = {Wajid, Faizan and Samet, Hanan},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2997006},
	isbn = {9781450345897},
	keywords = {GIS, NewsStand, geotagging, text mining},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {CrimeStand: spatial tracking of criminal activity},
	url = {https://doi.org/10.1145/2996913.2997006},
	year = {2016},
}

@inproceedings{10.1145/2996913.2997007,
	abstract = {In this paper, we present MoTrIS, a service-oriented framework which enables spatio-temporal query processing on multimodal networks that are composed of a road network and one or more schedule-based transportation networks. MoTrIS provides a remote access API, which allows for the development of applications that require the processing of routing queries on multimodal networks. We discuss the architecture of MoTrIS and we elaborate on each of its individual components. The data input module allows for the import of data from various sources into a spatial-enabled relational database. The network module builds a multimodal network by combining a road network with one or more transportation networks. The timetable module stores and queries the schedule for each transportation mode. The query processing module enables the execution of queries over the multimodal network. The visualization module exports the results into a visualizable format. Finally, we present a web application which allows users to create, modify and test advanced spatio-temporal services, and we demonstrate all the necessary steps for a user to build such a new service.},
	address = {New York, NY, USA},
	articleno = {82},
	author = {Chondrogiannis, Theodoros and Gamper, Johann and Cavaliere, Roberto and Ohnewein, Patrick},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2997007},
	isbn = {9781450345897},
	keywords = {multimodal networks, query services, route planning},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {MoTrIS: a framework for route planning on multimodal transportation networks},
	url = {https://doi.org/10.1145/2996913.2997007},
	year = {2016},
}

@inproceedings{10.1145/2996913.2997013,
	abstract = {Indexing and delivering spatial data to a massive user base composed of over a billion devices around the world stretches the limits of traditional infrastructure and operational tools. For instance, offline bulk indexing and loading fall short of viable solutions when it comes to data at scale; Integration with distributed systems such as Apache Hadoop© or Spark© is sparse, while data loading is often performed in a sub-optimal fashion by relying on intermediate file formats.We present in this paper an approach toward a hybrid on- line/offline indexing framework called Gloria that has been running in production settings for the past year at over 350k requests per seconds with lookup latencies under 5μs. The resulting output is an in-memory key-value store and we show that by leveraging higher level MapReduce [7] constructs as defined in FlumeJava [5], Gloria can achieve large scale key-value offline indexing in a fraction of the time required by traditional datastores while maintaining similar operational performance. Gloria also provides a spatial layer based on improvements to pointer-less quadtrees [12] and locational identifiers we call shift key that reduces the nearest neighbor problem in spatial data to simple key-value lookups. Shift keys have shown to outperform well established solutions such as Google S2 with locational key operations.},
	address = {New York, NY, USA},
	articleno = {43},
	author = {Kachemir, Rachid and Kellett, Brad and Behara, Krishna},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2997013},
	isbn = {9781450345897},
	keywords = {distributed bulk indexing, key value storage, locational identifiers, offline indexing, spatial index},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {Gloria: a batch friendly indexing and storage framework},
	url = {https://doi.org/10.1145/2996913.2997013},
	year = {2016},
}

@inproceedings{10.1145/2996913.2997014,
	abstract = {Large scale disasters cause severe social disorder and trigger mass evacuation activities. Managing the evacuation shelters efficiently is crucial for disaster management. Kumamoto prefecture, Japan, was hit by an enormous (Magnitude 7.3) earthquake on 16th of April, 2016. As a result, more than 10,000 buildings were severely damaged and over 100,000 people had to evacuate from their homes. After the earthquake, it took the decision makers several days to grasp the locations where people were evacuating, which delayed of distribution of supply and rescue. This situation was made even more complex since some people evacuated to places that were not designated as evacuation shelters. Conventional methods for grasping evacuation hotspots require on-foot field surveys that take time and are difficult to execute right after the hazard in the confusion.We propose a novel framework to efficiently estimate the evacuation hotspots after large disasters using location data collected from smartphones. To validate our framework and show the useful analysis using our output, we demonstrated the framework on the Kumamoto earthquake using GPS data of smartphones collected by Yahoo Japan. We verified that our estimation accuracy of evacuation hotspots were very high by checking the located facilities and also by comparing the population transition results with newspaper reports. Additionally, we demonstrated analysis using our framework outputs that would help decision makers, such as the population transition and function period of each hotspot. The efficiency of our framework is also validated by checking the processing time, showing that it could be utilized efficiently in disasters of any scale. Our framework provides useful output for decision makers that manage evacuation shelters after various kinds of large scale disasters.},
	address = {New York, NY, USA},
	articleno = {44},
	author = {Yabe, Takahiro and Tsubouchi, Kota and Sudo, Akihito and Sekimoto, Yoshihide},
	booktitle = {Proceedings of the 24th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/2996913.2997014},
	isbn = {9781450345897},
	keywords = {disaster management, evacuation hotspot detection, human mobility, location data, urban computing},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPACIAL '16},
	title = {A framework for evacuation hotspot detection after large scale disasters using location data from smartphones: case study of Kumamoto earthquake},
	url = {https://doi.org/10.1145/2996913.2997014},
	year = {2016},
}

@inproceedings{10.1145/3003421.3003423,
	abstract = {The main objectives of moving objects queries are to search for objects that either lie in some specific areas (i.e., range queries) or are close to one specific location (i.e., kNN queries). Such queries have been previously studied considering either offline database processes using some index techniques or online approaches where incoming data are processed to answer those queries "on the fly". The research presented in this paper considers hybrid queries applied to historical data as well as streaming data. When considering the specific context of the maritime domain and moving objects at sea, a key issue is to make a difference between covered and non covered areas (i.e., regions from where AIS positioning signals are either received or not received). This leads us to introduce the concept of "Black Holes" query where the objective is to identify regions respectively covered and non covered, this providing useful insights for maritime authorities in charge of the regulation of maritime transportation.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Salmon, Loic and Ray, Cyril and Claramunt, Christophe},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on GeoStreaming},
	doi = {10.1145/3003421.3003423},
	isbn = {9781450345798},
	keywords = {black holes, hybrid processing, maritime monitoring, moving objects},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWGS '16},
	title = {Continuous detection of black holes for moving objects at sea},
	url = {https://doi.org/10.1145/3003421.3003423},
	year = {2016},
}

@inproceedings{10.1145/3003421.3003430,
	abstract = {In this paper we systematically explore aggregate operations over moving regions. We propose new aggregate operations and organize the operations according to their spatial, temporal, or spatiotemporal focus.},
	address = {New York, NY, USA},
	articleno = {3},
	author = {McKenney, Mark and Khobrani, Khalil and Rangaraju, Prabhavi},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on GeoStreaming},
	doi = {10.1145/3003421.3003430},
	isbn = {9781450345798},
	keywords = {aggregate operations, moving regions, region streams},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {IWGS '16},
	title = {Categorizing spatiotemporal aggregates for moving regions},
	url = {https://doi.org/10.1145/3003421.3003430},
	year = {2016},
}

@inproceedings{10.1145/3003464.3003467,
	abstract = {The work in this paper is motivated from two different perspectives: First, gazetteers as an important data source for Geographic Information Retrieval (GIR) applications often lack historic place name information. More focused historic gazetteers are a far cry from being complete and often specialize only on certain geographic regions or time periods. Second, research on historic route descriptions---so called itineraries---is an important task in many research disciplines such as geography, linguistics, history, religion, or even medicine. This research on historic itineraries is characterized by manual, time-consuming work with only minimalistic IT support through gazetteers and map services.We address both perspectives and present a depth-first branch-and-bound (DFBnB) algorithm for deducing historic place names and thus the stops of ancient travel routes from itinerary tables. Multiple phonetic and character-based string distances are evaluated when resolving parts of an itinerary first published in 1563.},
	address = {New York, NY, USA},
	articleno = {3},
	author = {Blank, Daniel and Henrich, Andreas},
	booktitle = {Proceedings of the 10th Workshop on Geographic Information Retrieval},
	doi = {10.1145/3003464.3003467},
	isbn = {9781450345880},
	keywords = {depth-first branch-and-bound, gazetteer enrichment, geo-coding, historic place name disambiguation, itinerary resolution},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {GIR '16},
	title = {A depth-first branch-and-bound algorithm for geocoding historic itinerary tables},
	url = {https://doi.org/10.1145/3003464.3003467},
	year = {2016},
}

@inproceedings{10.1145/3003464.3003471,
	abstract = {Ongoing initiatives promoted by cultural institutions and public administrations engage in the development of textual corpora issued from the general public. In this work, we deal with a spoken corpus of life stories and a crowd-sourced Web corpus of people's contributions related to urban planning issues in their city. Located information constitutes an essential component in these corpora. Toponyms refer to official names (e.g. Congo) which are listed in gazetteers but often to generic locations such as un endroit tr\`{e}s beau (a beautiful place). Because of the nature of the corpora, these generic locations are inherently subjective, vague and descriptive. For enabling automated exploitation of these texts, it is crucial to properly detect such kinds of place mentions. In this sense, the present work provides a comparative study of state-of-art NER1 systems, most importantly of supervised tools such as Stanford NER, for the identification of generic locations in thematic corpora.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Brando, Carmen and Domingu\`{e}s, Catherine and Capeyron, Magali},
	booktitle = {Proceedings of the 10th Workshop on Geographic Information Retrieval},
	doi = {10.1145/3003464.3003471},
	isbn = {9781450345880},
	keywords = {NER, crowd-sourced corpus, generic part, life story, located object, spoken corpus, subjective toponym, toponym},
	location = {Burlingame, California},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {GIR '16},
	title = {Evaluation of NER systems for the recognition of place mentions in French thematic corpora},
	url = {https://doi.org/10.1145/3003464.3003471},
	year = {2016},
}

@inproceedings{10.1145/3003819.3003821,
	abstract = {Given a set of points in two dimensional space, statistically significant hotspot detection aims to detect locations where the concentration of points inside the hotspot is much higher than outside. Statistically significant hotspot detection is an important task in application domains such as epidemiology, ecology, criminology, etc. where it may reveal interesting information for domain experts. However, significant hotspot detection is challenging because of a lack of a generic technique for different hotspot patterns (i.e. shapes) and thus a large number of candidate hotspots to be enumerated and tested. Previous hotspot detection techniques focus on specific shapes (e.g. circles, rectangles, ellipses) to identify hotspot areas, but they cannot be used interchangeably which cause a vast variety of complicated and sometimes confusing techniques for each individual hotspot pattern. For example, a circular hotspot detection technique can not be used to discover a rectangular or an elliptical hotspot. In this paper, we propose a generic dual grid based pruning approach for hotspot detection that can be used for different hotspot patterns. We also present a cost analysis, a simplified experiment on the dataset size and a case study on a synthetic dataset to show the applicability of our proposed approach to circular hotspots.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Eftelioglu, Emre},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL PhD Symposium},
	doi = {10.1145/3003819.3003821},
	isbn = {9781450345842},
	keywords = {hotspot detection, spatial data mining, spatial scan statistics, statistical significance},
	location = {Burlingame, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL PhD '16},
	title = {A generic dual grid pruning approach for significant hotspot detection},
	url = {https://doi.org/10.1145/3003819.3003821},
	year = {2016},
}

@inproceedings{10.1145/3003965.3003970,
	abstract = {Understanding human mobility is important for planning and delivering various services in urban area. An important element for mobility understanding is to understand the context in which the movement takes place. In this direction, we propose a method for identifying significant locations and time periods along a bus route. The significance is based on special characteristics that locations have during specific time periods as determined from the effect of these locations on the movement of the bus. The method extracts discriminative features from the space, time and other selected attributes and then classifies locations and time periods into five significance classes. The classes are then rendered in different views for discovering and understanding patterns. The novelty of the method is an explicit consideration of the time dimension at different granularity levels and a visualization that facilitates comparison across the space and time dimensions while avoiding a visual clutter. We demonstrate the applicability of our approach by applying it on a large set of bus trajectories.},
	address = {New York, NY, USA},
	author = {Mazimpaka, Jean Damasc\`{e}ne and Timpf, Sabine},
	booktitle = {Proceedings of the 9th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3003965.3003970},
	isbn = {9781450345774},
	keywords = {bus stop, classification, data mining, movement data, spatiotemporal analysis, spatiotemporal context, visual analysis, visualization},
	location = {Burlingame, California},
	numpages = {6},
	pages = {43–48},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '16},
	title = {A visual and computational analysis approach for exploring significant locations and time periods along a bus route},
	url = {https://doi.org/10.1145/3003965.3003970},
	year = {2016},
}

@inproceedings{10.1145/3003965.3003972,
	abstract = {OpenStreetMap (OSM) is a free, open-source and popular mapping service. However, due to various reasons, it doesn't offer live traffic information or traffic prediction for China. This paper presents an approach and a system to learn a prediction model from graphical traffic condition data provided by Baidu Map, which is a commercial, close-source map provider in China, and apply the model on OSM so that one can predict the traffic conditions with nearly 90\% accuracy in various parts of Shanghai, China, even though no traffic data is available for that area from Baidu Map. This novel system can be useful in urban planning, transportation dispatching as well as personal travel planning.},
	address = {New York, NY, USA},
	author = {Xu, Frank F. and Lin, Bill Y. and Lu, Qi and Huang, Yifei and Zhu, Kenny Q.},
	booktitle = {Proceedings of the 9th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3003965.3003972},
	isbn = {9781450345774},
	location = {Burlingame, California},
	numpages = {6},
	pages = {37–42},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '16},
	title = {Cross-region traffic prediction for China on OpenStreetMap},
	url = {https://doi.org/10.1145/3003965.3003972},
	year = {2016},
}

@inproceedings{10.1145/3003965.3003974,
	abstract = {This paper investigates an on-demand spatial service broker for suggesting service provider propositions and the corresponding estimated waiting times to mobile consumers while meeting the consumer's maximum travel distance and waiting time constraints. The goal of the broker is to maximize the number of matched requests while also keeping the "ecosystem" functioning by engaging many service providers and balancing their assigned requests to provide them with incentives to stay in the system. This problem is important because of its many related societal applications in the on-demand and sharing economy (e.g. on-demand ride hailing services, on-demand food delivery, etc). Challenges of this problem include the need to satisfy many conflicting requirements for the broker, consumers and service providers and the high computational complexity for a large number of consumers and service providers. Related work in spatial crowdsourcing and ridesharing has mainly focused on maximizing the number of matched requests and minimizing travel cost, but did not consider the importance of engaging more service providers and balancing their assignments, which could become a priority when the available supply exceeds the demand. In this work, we propose a new category of service provider centric heuristics for meeting these conflicting requirements. We evaluated our algorithms using synthetic datasets with real-world characteristics. Experimental results show that our proposed heuristics can achieve a larger number of matched requests when supply and demand are balanced. They also engage a larger number of service providers with a more balanced provider assignment when the available supply greatly exceeds demand.},
	address = {New York, NY, USA},
	author = {Ali, Reem Y. and Eftelioglu, Emre and Shekhar, Shashi and Athavale, Shounak and Marsman, Eric},
	booktitle = {Proceedings of the 9th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3003965.3003974},
	isbn = {9781450345774},
	keywords = {on-demand services, on-demand spatial service broker, supply-demand matching},
	location = {Burlingame, California},
	numpages = {6},
	pages = {7–12},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '16},
	title = {Supply-demand ratio and on-demand spatial service brokers: a summary of results},
	url = {https://doi.org/10.1145/3003965.3003974},
	year = {2016},
}

@inproceedings{10.1145/3004725.3004733,
	abstract = {Trajectory analysis is a central problem in the era of big data due to numerous interconnected mobile devices generating unprecedented amounts of spatio-temporal trajectories. Unfortunately, datasets of spatial trajectories are quite difficult to analyse because of the computational complexity of the various existing distance measures. A significant amount of work in comparing two trajectories stems from calculating temporal alignments of the involved spatial points. With this paper, we propose an alignment-free method of representing spatial trajectories using low-dimensional feature vectors by summarizing the combinatorics of shape-derived string sequences. Therefore, we propose to translate trajectories into strings describing the evolving shape of each trajectory, and then provide a sparse matrix representation of these strings using frequencies of adjacencies of characters (n-grams). The final feature vectors are constructed by approximating this matrix with low-dimensional column space using singular value decomposition. New trajectories can be projected into this geometry for comparison. We show that this construction leads to low-dimensional feature vectors with surprising expressive power. We illustrate the usefulness of this approach in various datasets.},
	address = {New York, NY, USA},
	author = {Werner, Martin and Kiermeier, Marie},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
	doi = {10.1145/3004725.3004733},
	isbn = {9781450345828},
	keywords = {big data, moving objects, multi-modal trajectory, trajectory},
	location = {Burlingame, California},
	numpages = {8},
	pages = {19–26},
	publisher = {Association for Computing Machinery},
	series = {MobiGIS '16},
	title = {A low-dimensional feature vector representation for alignment-free spatial trajectory analysis},
	url = {https://doi.org/10.1145/3004725.3004733},
	year = {2016},
}

@inproceedings{10.1145/3004725.3004734,
	abstract = {We study five existing map construction algorithms, designed and tested with urban vehicle data in mind, and apply them to hiking trajectories with different terrain characteristics. Our main goal is to better understand the existing algorithms and to what extent they apply in a wider context. Indeed, our data differs from the one previously used to evaluate map construction algorithm in several aspects: higher GPS error, narrow and winding paths, and trajectories with its own characteristics in terms of speed or direction.We have chosen four different areas of varied geographic features. For each of them we have considered a set of hiking GPS trajectories, each with a total number of nodes between 38,000 and 288,000. For each algorithm we have analyzed the parameters it uses, and adjusted them to each data set. We present an analysis of the generated maps produced by each algorithm on each data set, and a discussion of the most important artifacts detected. We consider that this analysis sheds new light into the current challenges for map construction algorithms, and will be of help for designing new and better methods.},
	address = {New York, NY, USA},
	author = {Duran, David and Sacrist\'{a}n, Vera and Silveira, Rodrigo I.},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Mobile Geographic Information Systems},
	doi = {10.1145/3004725.3004734},
	isbn = {9781450345828},
	keywords = {GPS tracks, hiking data, map construction, trajectories},
	location = {Burlingame, California},
	numpages = {10},
	pages = {74–83},
	publisher = {Association for Computing Machinery},
	series = {MobiGIS '16},
	title = {Map construction algorithms: an evaluation through hiking data},
	url = {https://doi.org/10.1145/3004725.3004734},
	year = {2016},
}

@inproceedings{10.1145/3005422.3005425,
	abstract = {A practical WiFi-based positioning system has to be adaptable to the variations of indoor environmental dynamic factors. In this work, we propose a novel Wi-Fi indoor positioning and tracking framework which employs the spatial analysis and image processing techniques. The Wi-Fi surfaces can be dynamically constructed and updated and thus help to address the challenges of signal spatial heterogeneity and environmental variations. A mobile app for indoor positioning application has been developed as a proof of concept. Based on the experiments we conducted at the Esri campus, this method can achieve about 2-meter positioning accuracy. The proposed methodology and theoretical frame-work can guide engineers to implement cost-effective indoor positioning infrastructure, and thus offer insights on future smart campus applications. The introduced spatial analysis and geoprocessing workflow may also bring the attention of GIScientists to make more efforts to conquer the indoor positioning and tracking challenges.},
	address = {New York, NY, USA},
	author = {Gao, Song and Prasad, Sathya},
	booktitle = {Proceedings of the Eighth ACM SIGSPATIAL International Workshop on Indoor Spatial Awareness},
	doi = {10.1145/3005422.3005425},
	isbn = {9781450345859},
	keywords = {indoor positioning, spatial analysis, wifi fingerprinting},
	location = {Burlingame, California},
	numpages = {8},
	pages = {27–34},
	publisher = {Association for Computing Machinery},
	series = {ISA '16},
	title = {Employing spatial analysis in indoor positioning and tracking using wi-fi access points},
	url = {https://doi.org/10.1145/3005422.3005425},
	year = {2016},
}

@inproceedings{10.1145/3006299.3006314,
	abstract = {Given the recent advancement in the ubiquitous positioning technologies, it is now common to query terabytes of spatial data. These massive data are usually geo-distributed across multiple data centers to ensure their availability. Yet, at least one replica of the data is stored close to where the data are generated. Spatial queries are complex and computationally intensive, and therefore, distributed computation platforms, such as Hadoop are now used to improve their execution time. However, Hadoop is agnostic to the spatial data characteristics, and it randomly partitions and locates the data stored in its distributed file system which degrades the performance of the execution of spatial queries. In this paper, we propose CoS-HDFS, an extension to the Hadoop Distributed File System (HDFS) that takes into account the spatial characteristics of the data and accordingly co-locates them on the HDFS nodes that span multiple data centers. We integrate CoS-HDFS with SpatialHadoop, a MapReduce framework that natively supports spatial data, to make use of its implementation of spatial indexes, operations, and query interfaces. We experimentally demonstrate significant reduction in the network usage and total execution time in the case of spatial join queries on the TIGER dataset.},
	address = {New York, NY, USA},
	author = {Fahmy, Mariam Malak and Elghandour, Iman and Nagi, Magdy},
	booktitle = {Proceedings of the 3rd IEEE/ACM International Conference on Big Data Computing, Applications and Technologies},
	doi = {10.1145/3006299.3006314},
	isbn = {9781450346177},
	keywords = {spatial data, geo-distribution, co-location, HDFS},
	location = {Shanghai, China},
	numpages = {10},
	pages = {123–132},
	publisher = {Association for Computing Machinery},
	series = {BDCAT '16},
	title = {CoS-HDFS: co-locating geo-distributed spatial data in hadoop distributed file system},
	url = {https://doi.org/10.1145/3006299.3006314},
	year = {2016},
}

@inproceedings{10.1145/3006386.3006391,
	abstract = {Big Data has already proven itself as a valuable tool that lets geographers and urban researchers utilize large data resources to generate new insights. However, wider adoption of Big Data techniques in these areas is impeded by a number of difficulties in both knowledge discovery and data and science production. Typically users face such problems as disparate and scattered data, data management, spatial searching, insufficient computational capacity for data-driven analysis and modelling, and the lack of tools to quickly visualize and summarize large data and analysis results. Here we propose an architecture for an Urban Information System (UrbIS) that mitigates these problems by utilizing the Big Data as a Service (BDaaS) concept. With technological roots in High-performance Computing (HPC), BDaaS is based on the idea of outsourcing computations to different computing paradigms, scalable to super-computers. UrbIS aims to incorporate federated metadata search, integrated modeling and analysis, and geovisualization into a single seamless workflow. The system is under active development and is built around various emerging technologies that include hybrid and NoSQL databases, massively parallel systems, GPU computing, and WebGL-based geographic visualization. UrbIS is designed to facilitate the use of Big Data across multiple cities to better understand how urban areas impact the environment and how climate change and other environmental change impact urban areas.},
	address = {New York, NY, USA},
	author = {Sorokine, Alexandre and Karthik, Rajasekar and King, Anthony and Budhendra, Bhaduri},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3006386.3006391},
	isbn = {9781450345811},
	keywords = {big data as a service, environmental change impact, high-performance geocomputing, urban informatics},
	location = {Burlingame, California},
	numpages = {8},
	pages = {34–41},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '16},
	title = {Big data as a service from an urban information system},
	url = {https://doi.org/10.1145/3006386.3006391},
	year = {2016},
}

@inproceedings{10.1145/3006386.3006392,
	abstract = {Spatial data usually encapsulate semantic characterization of features which carry out important meaning and relations among objects, such as the containment between the extension of a region and of its constituent parts. The GeoUML methodology allows one to bring the gap between the definition of spatial integrity constraints at conceptual level and the realization of validation procedures. In particular, it automatically generates SQL validation queries starting from a conceptual specification and using predefined SQL templates. These queries can be used to check data contained into spatial relational databases, such as PostGIS.However, the quality requirements and the amount of available data are considerably growing making unfeasible the execution of these validation procedures. The use of the map-reduce paradigm can be effectively applied in such context since the same test can be performed in parallel on different data chunks and then partial results can be combined together to obtain the final set of violating objects. Pigeon is a data-flow language defined on top of Spatial Hadoop which provides spatial data types and functions. The aim of this paper is to explore the possibility to extend the GeoUML methodology by automatically producing Pigeon validation procedures starting from a set of predefined Pigeon macros. These scripts can be used in a map-reduce environment in order to make feasible the validation of large datasets.},
	address = {New York, NY, USA},
	author = {Migliorini, Sara and Belussi, Alberto and Negri, Mauro and Pelagatti, Giuseppe},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3006386.3006392},
	isbn = {9781450345811},
	keywords = {big data, map-reduce, spatial constraints, spatial validation},
	location = {Burlingame, California},
	numpages = {10},
	pages = {18–27},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '16},
	title = {Towards massive spatial data validation with SpatialHadoop},
	url = {https://doi.org/10.1145/3006386.3006392},
	year = {2016},
}

@inproceedings{10.1145/3011141.3011150,
	abstract = {Recent years have witnessed a rapid increase in the use of car navigation systems, which provide drivers with directions to their destinations. However, such systems do not always recommend a route that perfectly matches a driver's intent. Even when drivers intentionally change the driving route from the recommended one to another, most car navigation systems keep recommending or lead them back to the original recommended route. Such recommendations may not adequately reflect a driver's intent. We previously proposed a route recommendation method based on the estimation of a driver's intent by comparing the characteristics of a route selected by a driver and a route not selected by the driver but recommended by the car navigation system. In this study, we propose a method that can consider multiple costs and learn a driver's concept of the values for each cost; in brief, it represents an effective method for learning drivers' route selection preferences. In addition, we describe experimental evaluation results of our proposed method for driving route recommendations and learning drivers' route selection preferences.},
	address = {New York, NY, USA},
	author = {Hamada, Keisuke and Nakajima, Shinsuke and Kitayama, Daisuke and Sumiya, Kazutoshi},
	booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
	doi = {10.1145/3011141.3011150},
	isbn = {9781450348072},
	keywords = {route search, recommender system, difference amplification algorithm, car navigation},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {16–25},
	publisher = {Association for Computing Machinery},
	series = {iiWAS '16},
	title = {Experimental evaluation of method for driving route recommendation and learning drivers' route selection preferences},
	url = {https://doi.org/10.1145/3011141.3011150},
	year = {2016},
}

@inproceedings{10.1145/3011141.3011197,
	abstract = {In recent years, the importance of information on tourism is increasing because inbound tourists have a beneficial economic effect on regions that attract them. In general, the most popular method of obtaining tourism information is through guidebooks, which often consider only famous sightseeing spots. However, attractive spots are not available only in guidebooks. I consider that finding attractive spots on one's own would satisfy tourists. Therefore, I propose an extraction method for finding attractive Anaba (implying well-kept secret) spots based on users' evaluation and number of visitors. I define Anaba spots because they have low name recognition and high evaluation. In this study, I developed a prototype system by using a photo-sharing site to calculate name recognition and user's evaluation, and evaluated the proposed method.},
	address = {New York, NY, USA},
	author = {Kitayama, Daisuke},
	booktitle = {Proceedings of the 18th International Conference on Information Integration and Web-Based Applications and Services},
	doi = {10.1145/3011141.3011197},
	isbn = {9781450348072},
	keywords = {well-kept secret spots, user's evaluation, location based social network, geographic information retrieval},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {12–15},
	publisher = {Association for Computing Machinery},
	series = {iiWAS '16},
	title = {Extraction method for Anaba spots based on name recognition and user's evaluation},
	url = {https://doi.org/10.1145/3011141.3011197},
	year = {2016},
}

@inproceedings{10.1145/3012071.3012079,
	abstract = {In the past few years, texts have become an important spatial data resource, in addition to maps, satellite images and GPS. Electronic written texts used in mediated interactions, especially short messages (SMS, tweets, etc.), have triggered the emergence of new ways of writing. Extracting information from such short messages, which represent a rich source of information and opinion, is highly important due to the new and challenging text style. Short messages are, however, difficult to analyze because of their brief, unstructured and informal nature. The work presented in this paper is aimed at extracting spatial information from two authentic corpora of SMS and tweets in French in order to take advantage of the vast amount of geographical knowledge expressed in diverse natural language texts. We propose a process in which, firstly, we extract new spatial entities (e.g. Monpelier, Montpel are associated with the place name Montpellier). Secondly, we identify new spatial relations that precede these spatial entities (e.g. sur, par, etc.). Finally, we propose a general pattern for discovering spatial relations (e.g. SR+ Preposition). The task is very challenging and complex due to the specificity of short messages language, which is based on weakly standardized modes of writing (lexical creation, massive use of abbreviations, textual variants, etc.). The experiments that were carried out on the two corpora 88milSMS and Tweets highlight the efficiency of our proposed strategy for identifying new kinds of spatial entities and relations.},
	address = {New York, NY, USA},
	author = {Zenasni, Sarah and Kergosien, Eric and Roche, Mathieu and Teisseire, Maguelonne},
	booktitle = {Proceedings of the 8th International Conference on Management of Digital EcoSystems},
	doi = {10.1145/3012071.3012079},
	isbn = {9781450342674},
	keywords = {tweet corpus, spatial relations, spatial entities, similarity measure, n-grams of words, SMS corpus, POS tagging},
	location = {Biarritz, France},
	numpages = {8},
	pages = {189–196},
	publisher = {Association for Computing Machinery},
	series = {MEDES},
	title = {Extracting new spatial entities and relations from short messages},
	url = {https://doi.org/10.1145/3012071.3012079},
	year = {2016},
}

@inproceedings{10.1145/3017611.3017620,
	abstract = {Decision making after an emergency like those after a large-scale disaster (natural/man-made) is often impaired due the non-availability of crisis information from field. The key reason behind such hindrance in getting information off the field is due to disruption/breaking of conventional communication channel (manual or automatic) as an outcome of the crisis event. The post-crisis operations like evacuation, rescue-relief are affected at large due to poor decision making and lack of coordination among the field workers and officials in charge of the emergency management and mitigation. This leads to added suffering to the victims, increased death-toll, and mass agitation, anger and mistrust among all the stake-holders. Humanitarian organizations present crisis mapping services for shaping the rescue-relief activities. The crisis mapping systems collects crisis data from online social media, news feeds, etc., and portrays them through an online map server. However, in a situation when network is disrupted, such services become useless. In this work of ours, we would like to present an application that may run on Android-based mobile devices and could prepare 'localized' crisis map through 'offline' crowd-sourcing of situational data and a distributed processing of the collected data in seamless manner. To ensure that the generated localized crisis map hold the most important information, and that it contains information from almost every corner of the affected area, a novel data dissemination strategy is proposed. For better serving the affected community, the resulting crisis data is portrayed on a nice map interface generated locally, whenever possible. In addition to crisis data, mobility trails of other users, whenever available, are embedded on the same interface for the purpose of travel route suggestion for the users in a changing environment after the crisis.},
	address = {New York, NY, USA},
	articleno = {9},
	author = {Paul, Partha Sarathi and Dutta, Hridoy Sankar and Ghosh, Bishakh Chandra and Hazra, Krishnandu and Chakraborty, Sandip and Saha, Sujoy and Nandi, Subrata},
	booktitle = {Proceedings of the Second ACM SIGSPATIALInternational Workshop on the Use of GIS in Emergency Management},
	doi = {10.1145/3017611.3017620},
	isbn = {9781450345804},
	keywords = {opportunistic communication, offline crowdsourcing, crisis mapping},
	location = {Burlingame, California},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {EM-GIS '16},
	title = {Offline crisis mapping by opportunistic dissemination of crisis data after large-scale disasters},
	url = {https://doi.org/10.1145/3017611.3017620},
	year = {2016},
}

@article{10.1145/3017678,
	abstract = {Improving disaster management and recovery techniques is one of national priorities given the huge toll caused by man-made and nature calamities. Data-driven disaster management aims at applying advanced data collection and analysis technologies to achieve more effective and responsive disaster management, and has undergone considerable progress in the last decade. However, to the best of our knowledge, there is currently no work that both summarizes recent progress and suggests future directions for this emerging research area. To remedy this situation, we provide a systematic treatment of the recent developments in data-driven disaster management. Specifically, we first present a general overview of the requirements and system architectures of disaster management systems and then summarize state-of-the-art data-driven techniques that have been applied on improving situation awareness as well as in addressing users’ information needs in disaster management. We also discuss and categorize general data-mining and machine-learning techniques in disaster management. Finally, we recommend several research directions for further investigations.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Li, Tao and Xie, Ning and Zeng, Chunqiu and Zhou, Wubai and Zheng, Li and Jiang, Yexi and Yang, Yimin and Ha, Hsin-Yu and Xue, Wei and Huang, Yue and Chen, Shu-Ching and Navlakha, Jainendra and Iyengar, S. S.},
	doi = {10.1145/3017678},
	issn = {0360-0300},
	issue_date = {January 2018},
	journal = {ACM Comput. Surv.},
	keywords = {data mining, data management, application, Disaster information management},
	month = {mar},
	number = {1},
	numpages = {45},
	publisher = {Association for Computing Machinery},
	title = {Data-Driven Techniques in Disaster Information Management},
	url = {https://doi.org/10.1145/3017678},
	volume = {50},
	year = {2017},
}

@inproceedings{10.1145/3018896.3025155,
	abstract = {This paper presents the J-Park Simulator (JPS), a virtualisation of an Eco-Industrial Park (EIP). The JPS combines concepts of machine-to-machine (M2M) communication inspired by the Semantic Web and Industry 4.0, and advanced mathematical modelling to create a modelling platform for designing, computer-aided process engineering (CAPE) and managing an EIP. The overall aim is to reduce carbon footprint and maximise resource efficiency by taking advantage of symbiotic inter-company exchanges of material and energy. The paper outlines system architecture, supporting infrastructure, and its components such as database, data processing, data editing and visualisation, and system modelling tools. A cross-domain ontology is used to represent the wealth and complexity of data at multiple levels and across domains united in a shared data and information hub that provides real-time situational awareness of industrial processes. Networks of fast-to-evaluate surrogate models are employed to conduct real time simulations that quantify CO2 emission reduction using, for example, waste heat recovery (WHR), and carry out cross-domain simulations both at steady-state and in transient operation. The cross-domain ontology is furthermore used to answer semantic queries. The approach and some of the benefits of this platform are demonstrated in several case studies. We find that there is significant scope to realise as yet unexploited potential for energy savings.},
	address = {New York, NY, USA},
	articleno = {107},
	author = {Kleinelanghorst, Martin J and Zhou, Li and Sikorski, Janusz and Shyh, Eddy Foo Yi and Aditya, Kevin and Mosbach, Sebastian and Karimi, Iftekhar and Lau, Raymond and Kraft, Markus},
	booktitle = {Proceedings of the Second International Conference on Internet of Things, Data and Cloud Computing},
	doi = {10.1145/3018896.3025155},
	isbn = {9781450347747},
	keywords = {surrogate model, semantic web, ontoCAPE, internet of things, industry 4.0, eco-industrial park, cross-domain ontology},
	location = {Cambridge, United Kingdom},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {ICC '17},
	title = {J-park simulator: roadmap to smart eco-industrial parks},
	url = {https://doi.org/10.1145/3018896.3025155},
	year = {2017},
}

@inproceedings{10.1145/3019612.3019665,
	abstract = {Co-location pattern mining is a spatial data mining technique which can be used to find associations among spatial features. Our work is motivated by an application in environmental health where the goal is to investigate whether the maternal exposure during pregnancy to air pollutants could be potentially associated with adverse birth outcomes. Discovering such relationships can be defined as finding spatial associations (i.e. co-location patterns) between adverse birth outcomes and air pollutant emissions. In particular, our application problem requires to find specific co-location patterns which are common to many spatial groups and co-location patterns which can discriminate one spatial group from the others. Traditional co-location pattern mining methods are not capable of finding such specific patterns. Hence, to achieve the spatial group comparison task, we introduce two new spatial patterns: spatial contrast sets and spatial common sets, and techniques to efficiently mine them based on co-location pattern mining. Traditional co-location pattern mining methods rely on frequency based thresholds which discard rare patterns and find exaggerated noisy patterns which may not be equally prevalent in unseen data. Addressing these limitations, we propose to use statistical significance tests instead of frequency to quantify the strength of a pattern. Towards this end, we propose to apply Fisher's exact test to efficiently find statistically significant co-location rules and use them to discover spatial contrast and common sets. Our experiments reveal that the Fisher's test based method could indeed help in finding co-location patterns with a better statistical significance leading to find valid spatial contrast and common sets. With the proposed methods we discovered that air pollutants such as heavy metals, NO2 and PM are significantly associated with adverse birth outcomes conforming to the existing domain knowledge thus validating our approach. We also evaluated our methods with synthetic datasets which confirmed that our methods indeed extract the patterns we seek to find.},
	address = {New York, NY, USA},
	author = {Shazan, Mohomed and Jabbar, Mohomed and Za\"{\i}ane, Osmar R. and Osornio-Vargas, Alvaro},
	booktitle = {Proceedings of the Symposium on Applied Computing},
	doi = {10.1145/3019612.3019665},
	isbn = {9781450344869},
	keywords = {spatial contrast sets, spatial common sets, co-location patterns},
	location = {Marrakech, Morocco},
	numpages = {8},
	pages = {796–803},
	publisher = {Association for Computing Machinery},
	series = {SAC '17},
	title = {Discovering spatial contrast and common sets with statistically significant co-location patterns},
	url = {https://doi.org/10.1145/3019612.3019665},
	year = {2017},
}

@inproceedings{10.1145/3019612.3019679,
	abstract = {Several works have exploited the geographic information of photos through spatial clustering algorithms aiming at the automatic discovery of points of interest (POIs). The assumption is that dense regions in terms of geographically nearby photos are good POI surrogates. However, this approach fails when: (i) nearby photos point to different POIs, and (ii) POIs lay within a large distance from the camera. In (i) current approaches would erroneously associate nearby photos to the same POI, whereas in (ii) the photos would not be associated to the POI they really point at. In this paper, we propose to address these problems by devising two novel clustering-based strategies that exploit location along-side compass metadata for POI discovery. We use a large collection of geotagged and oriented photos collected from Flickr related to three different cities and show that our approaches can be more accurate than baselines solely based on location metadata.},
	address = {New York, NY, USA},
	author = {Lacerda, Yuri A. and Marinho, Leandro B. and de S. Baptista, Cl\'{a}udio and Renso, Chiara and Perego, Raffaele},
	booktitle = {Proceedings of the Symposium on Applied Computing},
	doi = {10.1145/3019612.3019679},
	isbn = {9781450344869},
	keywords = {point-of-interest detection, point-of-interest, photo clustering, oriented photos, multimedia clustering},
	location = {Marrakech, Morocco},
	numpages = {6},
	pages = {907–912},
	publisher = {Association for Computing Machinery},
	series = {SAC '17},
	title = {Exploiting photo location and direction for clustering-based points-of-interest discovery},
	url = {https://doi.org/10.1145/3019612.3019679},
	year = {2017},
}

@inproceedings{10.1145/3022227.3022301,
	abstract = {Map construction from vehicle trajectories has been an active challenge topic due to the progress of positioning technologies and high cost of map constructions since last decades. In our work, we focus on road network generation from a massive GPS data collected from a large number of vehicles, particularly the detection of intersections which provide the connectivity information of road network. Among several methods to detect intersections, image processing, observing the distribution of GPS points, and finding the behavioral characteristics of trajectories have been widely studied. However, actual roads are in three-dimensional space and there are overpass or underpass roads that can be falsely detected as intersections. In order to solve this problem, we extend our previous algorithm, Virtual Run [16] to the Bidirectional Virtual Run algorithm to detect the split point among roads. Moreover, by reversing the Virtual Run algorithm, this new algorithm can find the joining point of roads. For the evaluation, we used about 2000 actual vehicle trajectories gathered up with taxies in three metropolitan cities - Seoul, Pusan, and Sungnam in Korea.},
	address = {New York, NY, USA},
	articleno = {75},
	author = {Park, Jinkwan and Cho, Hwan-Gue},
	booktitle = {Proceedings of the 11th International Conference on Ubiquitous Information Management and Communication},
	doi = {10.1145/3022227.3022301},
	isbn = {9781450348881},
	keywords = {trajectory analysis, map generation, intersection detection, GPS},
	location = {Beppu, Japan},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {IMCOM '17},
	title = {Virtual running model for locating road intersections using GPS trajectory data},
	url = {https://doi.org/10.1145/3022227.3022301},
	year = {2017},
}

@inproceedings{10.1145/3035918.3054773,
	abstract = {The widespread use of GPS-enabled cellular devices, i.e., smart phones, led to the popularity of numerous mobile applications, e.g., social networks, micro-blogs, mobile web search, and crowd-powered reviews. These applications generate large amounts of geo-tagged textual data, i.e., spatial-keyword data. This data needs to be processed and queried at an unprecedented scale. The management of spatial-keyword data at this scale goes beyond the capabilities of centralized systems. We live in the era of big data and the big data model is currently been used to address scalability issues in various application domains. This has led to the development of various big spatial-keyword processing systems. These systems are designed to ingest, store, index, and query huge amounts of spatial-keyword data. In this 1.5 hour tutorial, we explore recent research efforts in the area of big spatial-keyword processing. First, we give main motivations behind big spatial-keyword systems with real-life applications. We describe the main models for big spatial-keyword processing, and list the popular spatial-keyword queries. Then, we present the approaches that have been adopted in big spatial-keyword processing systems with special attention to data indexing and spatial and keyword data partitioning. Finally, we conclude this tutorial with a discussion on some of the open problems and research directions in the area of big spatial-keyword query processing.},
	address = {New York, NY, USA},
	author = {Mahmood, Ahmed and Aref, Walid G.},
	booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
	doi = {10.1145/3035918.3054773},
	isbn = {9781450341974},
	keywords = {systems, spatial-keyword, query processing, indexing, big data},
	location = {Chicago, Illinois, USA},
	numpages = {6},
	pages = {1777–1782},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '17},
	title = {Query Processing Techniques for Big Spatial-Keyword Data},
	url = {https://doi.org/10.1145/3035918.3054773},
	year = {2017},
}

@inproceedings{10.1145/3035918.3064008,
	abstract = {Ridesharing enables drivers to share any empty seats in their vehicles with riders to improve the efficiency of transportation for the benefit of both drivers and riders. Different from existing studies in ridesharing that focus on minimizing the travel costs of vehicles, we consider that the satisfaction of riders (the utility values) is more important nowadays. Thus, we formulate the problem of utility-aware ridesharing on road networks (URR) with the goal of providing the optimal rider schedules for vehicles to maximize the overall utility, subject to spatial-temporal and capacity constraints. To assign a new rider to a given vehicle, we propose an efficient algorithm with a minimum increase in travel cost without reordering the existing schedule of the vehicle. We prove that the URR problem is NP-hard by reducing it from the 0-1 Knapsack problem and it is unlikely to be approximated within any constant factor in polynomial time through a reduction from the DENS k-SUBGRAPH problem. Therefore, we propose three efficient approximate algorithms, including a bilateral arrangement algorithm, an efficient greedy algorithm and a grouping-based scheduling algorithm, to assign riders to suitable vehicles with a high overall utility. Through extensive experiments, we demonstrate the efficiency and effectiveness of our URR approaches on both real and synthetic data sets.},
	address = {New York, NY, USA},
	author = {Cheng, Peng and Xin, Hao and Chen, Lei},
	booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
	doi = {10.1145/3035918.3064008},
	isbn = {9781450341974},
	keywords = {task scheduling, ridesharing},
	location = {Chicago, Illinois, USA},
	numpages = {14},
	pages = {1197–1210},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '17},
	title = {Utility-Aware Ridesharing on Road Networks},
	url = {https://doi.org/10.1145/3035918.3064008},
	year = {2017},
}

@inproceedings{10.1145/3038912.3052717,
	abstract = {The turn-by-turn directions provided in existing navigation applications are exclusively derived from underlying road network topology information, i.e., the connectivity of edges to each other. Therefore, the turn-by-turn directions are simplified as metric translation of physical world (e.g. distance/time to turn) to spoken language. Such translation - that ignores human cognition of the geographic space - is often verbose and redundant for the drivers who have knowledge about the geographical areas. In this paper, we study a Personalized RoutE Guidance System dubbed PaRE - with which the goal is to generate more customized and intuitive directions based on user generated content. PaRE utilizes a wealth of user generated historical trajectory data to extract namely "landmarks" (e.g., point of interests or intersections) and frequently visited routes between them from the road network. The extracted information is used to obtain cognitive customized directions for each user. We formalize this task as a problem of finding the optimal partition for a given route that maximizes the familiarity while minimizing the number of segments in the partition, and propose two efficient algorithms to solve it. For empirical study, we apply our solution to both real and synthetic trajectory datasets to evaluate the performance and effectiveness of PaRE.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Li, Yaguang and Su, Han and Demiryurek, Ugur and Zheng, Bolong and He, Tieke and Shahabi, Cyrus},
	booktitle = {Proceedings of the 26th International Conference on World Wide Web},
	doi = {10.1145/3038912.3052717},
	isbn = {9781450349130},
	keywords = {trajectory, personalized route guide, gis},
	location = {Perth, Australia},
	numpages = {10},
	pages = {637–646},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '17},
	title = {PaRE: A System for Personalized Route Guidance},
	url = {https://doi.org/10.1145/3038912.3052717},
	year = {2017},
}

@article{10.1145/3040200,
	abstract = {In recent years, thematic route planning is gaining popularity in recreational navigation. A growing number of people start to use route-planning services to prepare, ride, explore, and log their activities, with a particular focus on where they want to ride and what they want to see. In the context of cultural heritage, however, route planners still suffer from lack of data and route weighting/scoring mechanisms to achieve end-user satisfaction. In this article, we take advantage of mobile sensing and geotagging (r)evolution to tackle both issues and propose a novel framework for cultural heritage routing on top of RouteYou’s existing recreational navigation platform. Our first improvement focuses on the automatic collection and multimodal enrichment of thematic cultural heritage points of interest. Second, we introduce a weighting procedure for these points of interest and analyze their meta(data) quality and spatial coverage in our route databases. Finally, we present a novel routing algorithm targeted to cultural heritage exploration. Experimental results show that the proposed framework improves cultural heritage POI coverage and quality with respect to traditional recreational navigation routing algorithms. Furthermore, the proposed framework can easily be used in other thematic routing applications due to its generic architecture, making it a widely applicable approach.},
	address = {New York, NY, USA},
	articleno = {24},
	author = {Baker, Kevin and Verstockt, Steven},
	doi = {10.1145/3040200},
	issn = {1556-4673},
	issue_date = {October 2017},
	journal = {J. Comput. Cult. Herit.},
	keywords = {mobile sensing, geotagging, cultural heritage, Thematic routing},
	month = {jul},
	number = {4},
	numpages = {20},
	publisher = {Association for Computing Machinery},
	title = {Cultural Heritage Routing: A Recreational Navigation-based Approach in Exploring Cultural Heritage},
	url = {https://doi.org/10.1145/3040200},
	volume = {10},
	year = {2017},
}

@inproceedings{10.1145/3041021.3051102,
	abstract = {The world is a big place. At any given instant something is happening somewhere, but even when nothing in particular is going on people still find ways to generate data, such as posting on social media, taking photos, and issuing search queries. A substantial number of these actions is associated with a location, and in an increasingly mobile and connected world (both in terms of people and devices), this number is only bound to get larger. Yet, in the literature we observe that many researchers often unwittingly treat the geospatial dimension as if it were a regular feature dimension, despite it requiring special attention. In order to avoid pitfalls and to steer clear of erroneous conclusions, our tutorial aims to teach researchers and students how geotagged data differs from regular data, and to educate them on best practices when dealing with such data. We will cover the lifecycle of how geotagged data is used in research, where the topics range from how it is created, represented, processed, modeled, analyzed, visualized, and perceived. The tutorial requires both passive and active involvement - we not only present the material, but the attendees also get the opportunity to interact with it using a variety of open source data and tools that we have prepared using a virtual machine. Attendees should expect to leave the course with a high-level understanding of methods for properly using geospatial data and reporting results, the necessary context to better understand the geography literature, and resources for further engaging with georeferenced data.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Schifanella, Rossano and Thomee, Bart and Shamma, David A.},
	booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
	doi = {10.1145/3041021.3051102},
	isbn = {9781450349147},
	keywords = {tutorial, spatiotemporality, geotagged data, geospatial data},
	location = {Perth, Australia},
	numpages = {3},
	pages = {927–929},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '17 Companion},
	title = {The Lifecycle of Geotagged Data},
	url = {https://doi.org/10.1145/3041021.3051102},
	year = {2017},
}

@inproceedings{10.1145/3041021.3054150,
	abstract = {Human movement analysis and categorization of mobile users based on their movement semantics are challenging tasks. Further, due to security and privacy issues, insufficient labeled or user-annotated data (or, ground-truth data) makes the user-classification from GPS traces more complex. In this work, we present a framework which models user movement patterns containing both spatio-temporal and semantic information, generates semantic stay-point taxonomy by analysing GPS traces of all users, summarizes individuals' GPS traces and clusters users based on the semantics of their movement patterns. To alleviate labelled data scarcity problem while user categorization in a particular region of interest (ROI), we propose a method to transfer knowledge derived from a set of GPS traces of a geographically distanced but similar type of ROI. An extensive set of experiments using real GPS trace dataset of Kharagpur, India and Dartmouth, Hanover, USA have been carried out to demonstrate the effectiveness of our proposed framework.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Ghosh, Shreya and Ghosh, Soumya K.},
	booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
	doi = {10.1145/3041021.3054150},
	isbn = {9781450349147},
	keywords = {trajectory;gps data, spatial transfer learning, geocoding, geo-tagging, clustering, categorization},
	location = {Perth, Australia},
	numpages = {8},
	pages = {51–58},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '17 Companion},
	title = {Modeling of Human Movement Behavioral Knowledge from GPS Traces for Categorizing Mobile Users},
	url = {https://doi.org/10.1145/3041021.3054150},
	year = {2017},
}

@inproceedings{10.1145/3041021.3054249,
	abstract = {Street names say a lot about a country's or region's identity. So far, they have mostly been analyzed manually and for very limited regions (e.g., a city), and hardly any large-scale studies have been performed automatically. A phenomenon not yet studied are street names with date references. These are of special interest as they can be used to commemorate important events in a region's history. In this paper, we present our approach to automatically extract such street names across the world. We analyze the dates' temporal and geographic distribution, and automatically gather potential explanations why specific dates occur in particular regions. We further describe date-Rome, a tool to interactively explore the streets, their distribution, and possible explanations.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Andrade, Rosita and Str\"{o}tgen, Jannik},
	booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
	doi = {10.1145/3041021.3054249},
	isbn = {9781450349147},
	keywords = {temporal tagging, street names, map-based exploration},
	location = {Perth, Australia},
	numpages = {2},
	pages = {757–758},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '17 Companion},
	title = {All Dates Lead to Rome: Extracting and Explaining Temporal References in Street Names},
	url = {https://doi.org/10.1145/3041021.3054249},
	year = {2017},
}

@inproceedings{10.1145/3041021.3054731,
	abstract = {The proliferation of spatial data and the publication of multidimensional (MD) data on the Semantic Web (SW) has led to new opportunities for On-Line Analytical Processing (SOLAP) over spatial data using SPARQL. However, formulating such queries results in verbose statements and can easily become very difficult for inexperienced users. Hence, we have developed GeoSemOLAP to enable users without detailed knowledge of RDF and SPARQL to query the SW with SOLAP. GeoSemOLAP generates SPARQL queries based on high-level SOLAP operators and allows the user to interactively formulate queries using a graphical interface with interactive maps.},
	address = {Republic and Canton of Geneva, CHE},
	author = {G\"{u}r, Nurefsan and Nielsen, Jacob and Hose, Katja and Pedersen, Torben Bach},
	booktitle = {Proceedings of the 26th International Conference on World Wide Web Companion},
	doi = {10.1145/3041021.3054731},
	isbn = {9781450349147},
	keywords = {spatial olap, sparql, multidimensional rdf data},
	location = {Perth, Australia},
	numpages = {5},
	pages = {213–217},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '17 Companion},
	title = {GeoSemOLAP: Geospatial OLAP on the Semantic Web Made Easy},
	url = {https://doi.org/10.1145/3041021.3054731},
	year = {2017},
}

@inproceedings{10.1145/3041823.3041835,
	abstract = {Of the various applications of remote sensing data, characterizing the land-cover dynamics is of utmost significance, providing insights into science, management policy, and several regulatory actions. Recent research works indicate that there is a need to understand and monitor land-cover dynamics at regional scale rather than local scale. However, the regional change is a more generalized concept and therefore, the use of pixel based analysis alone may not be sufficient to get proper insights regarding the land-cover change in remotely sensed imagery. Moreover, higher spectral variation and mixed pixels are two key challenges imposed by satellite imagery, resulting into poor performance of existing pixel-based methods for regional land-cover change detection. In this work, we have proposed a novel approach for detecting regional land-cover changes in satellite imagery using spatio-temporal autocorrelation analysis. Autocorrelation among the neighborhood pixels at various spatio-temporal lags has been utilized here to address the problem of mixed pixel and spectral variation. An index (γ), based on the estimated autocorrelations, has been proposed to classify the regions as 'change' and 'no-change' regions. Moreover, a parameter (σ) has been introduced to provide the measure of regional change significance. The method has been evaluated with Landsat ETM+ imagery (30m resolution) of four zones in and around Kolkata (India), comprising a total of 430 sq. km area (\~{a} 4.8 \texttimes{} 105 pixels). The experimental results are encouraging, with an overall accuracy of 90.66\%.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Das, Monidipa and Ghosh, Soumya K.},
	booktitle = {Proceedings of the 4th ACM IKDD Conferences on Data Sciences},
	doi = {10.1145/3041823.3041835},
	isbn = {9781450348461},
	keywords = {Spatio-temporal autocorrelation, Remote sensing imagery, Regional change detection, Land-cover, Change significance},
	location = {Chennai, India},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {CODS '17},
	title = {Spatio-temporal Autocorrelation Analysis for Regional Land-cover Change Detection from Remote Sensing Data},
	url = {https://doi.org/10.1145/3041823.3041835},
	year = {2017},
}

@article{10.1145/3046790,
	abstract = {For blind travelers, finding crosswalks and remaining within their borders while traversing them is a crucial part of any trip involving street crossings. While standard Orientation \&amp; Mobility (O&amp;M) techniques allow blind travelers to safely negotiate street crossings, additional information about crosswalks and other important features at intersections would be helpful in many situations, resulting in greater safety and/or comfort during independent travel. For instance, in planning a trip a blind pedestrian may wish to be informed of the presence of all marked crossings near a desired route.We have conducted a survey of several O&amp;M experts from the United States and Italy to determine the role that crosswalks play in travel by blind pedestrians. The results show stark differences between survey respondents from the U.S. compared with Italy: the former group emphasized the importance of following standard O&amp;M techniques at all legal crossings (marked or unmarked), while the latter group strongly recommended crossing at marked crossings whenever possible. These contrasting opinions reflect differences in the traffic regulations of the two countries and highlight the diversity of needs that travelers in different regions may have.To address the challenges faced by blind pedestrians in negotiating street crossings, we devised a computer vision--based technique that mines existing spatial image databases for discovery of zebra crosswalks in urban settings. Our algorithm first searches for zebra crosswalks in satellite images; all candidates thus found are validated against spatially registered Google Street View images. This cascaded approach enables fast and reliable discovery and localization of zebra crosswalks in large image datasets. While fully automatic, our algorithm can be improved by a final crowdsourcing validation. To this end, we developed a Pedestrian Crossing Human Validation web service, which supports crowdsourcing, to rule out false positives and identify false negatives.},
	address = {New York, NY, USA},
	articleno = {11},
	author = {Ahmetovic, Dragan and Manduchi, Roberto and Coughlan, James M. and Mascetti, Sergio},
	doi = {10.1145/3046790},
	issn = {1936-7228},
	issue_date = {December 2017},
	journal = {ACM Trans. Access. Comput.},
	keywords = {visual impairments and blindness, satellite and street-level imagery, crowdsourcing, autonomous navigation, Orientation and mobility},
	month = {apr},
	number = {4},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {Mind Your Crossings: Mining GIS Imagery for Crosswalk Localization},
	url = {https://doi.org/10.1145/3046790},
	volume = {9},
	year = {2017},
}

@article{10.1145/3048385,
	abstract = {Digital maps trawl for real-time updates.},
	address = {New York, NY, USA},
	author = {Edwards, Chris},
	doi = {10.1145/3048385},
	issn = {0001-0782},
	issue_date = {April 2017},
	journal = {Commun. ACM},
	month = {mar},
	number = {4},
	numpages = {2},
	pages = {15–16},
	publisher = {Association for Computing Machinery},
	title = {Digitizing the world},
	url = {https://doi.org/10.1145/3048385},
	volume = {60},
	year = {2017},
}

@article{10.1145/3054132,
	abstract = {Team-based invasion sports such as football, basketball, and hockey are similar in the sense that the players are able to move freely around the playing area and that player and team performance cannot be fully analysed without considering the movements and interactions of all players as a group. State-of-the-art object tracking systems now produce spatio-temporal traces of player trajectories with high definition and high frequency, and this, in turn, has facilitated a variety of research efforts, across many disciplines, to extract insight from the trajectories. We survey recent research efforts that use spatio-temporal data from team sports as input and involve non-trivial computation. This article categorises the research efforts in a coherent framework and identifies a number of open research questions.},
	address = {New York, NY, USA},
	articleno = {22},
	author = {Gudmundsson, Joachim and Horton, Michael},
	doi = {10.1145/3054132},
	issn = {0360-0300},
	issue_date = {March 2018},
	journal = {ACM Comput. Surv.},
	keywords = {sports analysis, spatio-temporal data, spatial subdivision, soccer, performance metrics, network analysis, machine learning, hockey, handball, football, data mining, basketball, american football, Trajectory},
	month = {apr},
	number = {2},
	numpages = {34},
	publisher = {Association for Computing Machinery},
	title = {Spatio-Temporal Analysis of Team Sports},
	url = {https://doi.org/10.1145/3054132},
	volume = {50},
	year = {2017},
}

@inproceedings{10.1145/3055031.3055064,
	abstract = {E-Loc is an indoor localization system, which, through using existing indoor electric wiring, detects occupants' location. While many indoor localization technologies require intensive infrastructural supports, E-Loc obtain locations by injecting a signal into the protected earth line of existing residential power network. Caused by human body inside a room, the electromagnetic character changes can be detected to deduce a resident's location. We evaluate our system through experiments inside multiple rooms and our system is able to reach meter-level accuracy.},
	address = {New York, NY, USA},
	author = {Zhou, Tian and Zhang, Yue and Chen, Xinlei and Zhang, Pei and Zhang, Lin},
	booktitle = {Proceedings of the 16th ACM/IEEE International Conference on Information Processing in Sensor Networks},
	doi = {10.1145/3055031.3055064},
	isbn = {9781450348904},
	keywords = {socket, indoor localization, electric wiring},
	location = {Pittsburgh, Pennsylvania},
	numpages = {2},
	pages = {311–312},
	publisher = {Association for Computing Machinery},
	series = {IPSN '17},
	title = {E-loc: indoor localization through building electric wiring: poster abstract},
	url = {https://doi.org/10.1145/3055031.3055064},
	year = {2017},
}

@inproceedings{10.1145/3055167.3055179,
	address = {New York, NY, USA},
	author = {Sabek, Ibrahim},
	booktitle = {Proceedings of the 2017 ACM International Conference on Management of Data},
	doi = {10.1145/3055167.3055179},
	isbn = {9781450341998},
	keywords = {spatial join, spatial data management, query optimization, mapreduce},
	location = {Chicago, Illinois, USA},
	numpages = {3},
	pages = {28–30},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '17},
	title = {Optimizing Spatial Queries in MapReduce},
	url = {https://doi.org/10.1145/3055167.3055179},
	year = {2017},
}

@article{10.1145/3055598,
	address = {New York, NY, USA},
	author = {Scoic\u{a}, Adrian},
	doi = {10.1145/3055598},
	issn = {1528-4972},
	issue_date = {Spring 2017},
	journal = {XRDS},
	month = {apr},
	number = {3},
	numpages = {2},
	pages = {40–41},
	publisher = {Association for Computing Machinery},
	title = {Mikel Maron A life spent digitizing the sense of direction},
	url = {https://doi.org/10.1145/3055598},
	volume = {23},
	year = {2017},
}

@inproceedings{10.1145/3056662.3056681,
	abstract = {Volunteered Geographic Information (VGI) has been increasingly used in a variety of research studies over the recent years. Foursquare is one of the most commonly used sources of VGI. In this paper we investigate the possibility of using Foursquare data for inferring land use information from land cover classification of satellite imagery. Land cover is derived from satellite imagery by the application of supervised and unsupervised classification techniques. It is then integrated with Foursquare data, which comprises of land use information. The results of our study demonstrate that Foursquare data can be used for inferring up-to-date and reliable land use information from land cover classification of satellite imagery, to a reasonable extent. However the results also suggest that the use of Foursquare dataset as the only additional data source for inferring complete land use information is not a viable approach. But we believe that the combined use of multiple sources of VGI such as Foursquare data, OpenStreetMap data etc. together with remote sensing data would be an efficient way of generating comprehensive land use information.},
	address = {New York, NY, USA},
	author = {Jayanetti, J. A. A. M and Meedeniya, D. A. and Dilini, M. D. N. and Wickramapala, M. H. and Madushanka, J. H.},
	booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications},
	doi = {10.1145/3056662.3056681},
	isbn = {9781450348577},
	keywords = {volunteered geographic information, land cover/land use classification, heterogeneous data sources},
	location = {Bangkok, Thailand},
	numpages = {5},
	pages = {149–153},
	publisher = {Association for Computing Machinery},
	series = {ICSCA '17},
	title = {Enhanced land cover and land use information generation from satellite imagery and foursquare data},
	url = {https://doi.org/10.1145/3056662.3056681},
	year = {2017},
}

@inproceedings{10.1145/3056662.3056711,
	abstract = {Graphic Plotting based on military use has been a hot issue in the aspect of real-time situation research. It plays a very important role in various military fields like operation command. In civil fields, especially in the public security industry, applications based on Police Graphic Symbol Standards starts late but progresses fast. With the maturing of its application in public secrity field, Graphic Plotting for Police needs to be further improved. This paper presents a way to express and transmit Police Graphic Plotting data with SVG technology, designs and develops a Police Graphic Symbol authoring tool and Graphic Symbol publishing software. The software has been verified to be a good solution to Graphic Plotting in public security field.},
	address = {New York, NY, USA},
	author = {Lv, Dekui and Yu, Miao and Song, Jianyu and Qian, Kuidong and Cui, Yanjun},
	booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications},
	doi = {10.1145/3056662.3056711},
	isbn = {9781450348577},
	keywords = {police symbol publish, police graphic plotting, police geographic information system, flex, SVG, SOA},
	location = {Bangkok, Thailand},
	numpages = {7},
	pages = {137–143},
	publisher = {Association for Computing Machinery},
	series = {ICSCA '17},
	title = {Study on police graphic plotting technology based on web},
	url = {https://doi.org/10.1145/3056662.3056711},
	year = {2017},
}

@inproceedings{10.1145/3077136.3080753,
	abstract = {Social coding and open source repositories have become more and more popular. Software developers have various alternatives to contribute themselves to the communities and collaborate with others. However, nowadays there is no effective recommender suggesting developers appropriate repositories in both the academia and the industry. Although existing one-class collaborative filtering (OCCF) approaches can be applied to this problem, they do not consider particular constraints of social coding such as the programming languages, which, to some extent, associate the repositories with the developers. The aim of this paper is to investigate the feasibility of leveraging user programming language preference to improve the performance of OCCF-based repository recommendation. Based on matrix factorization, we propose language-regularized matrix factorization (LRMF), which is regularized by the relationships between user programming language preferences. Extensive experiments have been conducted on the real-world dataset of GitHub. The results demonstrate that our framework significantly outperforms five competitive baselines.},
	address = {New York, NY, USA},
	author = {Jiang, Jyun-Yu and Cheng, Pu-Jen and Wang, Wei},
	booktitle = {Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval},
	doi = {10.1145/3077136.3080753},
	isbn = {9781450350228},
	keywords = {user programming language preference, repository recommendation, open source repository, manifold regularization},
	location = {Shinjuku, Tokyo, Japan},
	numpages = {4},
	pages = {1173–1176},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '17},
	title = {Open Source Repository Recommendation in Social Coding},
	url = {https://doi.org/10.1145/3077136.3080753},
	year = {2017},
}

@inproceedings{10.1145/3078081.3078090,
	abstract = {Cultural heritage institutions have recently started to explore the added value of sharing their data, using Linked Open Data to integrate and enrich metadata of their collections. The catalogue of the Biblioteca Virtual Miguel de Cervantes contains about 200,000 records which were transformed to RDF triples employing basically the RDA vocabulary (Resource Description and Access) to describe the entities. However, further refinements are needed for the recognition and extraction of implicit relationships expressed in natural language, such as geographic locations and dates. In order to facilitate literature researchers in carrying out sophisticated location-based searches, a web-based prototype is proposed. The usability of this web interface were evaluated in a user-based study, the results of which are also reported. The methods applied for the automation of the enrichment process, which build upon open-source software components, are described here.},
	address = {New York, NY, USA},
	author = {Candela, Gustavo and Escobar, Pilar and Marco-Such, Manuel},
	booktitle = {Proceedings of the 2nd International Conference on Digital Access to Textual Cultural Heritage},
	doi = {10.1145/3078081.3078090},
	isbn = {9781450352659},
	keywords = {Semantic Web, Ontology, Metadata Enrichment, Linked Open Data, Interoperability, Cultural Heritage, Bibliographic Data},
	location = {G\"{o}ttingen, Germany},
	numpages = {6},
	pages = {169–174},
	publisher = {Association for Computing Machinery},
	series = {DATeCH2017},
	title = {Semantic Enrichment on Cultural Heritage collections: A case study using geographic information},
	url = {https://doi.org/10.1145/3078081.3078090},
	year = {2017},
}

@inproceedings{10.1145/3078714.3078747,
	abstract = {We present the information retrieval model adopted in the OnToMap Participatory GIS. The model addresses the limitations of keyword-based and category-based search by semantically interpreting the information needs specified in free-text search queries. The model is based on an ontological representation of linguistic and encyclopaedic knowledge, which makes it possible to exploit terms and synonyms occurring in the definitions of concepts to flexibly match the user's and system's terminologies. This feature enables users to query the application using their own vocabulary.},
	address = {New York, NY, USA},
	author = {Ardissono, Liliana and Lucenteforte, Maurizio and Mauro, Noemi and Savoca, Adriano and Voghera, Angioletta and La Riccia, Luigi},
	booktitle = {Proceedings of the 28th ACM Conference on Hypertext and Social Media},
	doi = {10.1145/3078714.3078747},
	isbn = {9781450347082},
	keywords = {participatory gis, ontologies, linked data, information search},
	location = {Prague, Czech Republic},
	numpages = {2},
	pages = {317–318},
	publisher = {Association for Computing Machinery},
	series = {HT '17},
	title = {OnToMap: Semantic Community Maps for Knowledge Sharing},
	url = {https://doi.org/10.1145/3078714.3078747},
	year = {2017},
}

@article{10.1145/3078850,
	abstract = {Given a spatial field and the traffic flow between neighboring locations, the early detection of gathering events (edge) problem aims to discover and localize a set of most likely gathering events. It is important for city planners to identify emerging gathering events that might cause public safety or sustainability concerns. However, it is challenging to solve the edge problem due to numerous candidate gathering footprints in a spatial field and the nontrivial task of balancing pattern quality and computational efficiency. Prior solutions to model the edge problem lack the ability to describe the dynamic flow of traffic and the potential gathering destinations because they rely on static or undirected footprints. In our recent work, we modeled the footprint of a gathering event as a Gathering Graph (G-Graph), where the root of the directed acyclic G-Graph is the potential destination and the directed edges represent the most likely paths traffic takes to move toward the destination. We also proposed an efficient algorithm called SmartEdge to discover the most likely nonoverlapping G-Graphs in the given spatial field. However, it is challenging to perform a systematic performance study of the proposed algorithm, due to unavailability of the ground truth of gathering events. In this article, we introduce an event simulation mechanism, which makes it possible to conduct a comprehensive performance study of the SmartEdge algorithm. We measure the quality of the detected patterns, in a systematic way, in terms of timeliness and location accuracy. The results show that, on average, the SmartEdge algorithm is able to detect patterns within a grid cell away (less than 500 meters) of the simulated events and detect patterns of the simulated events as early as 10 minutes prior to the first arrival to the gathering event.},
	address = {New York, NY, USA},
	articleno = {74},
	author = {Khezerlou, Amin Vahedian and Zhou, Xun and Li, Lufan and Shafiq, Zubair and Liu, Alex X. and Zhang, Fan},
	doi = {10.1145/3078850},
	issn = {2157-6904},
	issue_date = {November 2017},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {spatial data mining, early detection, Gathering event},
	month = {jul},
	number = {6},
	numpages = {24},
	publisher = {Association for Computing Machinery},
	title = {A Traffic Flow Approach to Early Detection of Gathering Events: Comprehensive Results},
	url = {https://doi.org/10.1145/3078850},
	volume = {8},
	year = {2017},
}

@article{10.1145/3078853,
	abstract = {Spatial Crowdsourcing (SC) is a novel platform that engages individuals in the act of collecting various types of spatial data. This method of data collection can significantly reduce cost and turnover time and is particularly useful in urban environmental sensing, where traditional means fail to provide fine-grained field data. In this study, we introduce hyperlocal spatial crowdsourcing, where all workers who are located within the spatiotemporal vicinity of a task are eligible to perform the task (e.g., reporting the precipitation level at their area and time). In this setting, there is often a budget constraint, either for every time period or for the entire campaign, on the number of workers to activate to perform tasks. The challenge is thus to maximize the number of assigned tasks under the budget constraint despite the dynamic arrivals of workers and tasks. We introduce a taxonomy of several problem variants, such as budget-per-time-period vs. budget-per-campaign and binary-utility vs. distance-based-utility. We study the hardness of the task assignment problem in the offline setting and propose online heuristics which exploit the spatial and temporal knowledge acquired over time. Our experiments are conducted with spatial crowdsourcing workloads generated by the SCAWG tool, and extensive results show the effectiveness and efficiency of our proposed solutions.},
	address = {New York, NY, USA},
	articleno = {37},
	author = {Tran, Luan and To, Hien and Fan, Liyue and Shahabi, Cyrus},
	doi = {10.1145/3078853},
	issn = {2157-6904},
	issue_date = {May 2018},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {participatory sensing, online task assignment, crowdsensing, budget constraints, Spatial crowdsourcing, GIS},
	month = {jan},
	number = {3},
	numpages = {26},
	publisher = {Association for Computing Machinery},
	title = {A Real-Time Framework for Task Assignment in Hyperlocal Spatial Crowdsourcing},
	url = {https://doi.org/10.1145/3078853},
	volume = {9},
	year = {2018},
}

@inproceedings{10.1145/3079628.3079665,
	abstract = {Proliferation of GPS-enabled mobile devices has brought a plurality of location-aware applications leveraging the location characteristics in the shared content, like photos and check-ins. While these applications provide contextual and relevant information, they also assume geo-tagged contents to be representative of the geo-bounded characteristics of location. In this paper, however, we show that the characteristics geo-tagged contents capture about a location can vary based on the familiarity of user (sharing the content) with the location. Using a large dataset of geo-tagged photos, we learn descriptive spatial photo characteristics and user temporal-location-familiarity to highlight unique characteristics photos capture of location, which vary significantly if taken by locals versus tourists. We then propose a ranking-approach to find most representative photos for a given city. A user-based evaluation shows photos are more diverse and characteristic of location compared to other popular baselines while being representative of how locals and tourists would describe the city.},
	address = {New York, NY, USA},
	author = {Kumar, Vikas and Bakhshi, Saeideh and Kennedy, Lyndon and Shamma, David A.},
	booktitle = {Proceedings of the 25th Conference on User Modeling, Adaptation and Personalization},
	doi = {10.1145/3079628.3079665},
	isbn = {9781450346351},
	keywords = {tourists, retrieval, location-familiarity, locals, image content},
	location = {Bratislava, Slovakia},
	numpages = {9},
	pages = {131–139},
	publisher = {Association for Computing Machinery},
	series = {UMAP '17},
	title = {Adaptive City Characteristics: How Location Familiarity Changes What Is Regionally Descriptive},
	url = {https://doi.org/10.1145/3079628.3079665},
	year = {2017},
}

@inproceedings{10.1145/3080546.3080548,
	abstract = {The concept of "location" provides one a useful dimension to explore, align, combine, and analyze data. Though one can rely on bespoke GIS systems to conduct their data analyses, we aim to investigate the feasibility of using Semantic Web technologies to leverage the exploration and enrichment of data in CSV files with the vast amount of geographic and geospatial data that are available on the Linked Data Web. In this paper, we propose a lightweight method and set of tools for: uplift - transforming non-RDF resources into RDF documents; creating links between RDF datasets; client-side processing of geospatial functions; and downlift - transforming (enriched) RDF documents back into a non-RDF format. With this approach, people who wish to avail of the spatial dimension in data can do so from their client (e.g., in a browser) without the need to rely on bespoke technology. This could be of great utility for decision makers and scholars, amongst others. We applied our approach on datasets that are hosted on the Irish open data portal, and combined it with authoritative geospatial data made available by Ordnance Survey Ireland (OSi). Albeit aware that our approach cannot compete with specialist tools, we do demonstrate its feasibility. Though currently conducted for enriching datasets hosted on the Irish open data portal, future work will look into broader governance and provenance aspects of geospatial data enriched dataset management.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Debruyne, Christophe and McGlinn, Kris and McNerney, Lorraine and O'Sullivan, Declan},
	booktitle = {Proceedings of the Fourth International ACM Workshop on Managing and Mining Enriched Geo-Spatial Data},
	doi = {10.1145/3080546.3080548},
	isbn = {9781450350471},
	keywords = {ordnance survey Ireland, linked data, interlinking, data enrichment, GeoSPARQL},
	location = {Chicago, Illinois},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {GeoRich '17},
	title = {A lightweight approach to explore, enrich and use data with a geospatial dimension with semantic web technologies},
	url = {https://doi.org/10.1145/3080546.3080548},
	year = {2017},
}

@inproceedings{10.1145/3085504.3085538,
	abstract = {This paper describes a new method to compute isochrones in multimodal spatial networks, which aims at finding a good trade-off between memory usage and runtime. In the past, approaches based on Dijkstra's algorithm have been proposed. For small networks, the entire network is first loaded in main memory, where the network is expanded to determine the isochrone. For large networks that do not fit in main memory, approaches that load the network vertex-by-vertex during the expansion phase have been proposed. They keep the memory footprint minimal, but have to query the database for each node in the isochrone, which can be very time consuming. The method presented in this paper uses tiles (which are well known from interactive online maps) to realize chunk-loading of vertices by utilizing so-called tile regions. This approach significantly reduces the number of database requests, while keeping the memory usage low. Our method is able to compute isochrones even in large networks at a reasonable time. An experimental evaluation shows that the new algorithm clearly outperforms previous competitive approaches such as MINE and MINEX.},
	address = {New York, NY, USA},
	articleno = {33},
	author = {Krismer, Nikolaus and Silbernagl, Doris and Specht, G\"{u}nther and Gamper, Johann},
	booktitle = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management},
	doi = {10.1145/3085504.3085538},
	isbn = {9781450352826},
	keywords = {spatial database, isochrone, algorithms},
	location = {Chicago, IL, USA},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {SSDBM '17},
	title = {Computing Isochrones in Multimodal Spatial Networks using Tile Regions},
	url = {https://doi.org/10.1145/3085504.3085538},
	year = {2017},
}

@inproceedings{10.1145/3091478.3091502,
	abstract = {A growing number of citizens and local governments have embraced the use of Twitter to communicate during natural disasters. Studies have shown that online communications during disasters can be explained using crisis communication taxonomies. However, such taxonomies are broad and general, and offer little insight into the detailed content of the communications. In this paper, we propose a semi-automatic framework to extract and compare, in retrospect, the digital communication footprints of citizens and governments during disasters. These footprints, which characterize the topics discussed during a disaster at different spatio-temporal scales, are computed in an unsupervised manner using topic models, and manually labelled to identify specific issues affecting the population. The end objective is to offer detailed information about issues affecting citizens during natural disasters and to compare these against local governments' communications. We evaluate the framework using Twitter communications from 18 snowstorms (including two blizzards) on the US east coast.},
	address = {New York, NY, USA},
	author = {Hong, Lingzi and Fu, Cheng and Torrens, Paul and Frias-Martinez, Vanessa},
	booktitle = {Proceedings of the 2017 ACM on Web Science Conference},
	doi = {10.1145/3091478.3091502},
	isbn = {9781450348966},
	keywords = {topic models, spatio-temporal modeling, disaster analytics, crisis communication},
	location = {Troy, New York, USA},
	numpages = {10},
	pages = {141–150},
	publisher = {Association for Computing Machinery},
	series = {WebSci '17},
	title = {Understanding Citizens' and Local Governments' Digital Communications During Natural Disasters: The Case of Snowstorms},
	url = {https://doi.org/10.1145/3091478.3091502},
	year = {2017},
}

@inproceedings{10.1145/3093338.3093366,
	abstract = {Earthquake hazard estimation requires systematic investigation of past records as well as fundamental processes that cause the quake. However, detailed long-term records of earthquakes at all scales (magnitude, space and time) are not available. Hence a synthetic method based on first principals could be employed to generate such records to bridge this critical gap of missing data. RSQSim is such a simulator that generates seismic event catalogs for several thousand years at various scales. This synthetic catalog contains rich detail about the earthquake events and associated properties.Exploring this data is of vital importance to validate the simulator as well as to identify features of interest such as quake time histories, conduct analyses such as calculating mean recurrence interval of events on each fault section. This work1 describes and demonstrates a prototype web based visual tool that enables domain scientists and students explore this rich dataset, as well as discusses refinement and streamlining of data management and analysis that is less error prone and scalable.},
	address = {New York, NY, USA},
	articleno = {48},
	author = {Chourasia, A. and Richards-Dinger, K. B. and Dieterich, J. H. and Cui, Y.},
	booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
	doi = {10.1145/3093338.3093366},
	isbn = {9781450352727},
	keywords = {earthquake simulators, data management, Visualization},
	location = {New Orleans, LA, USA},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {PEARC '17},
	title = {Visual Exploration and Analysis of Time Series Earthquake Data},
	url = {https://doi.org/10.1145/3093338.3093366},
	year = {2017},
}

@inproceedings{10.1145/3093338.3093378,
	abstract = {The interdisciplinary field of cyberGIS (geographic information science and systems (GIS) based on advanced cyberinfrastructure) has a major focus on data- and computation-intensive geospatial analytics. The rapidly growing needs across many application and science domains for such analytics based on disparate geospatial big data poses significant challenges to conventional GIS approaches. This paper describes CyberGIS-Jupyter, an innovative cyberGIS framework for achieving data-intensive, reproducible, and scalable geospatial analytics using the Jupyter Notebook based on ROGER - the first cyberGIS supercomputer. The framework adapts the Notebook with built-in cyberGIS capabilities to accelerate gateway application development and sharing while associated data, analytics and workflow runtime environments are encapsulated into application packages that can be elastically reproduced through cloud computing approaches. As a desirable outcome, data-intensive and scalable geospatial analytics can be efficiently developed and improved, and seamlessly reproduced among multidisciplinary users in a novel cyberGIS science gateway environment.},
	address = {New York, NY, USA},
	articleno = {18},
	author = {Yin, Dandong and Liu, Yan and Padmanabhan, Anand and Terstriep, Jeff and Rush, Johnathan and Wang, Shaowen},
	booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing 2017 on Sustainability, Success and Impact},
	doi = {10.1145/3093338.3093378},
	isbn = {9781450352727},
	keywords = {science gateway, geospatial big data, flood mapping, computational reproducibility, CyberGIS},
	location = {New Orleans, LA, USA},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {PEARC '17},
	title = {A CyberGIS-Jupyter Framework for Geospatial Analytics at Scale},
	url = {https://doi.org/10.1145/3093338.3093378},
	year = {2017},
}

@inproceedings{10.1145/3097983.3097985,
	abstract = {Recent years have witnessed the continuous growth of megalopolises worldwide, which makes urban safety a top priority in modern city life. Among various threats, dangerous goods such as gas and hazardous chemicals transported through and around cities have increasingly become the deadly "bomb" we sleep with every day. In both academia and government, tremendous efforts have been dedicated to dealing with dangerous goods transportation (DGT) issues, but further study is still in great need to quantify the problem and explore its intrinsic dynamics in a big data perspective. In this paper, we present a novel system called DGeye, which features a "duet" between DGT trajectory data and human mobility data for risky zones identification. Moreover, DGeye innovatively takes risky patterns as the keystones in DGT management, and builds causality networks among them for pain point identification, attribution and prediction. Experiments on both Beijing and Tianjin cities demonstrate the effectiveness of DGeye. In particular, the report generated by DGeye driven the Beijing government to lay down gas pipelines for the famous Guijie food street.},
	address = {New York, NY, USA},
	author = {Wang, Jingyuan and Chen, Chao and Wu, Junjie and Xiong, Zhang},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3097983.3097985},
	isbn = {9781450348874},
	keywords = {urban safety, pattern mining, intelligent transportation, causal analysis},
	location = {Halifax, NS, Canada},
	numpages = {9},
	pages = {1673–1681},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {No Longer Sleeping with a Bomb: A Duet System for Protecting Urban Safety from Dangerous Goods},
	url = {https://doi.org/10.1145/3097983.3097985},
	year = {2017},
}

@inproceedings{10.1145/3097983.3098018,
	abstract = {Taxi-calling apps are gaining increasing popularity for their efficiency in dispatching idle taxis to passengers in need. To precisely balance the supply and the demand of taxis, online taxicab platforms need to predict the Unit Original Taxi Demand (UOTD), which refers to the number of taxi-calling requirements submitted per unit time (e.g., every hour) and per unit region (e.g., each POI). Predicting UOTD is non-trivial for large-scale industrial online taxicab platforms because both accuracy and flexibility are essential. Complex non-linear models such as GBRT and deep learning are generally accurate, yet require labor-intensive model redesign after scenario changes (e.g., extra constraints due to new regulations). To accurately predict UOTD while remaining flexible to scenario changes, we propose LinUOTD, a unified linear regression model with more than 200 million dimensions of features. The simple model structure eliminates the need of repeated model redesign, while the high-dimensional features contribute to accurate UOTD prediction. We further design a series of optimization techniques for efficient model training and updating. Evaluations on two large-scale datasets from an industrial online taxicab platform verify that LinUOTD outperforms popular non-linear models in accuracy. We envision our experiences to adopt simple linear models with high-dimensional features in UOTD prediction as a pilot study and can shed insights upon other industrial large-scale spatio-temporal prediction problems.},
	address = {New York, NY, USA},
	author = {Tong, Yongxin and Chen, Yuqiang and Zhou, Zimu and Chen, Lei and Wang, Jie and Yang, Qiang and Ye, Jieping and Lv, Weifeng},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3097983.3098018},
	isbn = {9781450348874},
	keywords = {unit original taxi demands, prediction, feature engineering},
	location = {Halifax, NS, Canada},
	numpages = {10},
	pages = {1653–1662},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {The Simpler The Better: A Unified Approach to Predicting Original Taxi Demands based on Large-Scale Online Platforms},
	url = {https://doi.org/10.1145/3097983.3098018},
	year = {2017},
}

@inproceedings{10.1145/3097983.3098094,
	abstract = {Recommender system is one of the most popular data mining topics that keep drawing extensive attention from both academia and industry. Among them, POI (point of interest) recommendation is extremely practical but challenging: it greatly benefits both users and businesses in real-world life, but it is hard due to data scarcity and various context. While a number of algorithms attempt to tackle the problem w.r.t. specific data and problem settings, they often fail when the scenarios change. In this work, we propose to devise a general and principled SSL (semi-supervised learning) framework, to alleviate data scarcity via smoothing among neighboring users and POIs, and treat various context by regularizing user preference based on context graphs. To enable such a framework, we develop PACE (Preference And Context Embedding), a deep neural architecture that jointly learns the embeddings of users and POIs to predict both user preference over POIs and various context associated with users and POIs. We show that PACE successfully bridges CF (collaborative filtering) and SSL by generalizing the de facto methods matrix factorization of CF and graph Laplacian regularization of SSL. Extensive experiments on two real location-based social network datasets demonstrate the effectiveness of PACE.},
	address = {New York, NY, USA},
	author = {Yang, Carl and Bai, Lanxiao and Zhang, Chao and Yuan, Quan and Han, Jiawei},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3097983.3098094},
	isbn = {9781450348874},
	keywords = {semi-supervised learning, recommender systems, neural networks, collaborative filtering},
	location = {Halifax, NS, Canada},
	numpages = {10},
	pages = {1245–1254},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {Bridging Collaborative Filtering and Semi-Supervised Learning: A Neural Approach for POI Recommendation},
	url = {https://doi.org/10.1145/3097983.3098094},
	year = {2017},
}

@inproceedings{10.1145/3097983.3098122,
	abstract = {Spatial item recommendation has become an important means to help people discover interesting locations, especially when people pay a visit to unfamiliar regions. Some current researches are focusing on modelling individual and collective geographical preferences for spatial item recommendation based on users' check-in records, but they fail to explore the phenomenon of user interest drift across geographical regions, i.e., users would show different interests when they travel to different regions. Besides, they ignore the influence of public comments for subsequent users' check-in behaviors. Specifically, it is intuitive that users would refuse to check in to a spatial item whose historical reviews seem negative overall, even though it might fit their interests. Therefore, it is necessary to recommend the right item to the right user at the right location. In this paper, we propose a latent probabilistic generative model called LSARS to mimic the decision-making process of users' check-in activities both in home-town and out-of-town scenarios by adapting to user interest drift and crowd sentiments, which can learn location-aware and sentiment-aware individual interests from the contents of spatial items and user reviews. Due to the sparsity of user activities in out-of-town regions, LSARS is further designed to incorporate the public preferences learned from local users' check-in behaviors. Finally, we deploy LSARS into two practical application scenes: spatial item recommendation and target user discovery. Extensive experiments on two large-scale location-based social networks (LBSNs) datasets show that LSARS achieves better performance than existing state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Wang, Hao and Fu, Yanmei and Wang, Qinyong and Yin, Hongzhi and Du, Changying and Xiong, Hui},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3097983.3098122},
	isbn = {9781450348874},
	keywords = {user interest drift, recommendation, crowd sentiment, check-in behavior},
	location = {Halifax, NS, Canada},
	numpages = {9},
	pages = {1135–1143},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {A Location-Sentiment-Aware Recommender System for Both Home-Town and Out-of-Town Users},
	url = {https://doi.org/10.1145/3097983.3098122},
	year = {2017},
}

@inproceedings{10.1145/3097983.3098168,
	abstract = {Point-of-Interest (POI) demand modeling in urban regions is critical for many applications such as business site selection and real estate investment. While some efforts have been made for the demand analysis of some specific POI categories, such as restaurants, it lacks systematic means to support POI demand modeling. To this end, in this paper, we develop a systematic POI demand modeling framework, named Region POI Demand Identification (RPDI), to model POI demands by exploiting the daily needs of people identified from their large-scale mobility data. Specifically, we first partition the urban space into spatially differentiated neighborhood regions formed by many small local communities. Then, the daily activity patterns of people traveling in the city will be extracted from human mobility data. Since the trip activities, even aggregated, are sparse and insufficient to directly identify the POI demands, especially for underdeveloped regions, we develop a latent factor model that integrates human mobility data, POI profiles, and demographic data to robustly model the POI demand of urban regions in a holistic way. In this model, POI preferences and supplies are used together with demographic features to estimate the POI demands simultaneously for all the urban regions interconnected in the city. Moreover, we also design efficient algorithms to optimize the latent model for large-scale data. Finally, experimental results on real-world data in New York City (NYC) show that our method is effective for identifying POI demands for different regions.},
	address = {New York, NY, USA},
	author = {Liu, Yanchi and Liu, Chuanren and Lu, Xinjiang and Teng, Mingfei and Zhu, Hengshu and Xiong, Hui},
	booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3097983.3098168},
	isbn = {9781450348874},
	keywords = {region demand, point-of-interest, human mobility},
	location = {Halifax, NS, Canada},
	numpages = {9},
	pages = {947–955},
	publisher = {Association for Computing Machinery},
	series = {KDD '17},
	title = {Point-of-Interest Demand Modeling with Human Mobility Patterns},
	url = {https://doi.org/10.1145/3097983.3098168},
	year = {2017},
}

@inproceedings{10.1145/3099023.3099030,
	abstract = {This demo paper describes the semantic query interpretation model adopted in the OnToMap Participatory GIS and presents its benefits to information retrieval and personalized information presentation.},
	address = {New York, NY, USA},
	author = {Ardissono, Liliana and Lucenteforte, Maurizio and Mauro, Noemi and Savoca, Adriano and Voghera, Angioletta and La Riccia, Luigi},
	booktitle = {Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization},
	doi = {10.1145/3099023.3099030},
	isbn = {9781450350679},
	keywords = {user modeling, ontologies, information search},
	location = {Bratislava, Slovakia},
	numpages = {2},
	pages = {101–102},
	publisher = {Association for Computing Machinery},
	series = {UMAP '17},
	title = {Semantic Interpretation of Search Queries for Personalization},
	url = {https://doi.org/10.1145/3099023.3099030},
	year = {2017},
}

@inproceedings{10.1145/3099023.3099087,
	abstract = {The exploration of cultural heritage information is challenged by the fact that most data provided by online resources is fragmented and it is repository or application-centered. In order to address this issue, a data integration approach should be adopted, that makes it possible to generate custom views, focused on the user's information needs, but easily extensible to support the inspection of topically related contents.In this paper, we present a model supporting the management of thematic maps for information exploration, and their integration with query expansion during the interaction with the user. Our model is based on: (i) an ontological domain knowledge representation for describing the meaning of concepts and their semantic relations; (ii) a semantic interpretation model for identifying the concepts referenced in the user's queries. We are experimenting our model in the OnToMap Participatory GIS, which manages interactive community maps for information sharing and participatory decision-making.},
	address = {New York, NY, USA},
	author = {Mauro, Noemi and Ardissono, Liliana},
	booktitle = {Adjunct Publication of the 25th Conference on User Modeling, Adaptation and Personalization},
	doi = {10.1145/3099023.3099087},
	isbn = {9781450350679},
	keywords = {thematic maps, personalization, information search},
	location = {Bratislava, Slovakia},
	numpages = {6},
	pages = {337–342},
	publisher = {Association for Computing Machinery},
	series = {UMAP '17},
	title = {Thematic Maps for Geographical Information Search},
	url = {https://doi.org/10.1145/3099023.3099087},
	year = {2017},
}

@article{10.1145/3100243.3100247,
	abstract = {We investigate the mapping of pet activity using social media. Specifically, we perform cat and dog detection in a large collection of georeferenced images in San Francisco. We compare detection based on keyword search in user-supplied tags to detection based on image content using state-of-the-art deep-learning classification methods. The resulting city-scale spatial distribution of cat and dog activity makes sense based on our knowledge of the region. Our approach represents a general framework for mapping phenomena that are difficult to observe through traditional means.},
	address = {New York, NY, USA},
	author = {Jose, Aaron San and Hernandez, Eduardo},
	doi = {10.1145/3100243.3100247},
	issue_date = {November 2016},
	journal = {SIGSPATIAL Special},
	keywords = {georeferenced images, geographic knowledge discovery},
	month = {may},
	number = {3},
	numpages = {2},
	pages = {5–6},
	publisher = {Association for Computing Machinery},
	title = {City-scale mapping of pets using georeferenced images},
	url = {https://doi.org/10.1145/3100243.3100247},
	volume = {8},
	year = {2017},
}

@inproceedings{10.1145/3102254.3102273,
	abstract = {Road traffic injuries are a critical public health challenge that requires valuable efforts for effective and sustainable prevention. Worldwide, an estimated 1.2 million people are killed in road crashes each year and as many as 50 million are injured. An analysis of data provided by authoritative sources can be a valuable source for understanding which are the most critical points on the road network. The aim of this paper is to discover data about road accidents in Italy and to provide useful visualization for improving road safety. Starting from the annual report of road accidents of the Automobile Club of Italy, we transform the original data into an RDF dataset according to the Linked Open Data principles and connect it to external datasets. Then, an integration with Open Street Map allows to display the accident data on a map. Here, the final user is able to identify which road sections are most critical based on the number of deaths, injuries or accidents.},
	address = {New York, NY, USA},
	articleno = {18},
	author = {Colacino, Vincenzo Giuseppe and Po, Laura},
	booktitle = {Proceedings of the 7th International Conference on Web Intelligence, Mining and Semantics},
	doi = {10.1145/3102254.3102273},
	isbn = {9781450352253},
	keywords = {visualization, road fatalities, road accident rate, map, linked open data, heat map, geographic visualization, data integration, RDF, OWL, LOD},
	location = {Amantea, Italy},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {WIMS '17},
	title = {Managing road safety through the use of linked data and heat maps},
	url = {https://doi.org/10.1145/3102254.3102273},
	year = {2017},
}

@article{10.1145/3105576,
	abstract = {A knowledgeable observer of a game of football (soccer) can make a subjective evaluation of the quality of passes made between players during the game, such as rating them as Good, OK, or Bad. In this article, we consider the problem of producing an automated system to make the same evaluation of passes and present a model to solve this problem.Recently, many professional football leagues have installed object tracking systems in their stadiums that generate high-resolution and high-frequency spatiotemporal trajectories of the players and the ball. Beginning with the thesis that much of the information required to make the pass ratings is available in the trajectory signal, we further postulated that using complex data structures derived from computational geometry would enable domain football knowledge to be included in the model by computing metric variables in a principled and efficient manner. We designed a model that computes a vector of predictor variables for each pass made and uses machine learning techniques to determine a classification function that can accurately rate passes based only on the predictor variable vector.Experimental results show that the learned classification functions can rate passes with 90.2\% accuracy. The agreement between the classifier ratings and the ratings made by a human observer is comparable to the agreement between the ratings made by human observers, and suggests that significantly higher accuracy is unlikely to be achieved. Furthermore, we show that the predictor variables computed using methods from computational geometry are among the most important to the learned classifiers.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Chawla, Sanjay and Estephan, Jo\"{e}l and Gudmundsson, Joachim and Horton, Michael},
	doi = {10.1145/3105576},
	issn = {2374-0353},
	issue_date = {June 2017},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {supervised learning, spatial algorithms, feature engineering, computational geometry, Classification},
	month = {aug},
	number = {2},
	numpages = {30},
	publisher = {Association for Computing Machinery},
	title = {Classification of Passes in Football Matches Using Spatiotemporal Data},
	url = {https://doi.org/10.1145/3105576},
	volume = {3},
	year = {2017},
}

@inproceedings{10.1145/3105831.3105834,
	abstract = {As cities are becoming green and smart, public information systems are being revamped to adopt digital technologies. There are several sources (official or not) that can provide information related to a city. The availability of multiple sources enables the design of advanced analyses for offering valuable services to both citizens and municipalities. However, such analyses would fail if the considered data were affected by errors and uncertainties: Data Quality is one of the main requirements for the successful exploitation of the available information. This paper highlights the importance of the Data Quality evaluation in the context of geographical data sources. Moreover, we describe how the Entity Matching task can provide additional information to refine the quality assessment and, consequently, obtain a better evaluation of the reliability data sources. Data gathered from the public transportation and urban areas of Curitiba, Brazil, are used to show the strengths and effectiveness of the presented approach.},
	address = {New York, NY, USA},
	author = {Ara\'{u}jo, Tiago Brasileiro and Cappiello, Cinzia and Kozievitch, Nadia Puchalski and Mestre, Demetrio Gomes and Pires, Carlos Eduardo Santos and Vitali, Monica},
	booktitle = {Proceedings of the 21st International Database Engineering \&amp; Applications Symposium},
	doi = {10.1145/3105831.3105834},
	isbn = {9781450352208},
	keywords = {Smart Cities, Entity Matching, Data Quality, Data Analysis},
	location = {Bristol, United Kingdom},
	numpages = {5},
	pages = {304–308},
	publisher = {Association for Computing Machinery},
	series = {IDEAS '17},
	title = {Towards Reliable Data Analyses for Smart Cities},
	url = {https://doi.org/10.1145/3105831.3105834},
	year = {2017},
}

@inproceedings{10.1145/3105971.3105982,
	abstract = {Visualizing time series data with a spatial context is a problem that appears more and more often, since small and lightweight GPS devices allow us to enrich the time series data with position information. One example is the visualization of the energy output of power plants. We present a web-based application that aims to provide information about the energy production of a specified region, along with location information about the power plants. The application is intended to be used as a solid data basis for political discussions, nudging, and story telling about the German energy transition to renewables, called "Energiewende". It was therefore designed to be intuitive, easy to use, and provide information for a broad spectrum of users that do not need any domain-specific knowledge. Users are able to select different categories of power plants and look up their positions on an overview map. Glyphs indicate their exact positions and a selection mechanism allows users to compare the power output on different time scales using stacked area charts or ThemeRivers. As an evaluation of the application, we have collected web access statistics and conducted an online survey with respect to the intuitiveness, usability, and informativeness.},
	address = {New York, NY, USA},
	author = {Rodrigues, Nils and Netzel, Rudolf and Ullah, Kazi Riaz and Burch, Michael and Schultz, Alexander and Burger, Bruno and Weiskopf, Daniel},
	booktitle = {Proceedings of the 10th International Symposium on Visual Information Communication and Interaction},
	doi = {10.1145/3105971.3105982},
	isbn = {9781450352925},
	keywords = {time series data, geographic visualization, exploratory data analysis},
	location = {Bangkok, Thailand},
	numpages = {8},
	pages = {37–44},
	publisher = {Association for Computing Machinery},
	series = {VINCI '17},
	title = {Visualization of time series data with spatial context: communicating the energy production of power plants},
	url = {https://doi.org/10.1145/3105971.3105982},
	year = {2017},
}

@inproceedings{10.1145/3106195.3106222,
	abstract = {Geographic Information Systems (GIS) play a critical role for supporting the development of Cyber Physical Systems (CPS), since they allow geolocating users and the "things" or smart objects that constitute a CPS, providing a realistic vision in quasi real-time. This has increased the demand of developing web-based GIS applications to be deployed in the different devices and wearables of the CPS with short time-to-market. This demand and the fact that web-based GIS applications of CPS share many features and known variability justifies why they present the perfect setting to apply software product-lines engineering (SPLE). In this paper, we present the experience of developing a web-based GIS product line in the SME Enxenio, and the methodology applied to define the product line. In addition, we present the results obtained providing the GIS community with a reference SPL that is ready for its evolution and enrichment.},
	address = {New York, NY, USA},
	author = {Corti\~{n}as, Alejandro and Luaces, Miguel R. and Pedreira, Oscar and Places, \'{A}ngeles S. and P\'{e}rez, Jennifer},
	booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
	doi = {10.1145/3106195.3106222},
	isbn = {9781450352215},
	keywords = {web-based geographic information systems, scaffolding, Software product line engineering},
	location = {Sevilla, Spain},
	numpages = {5},
	pages = {190–194},
	publisher = {Association for Computing Machinery},
	series = {SPLC '17},
	title = {Web-based Geographic Information Systems SPLE: Domain Analysis and Experience Report},
	url = {https://doi.org/10.1145/3106195.3106222},
	year = {2017},
}

@inproceedings{10.1145/3106426.3106498,
	abstract = {Textual queries are largely employed in information retrieval to let users specify search goals in a natural way. However, differences in user and system terminologies can challenge the identification of the user's information needs, and thus the generation of relevant results. We argue that the explicit management of ontological knowledge, and of the meaning of concepts (by integrating linguistic and encyclopaedic knowledge in the system ontology), can improve the analysis of search queries, because it enables a flexible identification of the topics the user is searching for, regardless of the adopted vocabulary. This paper proposes an information retrieval support model based on semantic concept identification. Starting from the recognition of the ontology concepts that the search query refers to, this model exploits the qualifiers specified in the query to select information items on the basis of possibly fine-grained features. Moreover, it supports query expansion and reformulation by suggesting the exploration of semantically similar concepts, as well as of concepts related to those referred in the query through thematic relations. A test on a data-set collected using the OnToMap Participatory GIS has shown that this approach provides accurate results.},
	address = {New York, NY, USA},
	author = {Mauro, Noemi and Ardissono, Liliana and Savoca, Adriano},
	booktitle = {Proceedings of the International Conference on Web Intelligence},
	doi = {10.1145/3106426.3106498},
	isbn = {9781450349512},
	keywords = {participatory GIS, ontologies, linked data, information search},
	location = {Leipzig, Germany},
	numpages = {8},
	pages = {34–41},
	publisher = {Association for Computing Machinery},
	series = {WI '17},
	title = {Concept-aware geographic information retrieval},
	url = {https://doi.org/10.1145/3106426.3106498},
	year = {2017},
}

@inproceedings{10.1145/3109729.3109759,
	abstract = {The SME (Small and medium-sized enterprise) Enxenio has developed many web-based Geographic Information Systems within the last decade. Since the demand for GIS is increasing, Enxenio decided to apply Software Product Line Engineering to this domain to facilitate the development of complete web-based GIS applications, increasing their quality, improving the time-to-market and, at the same time, reducing their cost to its clients. This demo shows the resulting tool of this process, which is able to generate the source code of a web-based GIS from the set of desired features and the definition of its data model. This tool can be run within a web browser and the derivation engine in charge of generating the code is based on the scaffolding technique.},
	address = {New York, NY, USA},
	author = {Corti\~{n}as, Alejandro and Luaces, Miguel R. and Pedreira, Oscar and Places, \'{A}ngeles S.},
	booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume B},
	doi = {10.1145/3109729.3109759},
	isbn = {9781450351195},
	keywords = {web-based geographic information systems, scaffolding, Software product line engineering},
	location = {Sevilla, Spain},
	numpages = {4},
	pages = {46–49},
	publisher = {Association for Computing Machinery},
	series = {SPLC '17},
	title = {Scaffolding and in-browser generation of web-based GIS applications in a SPL tool},
	url = {https://doi.org/10.1145/3109729.3109759},
	year = {2017},
}

@inproceedings{10.1145/3109761.3158378,
	abstract = {This work extends the Fuzzy Lattice Reasoning (FLR) Classifier to manage spatial attributes, and spatial relationships. Specifically, we concentrate on spatial entities, as countries, cities, or states. Lattice Theory requires the elements of a Lattice to be partially ordered. To match such requirement, spatial entities are represented as a graph, whose number of nodes is equal to the amount of unique values of the spatial attribute elements. Then, the graph nodes are linearly arranged to formulate a partially ordered set; and thus be included in the Fuzzy Lattice classifier. The overall problem of incorporating spatial attributes in FLR was deduced to a Minimum Linear Arrangement problem. A corresponding open-source implementation in R has been made available on CRAN repository. The proposed method was evaluated using an open spatial dataset from the National Ambient Air Quality Standards (NAAQS). We investigated whether the addition of the spatial attribute contributed to any improvements in classification accuracy; and how linear arrangement alternatives may affect it. Experimental results showed that classification accuracy is above 85\% in all cases, and the use of spatial attributes resulted to an increased accuracy of 92\%. Alternative linear arrangements did not contribute significantly in improving classification accuracy in this case study.},
	address = {New York, NY, USA},
	articleno = {52},
	author = {Mavridis, Constantinos and Athanasiadis, Ioannis N.},
	booktitle = {Proceedings of the 1st International Conference on Internet of Things and Machine Learning},
	doi = {10.1145/3109761.3158378},
	isbn = {9781450352437},
	keywords = {spatial data mining, spatial data, spatial classification, linear arrangement, lattice theory, fuzzy lattice reasoning, classification},
	location = {Liverpool, United Kingdom},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {IML '17},
	title = {Spatial classification with fuzzy lattice reasoning},
	url = {https://doi.org/10.1145/3109761.3158378},
	year = {2017},
}

@inproceedings{10.1145/3121050.3121104,
	abstract = {Recently, the geolocalisation of tweets has become an important feature for a wide range of tasks in Information Retrieval and other domains, such as real-time event detection, topic detection or disaster and emergency analysis. However, the number of relevant geo-tagged tweets available remains insufficient to reliably perform such tasks. Thus, predicting the location of non-geotagged tweets is an important yet challenging task, which can increase the sample of geo-tagged data and help to a wide range of tasks. In this paper, we propose a location inference method that utilises a ranking approach combined with a majority voting of tweets weighted based on the credibility of its source (Twitter user). Using geo-tagged tweets from two cities, Chicago and New York (USA), our experimental results demonstrate that our method (statistically) significantly outperforms our baselines in terms of accuracy, and error distance, in both cities, with the cost of decrease in recall.},
	address = {New York, NY, USA},
	author = {Gonzalez Paule, Jorge David and Moshfeghi, Yashar and Jose, Joemon M. and Thakuriah, Piyushimita (Vonu)},
	booktitle = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval},
	doi = {10.1145/3121050.3121104},
	isbn = {9781450344906},
	keywords = {weighted majority voting, user credibility, twitter, information retrieval, geolocalisation, fine-grained},
	location = {Amsterdam, The Netherlands},
	numpages = {4},
	pages = {313–316},
	publisher = {Association for Computing Machinery},
	series = {ICTIR '17},
	title = {On Fine-Grained Geolocalisation of Tweets},
	url = {https://doi.org/10.1145/3121050.3121104},
	year = {2017},
}

@article{10.1145/3121436,
	abstract = {Technology that falsifies navigation data presents significant dangers to public and private organizations.},
	address = {New York, NY, USA},
	author = {Kugler, Logan},
	doi = {10.1145/3121436},
	issn = {0001-0782},
	issue_date = {September 2017},
	journal = {Commun. ACM},
	month = {aug},
	number = {9},
	numpages = {2},
	pages = {18–19},
	publisher = {Association for Computing Machinery},
	title = {Why GPS spoofing is a threat to companies, countries},
	url = {https://doi.org/10.1145/3121436},
	volume = {60},
	year = {2017},
}

@article{10.1145/3125634,
	abstract = {Grids are commonly used as histograms to process spatial data in order to detect frequent patterns, predict destinations, or to infer popular places. However, they have not been previously used for GPS trajectory similarity searches or retrieval in general. Instead, slower and more complicated algorithms based on individual point-pair comparison have been used. We demonstrate how a grid representation can be used to compute four different route measures: novelty, noteworthiness, similarity, and inclusion. The measures may be used in several applications such as identifying taxi fraud, automatically updating GPS navigation software, optimizing traffic, and identifying commuting patterns. We compare our proposed route similarity measure, C-SIM, to eight popular alternatives including Edit Distance on Real sequence (EDR) and Frechet distance. The proposed measure is simple to implement and we give a fast, linear time algorithm for the task. It works well under noise, changes in sampling rate, and point shifting. We demonstrate that by using the grid, a route similarity ranking can be computed in real-time on the Mopsi20141 route dataset, which consists of over 6,000 routes. This ranking is an extension of the most similar route search and contains an ordered list of all similar routes from the database. The real-time search is due to indexing the cell database and comes at the cost of spending 80\% more memory space for the index. The methods are implemented inside the Mopsi2 route module.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Mariescu-Istodor, Radu and Fr\"{a}nti, Pasi},
	doi = {10.1145/3125634},
	issn = {2374-0353},
	issue_date = {September 2017},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {similarity, search, novelty, indexing, grid, GPS routes},
	month = {sep},
	number = {3},
	numpages = {28},
	publisher = {Association for Computing Machinery},
	title = {Grid-Based Method for GPS Route Analysis for Retrieval},
	url = {https://doi.org/10.1145/3125634},
	volume = {3},
	year = {2017},
}

@inproceedings{10.1145/3126973.3126979,
	abstract = {Nowadays, as many devices like mobile phones and smart watch/band are equipped with GPS-devices, a large volume of trajectory data is generated every day. With the availability of such trajectory data, many mining tasks have been proposed and investigated in the past decade. Since the raw trajectory data is usually very large, it is a big challenge to analyse and mine the raw data directly. In order to address this issue, a branch of research has been done to compress the trajectory data. This paper surveys recent research about trajectory compression. An overview of existing techniques for trajectory compression is provided.},
	address = {New York, NY, USA},
	author = {Feng, Kaiyu and Shen, Zhiqi},
	booktitle = {Proceedings of the 2nd International Conference on Crowd Science and Engineering},
	doi = {10.1145/3126973.3126979},
	isbn = {9781450353755},
	keywords = {Trajectory Compressing, Trajectory, Survey},
	location = {Beijing, China},
	numpages = {4},
	pages = {68–71},
	publisher = {Association for Computing Machinery},
	series = {ICCSE'17},
	title = {Compressing Trajectory for Trajectory Indexing},
	url = {https://doi.org/10.1145/3126973.3126979},
	year = {2017},
}

@inproceedings{10.1145/3127404.3127443,
	abstract = {Recently, spatial collaborations1 and crowdsourcing has emerged as a novel typical pattern for applying to a range of problems. A key problem of spatial collaboration is to allocate suitable workers to nearby tasks in a real-time online way. Traditional crowdsourcing algorithms always consider the quality of worker with prior knowledge. However, in online crowdsourcing context, the quality of crowd-workers is unknown and uncertain. It is so hard for such task crowdsourcing process in an inherently online and dynamic environment. To solve this spatial crowdsourcing problem, the branch-and-bound R-tree data structure is employed in our algorithms to prune the search tree of the nearby crowd-workers. Furthermore, we introduce a new online algorithm to deal with the uncertain crowdsourcing problems. Theoretical analysis and extensive experiments are conducted for validation purpose; and the experimental results show that our algorithms outperform several existing algorithms in terms of computation time in dealing with the increasing number of crowdsourcing task executing candidates.},
	address = {New York, NY, USA},
	author = {Sun, Yong and Wang, Jun and Tan, Wenan},
	booktitle = {Proceedings of the 12th Chinese Conference on Computer Supported Cooperative Work and Social Computing},
	doi = {10.1145/3127404.3127443},
	isbn = {9781450353526},
	keywords = {task allocation, online algorithms, collaborative computing, Spatial crowdsourcing},
	location = {Chongqing, China},
	numpages = {4},
	pages = {205–208},
	publisher = {Association for Computing Machinery},
	series = {ChineseCSCW '17},
	title = {Online Algorithms of Task Allocation in Spatial Crowdsourcing},
	url = {https://doi.org/10.1145/3127404.3127443},
	year = {2017},
}

@inproceedings{10.1145/3127526.3127534,
	abstract = {We analyze nearly 20 million geocoded PubMed articles with author affiliations. Using K-means clustering for the lower 48 US states and mainland China, we find that the average published paper is within a relatively short distance of a few centroids. These centroids have shifted very little over the past 30 years, and the distribution of distances to these centroids has not changed much either. The overall country centroids have gradually shifted south (about 0.2° for the USA and 1.7° for China), while the longitude has not moved significantly. These findings indicate that there are few large scientific hubs in the USA and China and the typical investigator is within geographical reach of one such hub. This sets the stage to study centralization of biomedical research at national and regional levels across the globe, and over time.},
	address = {New York, NY, USA},
	author = {Guan, Yingjun and Du, Jing and Torvik, Vetle I.},
	booktitle = {Proceedings of the 6th International Workshop on Mining Scientific Publications},
	doi = {10.1145/3127526.3127534},
	isbn = {9781450353885},
	keywords = {clustering, bibliographic databases, author affiliation, Geocoding},
	location = {Toronto, ON, Canada},
	numpages = {6},
	pages = {40–45},
	publisher = {Association for Computing Machinery},
	series = {WOSP 2017},
	title = {Geographical Distribution of Biomedical Research in the USA and China},
	url = {https://doi.org/10.1145/3127526.3127534},
	year = {2017},
}

@inproceedings{10.1145/3127540.3127543,
	abstract = {Accurate, up-to-date maps of transient traffic and hazards are invaluable to drivers, city managers, and the emerging class of self-driving vehicles. We present LiveMap, a scalable, automated system for acquiring, curating, and disseminating detailed, continually-updated road conditions in a region. LiveMap leverages in-vehicle cameras, sensors, and processors to crowd-source hazard detection without human intervention. We build a real-time simulation framework that allows a mix of real and simulated components to be tested together at scale. We demonstrate that LiveMap can work well at city scales within the limits of today's cellular network bandwidth. We also show the feasibility of accurate, in-vehicle, computer-vision-based hazard detection.},
	address = {New York, NY, USA},
	author = {Hu, Wenlu and Feng, Ziqiang and Chen, Zhuo and Harkes, Jan and Pillai, Padmanabhan and Satyanarayanan, Mahadev},
	booktitle = {Proceedings of the 20th ACM International Conference on Modelling, Analysis and Simulation of Wireless and Mobile Systems},
	doi = {10.1145/3127540.3127543},
	isbn = {9781450351621},
	keywords = {vehicular systems, situational awareness, maps, edge computing, driverless cars, cloudlet, cloud computing, automotive systems},
	location = {Miami, Florida, USA},
	numpages = {10},
	pages = {161–170},
	publisher = {Association for Computing Machinery},
	series = {MSWiM '17},
	title = {Live Synthesis of Vehicle-Sourced Data Over 4G LTE},
	url = {https://doi.org/10.1145/3127540.3127543},
	year = {2017},
}

@inproceedings{10.1145/3129292.3129296,
	abstract = {A telecommunication company (telco) is traditionally only perceived as the entity that provides telecommunication services, such as telephony and data communication access to users. However, the IP backbone infrastructure of such entities spanning densely urban spaces and widely rural areas, provides nowadays a unique opportunity to collect immense amounts of mobility data that can provide valuable insights for road traffic management and avoidance. In this paper we outline the components of the Traffic-TBD (Traffic Telco Big Data) architecture, which aims to become an innovative road traffic analytic and prediction system with the following desiderata: i) provide micro-level traffic modeling and prediction that goes beyond the current state provided by Internet-based navigation enterprises utilizing crowdsourcing; ii) retain the location privacy boundaries of users inside their mobile network operator, to avoid the risks of exposing location data to third-party mobile applications; and iii) be available with minimal costs and using existing infrastructure (i.e., cell towers and TBD data streams are readily available inside a telco). Road traffic understanding, management and analytics can minimize the number of road accidents, optimize fuel and energy consumption, avoid unexpected delays, contribute to a macroscopic spatio-temporal understanding of traffic in cities but also to "smart" societies through applications in city planning, public transportation, logistics and fleet management for enterprises, startups and governmental bodies.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Costa, Constantinos and Chatzimilioudis, Georgios and Zeinalipour-Yazti, Demetrios and Mokbel, Mohamed F.},
	booktitle = {Proceedings of the International Workshop on Real-Time Business Intelligence and Analytics},
	doi = {10.1145/3129292.3129296},
	isbn = {9781450354257},
	keywords = {Telco, Road Traic, Data Analytics, Big Data},
	location = {Munich, Germany},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {BIRTE '17},
	title = {Towards Real-Time Road Traffic Analytics using Telco Big Data},
	url = {https://doi.org/10.1145/3129292.3129296},
	year = {2017},
}

@article{10.1145/3130982,
	abstract = {Economic loss caused by natural disasters is increasing in many cities around the world. There is an increasing demand for a method that effectively measures the fragility of people flow to appropriately plan the future investment into infrastructure. Conventional methods measure the fragility of urban systems using infrastructure data such as the road and railway networks. However, these methods are costly to perform, cannot directly measure the disruption on human activities caused by disasters, nor can they be applied for individual disasters. Here, we propose a novel method that quantifies the fragility of cities through detecting the delay in commuting activities using GPS data collected from smartphones. Because commuting activities are daily routines for many people, commuting flow has little day-to-day fluctuation, which makes it an appropriate metric for detecting anomalies and disruption in urban systems. This method can be utilized in any city in the world regardless of differences in network structures or population distribution, as long as people commute on a daily basis. We validate our method in various cities for snowfall and typhoons using real datasets in Japan, and show that intuitive results can be obtained. Our method's reliability is clarified by comparing the results with conventional metrics. We also present useful analyses and applications of CityFlowFragility for urban planning and disaster management.},
	address = {New York, NY, USA},
	articleno = {117},
	author = {Yabe, Takahiro and Tsubouchi, Kota and Sekimoto, Yoshihide},
	doi = {10.1145/3130982},
	issue_date = {September 2017},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	keywords = {mobile phone GPS data, disaster fragility, Urban human dynamics},
	month = {sep},
	number = {3},
	numpages = {17},
	publisher = {Association for Computing Machinery},
	title = {CityFlowFragility: Measuring the Fragility of People Flow in Cities to Disasters using GPS Data Collected from Smartphones},
	url = {https://doi.org/10.1145/3130982},
	volume = {1},
	year = {2017},
}

@inproceedings{10.1145/3132218.3132223,
	abstract = {Building Information Modelling (BIM) is a key enabler to support integration of building data within the buildings life cycle and is an important aspect to support a wide range of use cases, related to building navigation, control, sustainability, etc. Open BIM faces several challenges related to standardization, data interdependency, data access, and security. In addition to these technical challenges, there remains the barrier among BIM developers who wish to protect their intellectual property, as full 3D BIM development requires expertise and effort. This means that there is often limited availability of BIM models. In Ireland, the Ordnance Survey Ireland (OSi) has a substantial dataset which includes not only GIS data (polygon footprint, geodetic coordinate), but also additional building specific data (form and function). In this paper we demonstrate the use of an applied and tested methodology for uplifting GIS data (relational data) into RDF (GeoSPARQL and OSi ontology) and demonstrate how this data is used for interlinking to other building data with an initial, simple exploratory example, taken from DBpedia. By interlinking building data and making it available, new insights about buildings in Ireland can be made, currently not possible due to lack of availability of data. This is an important step towards the iterative integration of ever more complex BIM models into the wider web of data to support the aforementioned use cases.},
	address = {New York, NY, USA},
	author = {McGlinn, Kris and Debruyne, Christophe and McNerney, Lorraine and O'Sullivan, Declan},
	booktitle = {Proceedings of the 13th International Conference on Semantic Systems},
	doi = {10.1145/3132218.3132223},
	isbn = {9781450352963},
	keywords = {Ordnance Survey Ireland, Linked Data, Interlinking, Information Modelling, Geographic Information System, GeoSPARQL},
	location = {Amsterdam, Netherlands},
	numpages = {8},
	pages = {57–64},
	publisher = {Association for Computing Machinery},
	series = {Semantics2017},
	title = {Integrating Ireland's Geospatial Information to Provide Authoritative Building Information Models},
	url = {https://doi.org/10.1145/3132218.3132223},
	year = {2017},
}

@inproceedings{10.1145/3132300.3132322,
	abstract = {Optical interferometry images are widely used in many research areas such as geoscience and remote sensing. However, the phase image which gets from the sensors is modulo-2π, and an important process is to estimate the absolute phase from the observation which is termed phase unwrapping. This paper addresses the absolute phase estimation of large scale images. The proposed phase unwrapping method is patch based which divides the large interferogram into small patches and does the unwrapping based on Markov Random Field for each small patch. This paper also analyzes the two types of errors that may be caused in the combination of the patches and studies how to get rid of these errors. The advantage of the proposal is that the method can be used for the wrapped phase images with discontinuous areas. Experiments in the paper show the efficiency of the proposed algorithm.},
	address = {New York, NY, USA},
	author = {Hongxing, Hao and Lingda, Wu and Xiaorui, Song},
	booktitle = {Proceedings of the International Conference on Imaging, Signal Processing and Communication},
	doi = {10.1145/3132300.3132322},
	isbn = {9781450352895},
	keywords = {Remote sensing, Phase Unwrapping, InSAR image processing, Digital Elevation Model data},
	location = {Penang, Malaysia},
	numpages = {5},
	pages = {76–80},
	publisher = {Association for Computing Machinery},
	series = {ICISPC 2017},
	title = {A Phase Unwrapping Method for Large Scale Interferometric Phase Images},
	url = {https://doi.org/10.1145/3132300.3132322},
	year = {2017},
}

@inproceedings{10.1145/3132847.3132906,
	abstract = {Which venue is a tweet posted from? We referred this as fine-grained geolocation. To solve this problem effectively, we develop novel techniques to exploit each posting user's content history. This is motivated by our finding that most users do not share their visitation history, but have ample content history from tweet posts.  We formulate fine-grained geolocation as a ranking problem whereby given a test tweet, we rank candidate venues. We propose several models that leverage on three types of signals from locations, users and peers. Firstly, the location signals are words that are indicative of venues. We propose a location-indicative weighting scheme to capture this. Next we exploit user signals from each user's content history to enrich the very limited content of their tweets which have been targeted for geolocation. The intuition is that the user's other tweets may have been from the test venue or related venues, thus providing informative words. In this regard, we propose query expansion as the enrichment approach. Finally, we exploit the signals from peer users who have similar content history and thus potentially similar visitation behavior as the users of the test tweets. This suggests collaborative filtering where visitation information is propagated via content similarities. We proposed several models incorporating different combinations of the three signals. Our experiments show that the best model incorporates all three signals. It performs 6\% to 40\% better than the baselines depending on the metric and dataset.},
	address = {New York, NY, USA},
	author = {Chong, Wen-Haw and Lim, Ee-Peng},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	doi = {10.1145/3132847.3132906},
	isbn = {9781450349185},
	keywords = {tweet geolocation, query expansion, collaborative filtering},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {1279–1288},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Tweet Geolocation: Leveraging Location, User and Peer Signals},
	url = {https://doi.org/10.1145/3132847.3132906},
	year = {2017},
}

@inproceedings{10.1145/3132847.3132964,
	abstract = {Detecting anomalous patterns from dynamic and multi-attributed network systems has been a challenging problem due to the complication of temporal dynamics and the variations reflected in multiple data sources. We propose a Multi-view Time-Series Hypersphere Learning (MTHL) approach that leverages multi-view learning and support vector description to tackle this problem. Given a dynamic network with time-varying edge and node properties, MTHL projects multi-view time-series data into a shared latent subspace, and then learns a compact hypersphere surrounding normal samples with soft constraints. The learned hypersphere allows for effectively distinguishing normal and abnormal cases. We further propose an efficient, two-stage alternating optimization algorithm as a solution to the MTHL. Extensive experiments are conducted on both synthetic and real datasets. Results demonstrate that our method outperforms the state-of-the-art baseline methods in detecting three types of events that involve (i) time-varying features alone, (ii) time-aggregated features alone, as well as (iii) both features. Moreover, our approach exhibits consistent and good performance in face of issues including noises, anomaly pollution in training phase and data imbalance.},
	address = {New York, NY, USA},
	author = {Teng, Xian and Lin, Yu-Ru and Wen, Xidao},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	doi = {10.1145/3132847.3132964},
	isbn = {9781450349185},
	keywords = {urban computing, time- series mining, multi-view learning, dynamic networks, anomaly detection},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {827–836},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Anomaly Detection in Dynamic Networks using Multi-view Time-Series Hypersphere Learning},
	url = {https://doi.org/10.1145/3132847.3132964},
	year = {2017},
}

@inproceedings{10.1145/3132847.3133024,
	abstract = {Crime prediction plays a crucial role in improving public security and reducing the financial loss of crimes. The vast majority of traditional algorithms performed the prediction by leveraging demographic data, which could fail to capture the dynamics of crimes in urban. In the era of big data, we have witnessed advanced ways to collect and integrate fine-grained urban, mobile, and public service data that contains various crime-related sources and rich temporal-spatial information. Such information provides better understandings about the dynamics of crimes and has potentials to advance crime prediction. In this paper, we exploit temporal-spatial correlations in urban data for crime prediction. In particular, we validate the existence of temporal-spatial correlations in crime and develop a principled approach to model these correlations into the coherent framework TCP for crime prediction. The experimental results on real-world data demonstrate the effectiveness of the proposed framework. Further experiments have been conducted to understand the importance of temporal-spatial correlations in crime prediction.},
	address = {New York, NY, USA},
	author = {Zhao, Xiangyu and Tang, Jiliang},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	doi = {10.1145/3132847.3133024},
	isbn = {9781450349185},
	keywords = {temporal-spatial correlation, crime prevention, crime prediction},
	location = {Singapore, Singapore},
	numpages = {10},
	pages = {497–506},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {Modeling Temporal-Spatial Correlations for Crime Prediction},
	url = {https://doi.org/10.1145/3132847.3133024},
	year = {2017},
}

@inproceedings{10.1145/3132847.3133184,
	abstract = {Due to the popularity of social networks with geo-tagged activities, so-called location-based social networks (LBSN), a number of methods have been proposed for influence maximization for applications such as word-of-mouth marketing (WOMM), and out-of-home marketing (OOH). It is thus important to analyze and compare these different approaches. In this demonstration, we present a unified system IMaxer that both provides a complete pipeline of state-of-the-art and novel models and algorithms for influence maximization (IM) as well as allows to evaluate and compare IM techniques for a particular scenario. IMaxer allows to select and transform the required data from raw LBSN datasets. It further provides a unified model that utilizes interactions of nodes in an LBSN, i.e., users and locations, for capturing diverse types of information propagations. On the basis of these interactions, influential nodes can be found and their potential influence can be simulated and visualized using Google Maps and graph visualization APIs. Thus, IMaxer allows users to compare and pick the most suitable IM method in terms of effectiveness and cost.},
	address = {New York, NY, USA},
	author = {Saleem, Muhammad Aamir and Kumar, Rohit and Calders, Toon and Xie, Xike and Pedersen, Torben Bach},
	booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
	doi = {10.1145/3132847.3133184},
	isbn = {9781450349185},
	keywords = {unified system, location-based social networks, influence maximization},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {2523–2526},
	publisher = {Association for Computing Machinery},
	series = {CIKM '17},
	title = {IMaxer: A Unified System for Evaluating Influence Maximization in Location-based Social Networks},
	url = {https://doi.org/10.1145/3132847.3133184},
	year = {2017},
}

@inproceedings{10.1145/3136560.3136570,
	abstract = {Reducing the road traffic congestion has become an important challenge in recent years; the researchers have focused on identifying causes and remedies for the traffic congestion. However, the impact of the locations and activity time of points of interests (POIs) on the road traffic has not yet been explored. Moreover, in developing countries, POIs are not established in planned manner and cause the traffic congestion. In this paper, we analyze how POI locations and activities affect the road traffic. Specifically, we identify the congestion pattern caused by POI activities in the field study and develop a model to quantify the spatio-temporal impact of POI activities on the traffic congestion. Moreover, we develop a model to predict the impact of POI activities on the road traffic congestion using fuzzy regressions for different POI categories. We perform a set of case studies and experiments to show the effectiveness of our models using real datasets of POI activities and road traffic.},
	address = {New York, NY, USA},
	articleno = {9},
	author = {Hakim, Ashraful and Hashem, Tanzima and Ali, Mohammed Eunus},
	booktitle = {Proceedings of the Ninth International Conference on Information and Communication Technologies and Development},
	doi = {10.1145/3136560.3136570},
	isbn = {9781450352772},
	keywords = {traffic impact quantification, traffic impact prediction, traffic congestion, fuzzy linear regression, POI impact},
	location = {Lahore, Pakistan},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	series = {ICTD '17},
	title = {Quantification and Prediction Models for the Impact of POIs on Road Traffic Congestion in Developing Countries},
	url = {https://doi.org/10.1145/3136560.3136570},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139963,
	abstract = {Effective processing of extremely large volumes of spatial data has led to many organizations employing distributed processing frameworks. Apache Spark is one such open-source framework that is enjoying widespread adoption. Within this data space, it is important to note that most of the observational data (i.e., data collected by sensors, either moving or stationary) has a temporal component, or timestamp. In order to perform advanced analytics and gain insights, the temporal component becomes equally important as the spatial and attribute components. In this paper, we detail several variants of a spatial join operation that addresses both spatial, temporal, and attribute-based joins. Our spatial join technique differs from other approaches in that it combines spatial, temporal, and attribute predicates in the join operator.In addition, our spatio-temporal join algorithm and implementation differs from others in that it runs in commercial off-the-shelf (COTS) application. The users of this functionality are assumed to be GIS analysts with little if any knowledge of the implementation details of spatio-temporal joins or distributed processing. They are comfortable using simple tools that do not provide the ability to tweak the configuration of the},
	address = {New York, NY, USA},
	articleno = {20},
	author = {Whitman, Randall T. and Park, Michael B. and Marsh, Bryan G. and Hoel, Erik G.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139963},
	isbn = {9781450354905},
	keywords = {spatio-temporal join, distributed processing, Spatial join, Spark, Hadoop, HDFS},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Spatio-Temporal Join on Apache Spark},
	url = {https://doi.org/10.1145/3139958.3139963},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139964,
	abstract = {We propose a new approach for constructing the underlying map from trajectory data. Our algorithm is based on the idea that road segments can be identified as stable subtrajectory clusters in the data. For this, we consider how subtrajectory clusters evolve for varying distance values, and choose stable values for these. In doing so we avoid a global proximity parameter. Within trajectory clusters, we choose representatives, which are combined to form the map. We experimentally evaluate our algorithm on vehicle and hiking tracking data. These experiments demonstrate that our approach can naturally separate roads that run close to each other and can deal with outliers in the data, two issues that are notoriously difficult in road network reconstruction.},
	address = {New York, NY, USA},
	articleno = {14},
	author = {Buchin, Kevin and Buchin, Maike and Duran, David and Fasy, Brittany Terese and Jacobs, Roel and Sacristan, Vera and Silveira, Rodrigo I. and Staals, Frank and Wenk, Carola},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139964},
	isbn = {9781450354905},
	keywords = {map construction, geometric algorithms, clustering, Trajectories},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Clustering Trajectories for Map Construction},
	url = {https://doi.org/10.1145/3139958.3139964},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139967,
	abstract = {This paper provides the first attempt for a full-fledged query optimizer for MapReduce-based spatial join algorithms. The optimizer develops its own taxonomy that covers almost all possible ways of doing a spatial join for any two input datasets. The optimizer comes in two flavors; cost-based and rule-based. Given two input data sets, the cost-based query optimizer evaluates the costs of all possible options in the developed taxonomy, and selects the one with the lowest cost. The rule-based query optimizer abstracts the developed cost models of the cost-based optimizer into a set of simple easy-to-check heuristic rules. Then, it applies its rules to select the lowest cost option. Both query optimizers are deployed and experimentally evaluated inside a widely used open-source MapReduce-based big spatial data system. Exhaustive experiments show that both query optimizers are always successful in taking the right decision for spatially joining any two datasets of up to 500GB each.},
	address = {New York, NY, USA},
	articleno = {21},
	author = {Sabek, Ibrahim and Mokbel, Mohamed F.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139967},
	isbn = {9781450354905},
	keywords = {Spatial Join, Query Optimization, MapReduce, Hadoop},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {On Spatial Joins in MapReduce},
	url = {https://doi.org/10.1145/3139958.3139967},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139968,
	abstract = {As the underground infrastructure systems of cities age, maintenance and repair become an increasing concern. Cities face difficulties in planning maintenance, predicting and responding to infrastructure related issues, and in realizing their vision to be a smart city due to their incomplete understanding of the existing state of the infrastructure. Only few cities have accurate and complete digital information on their underground infrastructure (e.g., electricity, water, natural gas) systems, which poses problems to those planning and performing construction projects. To address these issues, we introduce GUIDES as a new data conversion and management framework for urban underground infrastructure systems that enable city administrators, workers, and contractors along with the general public and other users to query digitized and integrated data to make smarter decisions. This demo paper presents the GUIDES architecture and describes two of its central components: (i) mapping of underground infrastructure systems, and (ii) integration of heterogeneous geospatial data.},
	address = {New York, NY, USA},
	articleno = {90},
	author = {Balasubramani, Booma Sowkarthiga and Belingheri, Omar and Boria, Eric S. and Cruz, Isabel F. and Derrible, Sybil and Siciliano, Michael D.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139968},
	isbn = {9781450354905},
	keywords = {Urban underground infrastructure, Smart cities, Ontology, Geospatial data, GIS, Data management, Data integration, Data analytics},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {GUIDES: Geospatial Urban Infrastructure Data Engineering Solutions},
	url = {https://doi.org/10.1145/3139958.3139968},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139970,
	abstract = {This research aims to develop a method for maximizing the accuracy of next place prediction (NPP) in places that are unfamiliar to each mobile phone users. NPP is a problem of predicting the next place of the user given his/her current place and current time. In places that are unfamiliar to the person, it is difficult to predict the next place based on the person's historical location data because there are just a few or no data in such places for each user. Furthermore, it is also difficult to rely on the regularity of human mobility because tourists' mobility is easily affected by many external factors, such as weather. Our research aims to solve the difficulties in NPP in unfamiliar places by focusing on contextual factors such as weather, transportation means, place of residence, and time.},
	address = {New York, NY, USA},
	articleno = {76},
	author = {Maeda, Takashi Nicholas and Tsubouchi, Kota and Toriumi, Fujio},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139970},
	isbn = {9781450354905},
	keywords = {tourists' mobility, next place prediction, movement pattern},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Next place prediction in unfamiliar places considering contextual factors},
	url = {https://doi.org/10.1145/3139958.3139970},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139977,
	abstract = {The recent proliferation of positioning devices has boosted the requirement for efficient methods of processing and analyzing large amounts of recorded movement data. Besides the geographic position, for many application domains there is more relevant time-dependent information such as speed, elevation, street names, or transportation modes, depending on the kind of moving object and on the evaluation purpose. In this paper, we present an application of a new framework that efficiently analyzes datasets with several time-dependent attributes of different types, using a highly flexible and expressive pattern language. In contrast to previous variants, the semantics of the language has been changed to make it more expressive and flexible, and the efficiency has been improved.For a fast processing, we apply a multi-index consisting of several single indexes whose types depend on the attribute types. The flexibility and efficiency of our approach are demonstrated by identifying flights with certain properties among a dataset from Aircraft Traffic Control with the help of a sophisticated pattern.},
	address = {New York, NY, USA},
	articleno = {88},
	author = {Vald\'{e}s, Fabio and G\"{u}ting, Ralf Hartmut},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139977},
	isbn = {9781450354905},
	keywords = {Tuples of Time-Dependent Values, Pattern Matching, Indexing},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Efficient Multi-Attribute Analysis for Trajectories: A Case Study for Aircraft},
	url = {https://doi.org/10.1145/3139958.3139977},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139981,
	abstract = {Understanding where traffic accidents occur is crucial for improving road safety and proper traffic enforcement allocation. One of the most common methods of analyzing traffic accidents is spatial hotspot detection. Existing hotspot detection methods, e.g., spatial scan statistics, spatial and spatiotemporal kernel density estimation, mostly focus on Euclidean space. These methods ignore an important aspect of traffic accident hotspots, i.e., traffic accident locations are constrained to road networks. Several techniques have been proposed to detect spatial hotspot on the network space, including network kernel density-estimation, and significant linear route detection, but the time dimension and temporal dynamics of hotspots are not incorporated. To address the limitations of existing methods, we demonstrated a new method called Spatial-Temporal Network Kernel Density Estimation (STNKDE) that integrates both of these features. We also developed a prototype system and visualized the dynamics of traffic accident hotspots in New York City 2017.},
	address = {New York, NY, USA},
	articleno = {98},
	author = {Romano, Benjamin and Jiang, Zhe},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139981},
	isbn = {9781450354905},
	keywords = {traffic accident, spatio-temporal network kernel density, law enforcement, Spatio-temporal hotspot},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Visualizing Traffic Accident Hotspots Based on Spatial-Temporal Network Kernel Density Estimation},
	url = {https://doi.org/10.1145/3139958.3139981},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139983,
	abstract = {It is often desirable to analyse trajectory data in local coordinates relative to a reference location. Similarly, temporal data also needs to be transformed to be relative to an event. Together, temporal and spatial contextualisation permits comparative analysis of similar trajectories taken across multiple reference locations. To the GIS professional, the procedures to establish a reference frame at a location and reproject the data into local coordinates are well known, albeit tedious. However, GIS tools are now often used by subject matter experts who may not have the deep knowledge of coordinate frames and projections required to use these techniques effectively.We introduce a novel method for representing spatio-temporal reference frames using ordinary geographic objects available in GIS tools. We argue that our method both reduces the number of manual steps required to reproject data to a local reference frame, in addition to reducing the number of concepts a novice user would need to learn.},
	address = {New York, NY, USA},
	articleno = {95},
	author = {Simmons, Andrew and Vasa, Rajesh},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139983},
	isbn = {9781450354905},
	keywords = {Trajectories, Reference Frames, Projection, GPS, GNSS},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Spatio-Temporal Reference Frames as Geographic Objects},
	url = {https://doi.org/10.1145/3139958.3139983},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139984,
	abstract = {The traffic volume on road segments is a vital property of the transportation efficiency. City-wide traffic volume information can benefit people with their everyday life, and help the government on better city planning. However, there are no existing methods that can monitor the traffic volume of every road, because they are either too expensive or inaccurate. Fortunately, nowadays we can collect a large amount of urban data which provides us the opportunity to tackle this problem. In this paper, we propose a novel framework to infer the city-wide traffic volume information with data collected by loop detectors and taxi trajectories. Although these two data sets are incomplete, sparse and from quite different domains, the proposed spatio-temporal semi-supervised learning model can take the full advantages of both data and accurately infer the volume of each road. In order to provide a better interpretation on the inference results, we also derive the confidence of the inference based on spatio-temporal properties of traffic volume. Real-world data was collected from 155 loop detectors and 6,918 taxis over a period of 17 days in Guiyang China. The experiments performed on this large urban data set demonstrate the advantages of the proposed framework on correctly inferring the traffic volume in a city-wide scale.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Meng, Chuishi and Yi, Xiuwen and Su, Lu and Gao, Jing and Zheng, Yu},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139984},
	isbn = {9781450354905},
	keywords = {trajectory, traffic volume, semi-supervised learning, loop detector, Urban computing},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {City-wide Traffic Volume Inference with Loop Detector Data and Taxi Trajectories},
	url = {https://doi.org/10.1145/3139958.3139984},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139985,
	abstract = {An important problem in terrain analysis is modeling how water flows across a terrain and creates floods by filling up depressions. In this paper we study the flooding query problem: Given a rain region R and a query point q on the terrain, quickly determine how much rain has to fall in R so that q is flooded. Available terrain data is often subject to uncertainty which must be incorporated into the terrain analysis. For instance, the digital elevation models of terrains have to be refined to incorporate underground pipes, tunnels, and waterways under bridges, but there is often uncertainty in their existence. By representing the uncertainty in the terrain data explicitly, we can develop methods for flood risk analysis that properly incorporate terrain uncertainty when reporting what areas are at risk of flooding.We present two results. First, we present a linear size data structure that given a terrain (with no data uncertainty) can answer the flooding query in O(m log2 n) time, where m is the number of minima of the terrain at which rain is falling and n is the number of vertices of the terrain. Next, we extend this data structure to handle "uncertain" terrains, using a standard Monte Carlo method. Given a probability distribution on terrains, our data structure solves the problem of determining the probability that if a specified amount of rain falls on a given region a query point is flooded. We implement our data structures and show that they work very well in practice.},
	address = {New York, NY, USA},
	articleno = {36},
	author = {Rav, Mathias and Lowe, Aaron and Agarwal, Pankaj K.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139985},
	isbn = {9781450354905},
	keywords = {stochastic process, geographical information systems, data uncertainty, contour trees, Terrains, Monte Carlo method},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Flood Risk Analysis on Terrains},
	url = {https://doi.org/10.1145/3139958.3139985},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139986,
	abstract = {With increased accessibility of large scale open data, public health studies are able to take advantage of integrative spatial big data to increase the spatial resolution to community or neighborhood level. One critical information for such studies is the large number of addresses of patients, which is private and highly sensitive. Geocoding such massive private addresses poses major challenges for public health researchers. Many geocoders provide only Web APIs which require sending private addresses over the Internet, which is not feasible. Commercial geocoders require high licensing fee and often have limitations on daily usage, which becomes a major hurdle for researchers. Scalability is another major challenge for large scale address dataset. In this paper, we present EaserGeocoder, a novel open source geocoder for effectively geocoding massive address datasets. EaserGeocoder takes an integrative approach by using multiple references based on open address data sources contributed by governments or communities. It takes a machine learning approach to automatically find the best answer from candidates produced by multiple references. The system provides high scalability through parallel processing. Our comparative studies demonstrate Easer-Geocoder outperforms open source geocoders and is comparable to commercial ones in terms of both accuracy and error. It provides a cost-effective and feasible solution for large scale public health studies.},
	address = {New York, NY, USA},
	articleno = {26},
	author = {Rashidian, Sina and Dong, Xinyu and Avadhani, Amogh and Poddar, Prachi and Wang, Fusheng},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139986},
	isbn = {9781450354905},
	keywords = {Text Searching, Geographic Information System, Geocoding},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Effective Scalable and Integrative Geocoding for Massive Address Datasets},
	url = {https://doi.org/10.1145/3139958.3139986},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139995,
	abstract = {Rapid urbanization has posed significant burden on urban transportation infrastructures. In today's cities, both private and public transits have clear limitations to fulfill passengers' needs for quality of experience (QoE): Public transits operate along fixed routes with long wait time and total transit time; Private transits, such as taxis, private shuttles and ride-hailing services, provide point-to-point transits with high trip fare. In this paper, we propose CityLines, a transformative urban transit system, employing hybrid hub-and-spoke transit model with shared shuttles. Analogous to Airlines services, the proposed CityLines system routes urban trips among spokes through a few hubs or direct paths, with travel time as short as private transits and fare as low as public transits. CityLines allows both point-to-point connection to improve the passenger QoE, and hub-and-spoke connection to reduce the system operation cost. Our evaluation results show that CityLines framework can achieve both short travel time and high ride-sharing ratio.},
	address = {New York, NY, USA},
	articleno = {51},
	author = {Liu, Guanxiong and Li, Yanhua and Zhang, Zhi-Li and Luo, Jun and Zhang, Fan},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139995},
	isbn = {9781450354905},
	keywords = {urban computing, spatio-temporal data analytics, Hub-and-spoke Network},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {CityLines: Hybrid Hub-and-Spoke Urban Transit System},
	url = {https://doi.org/10.1145/3139958.3139995},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139997,
	abstract = {A large number of self-driving cars will be on roads in the near future. They will change traffic significantly. Self-driving cars can infer and decide travel paths from passenger input. Passengers do not need to involve in route planning. This provides great opportunities for traffic management systems to collaborate and achieve more efficient traffic management. By knowing most source-destination pairs of the passengers, we envisage an increasingly integrated system that can optimize routes and traffic lights to minimize travel time. By optimally scheduling time of travel and traffic light switching timings, such systems can also provide simultaneously emergency corridors for high priority vehicles such as police cars, fire engines, and ambulances when required.},
	address = {New York, NY, USA},
	articleno = {10},
	author = {Ramamohanarao, Kotagiri and Qi, Jianzhong and Tanin, Egemen and Motallebi, Sadegh},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139997},
	isbn = {9781450354905},
	keywords = {Traffic Optimization, Navigation System, Automated Vehicles},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {From How to Where: Traffic Optimization in the Era of Automated Vehicles},
	url = {https://doi.org/10.1145/3139958.3139997},
	year = {2017},
}

@inproceedings{10.1145/3139958.3139999,
	abstract = {We define the crossing graph of a given embedded graph (such as a road network) to be a graph with a vertex for each edge of the embedding, with two crossing graph vertices adjacent when the corresponding two edges of the embedding cross each other. In this paper, we study the sparsity properties of crossing graphs of real-world road networks. We show that, in large road networks (the Urban Road Network Dataset), the crossing graphs have connected components that are primarily trees, and that the remaining non-tree components are typically sparse (technically, that they have bounded degeneracy). We prove theoretically that when an embedded graph has a sparse crossing graph, it has other desirable properties that lead to fast algorithms for shortest paths and other algorithms important in geographic information systems. Notably, these graphs have polynomial expansion, meaning that they and all their subgraphs have small separators.},
	address = {New York, NY, USA},
	articleno = {40},
	author = {Eppstein, David and Gupta, Siddharth},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3139999},
	isbn = {9781450354905},
	keywords = {sparsity, road network, nonplanar graphs, crossings},
	location = {Redondo Beach, CA, USA},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Crossing Patterns in Nonplanar Road Networks},
	url = {https://doi.org/10.1145/3139958.3139999},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140004,
	abstract = {Real-time visualization in web-based system remains challenging due to the amount of information associated to a 3D urban models. However, these 3D models are not able to provide advanced management of urban infrastructures, such as underground facilities. Nowadays, 3D GIS is considered the appropriate tool to provide accurate analysis and decision support based on spatial data. This paper presents a web-GIS application for 3D visualization, navigation, interaction and analysis of underground infrastructures through virtual reality. The growth of underground cities is a complex problem without easy solutions. In general, these infrastructures cannot be directly visualized. Thus, subsoil mapping can help us to develop a clearer representation of underground's pipes, cables or water mains. In addition, the approach of virtual reality provides an immersive experience and novelty interaction to acquire a complete knowledge about underground city structures. Experimental results show an integral application for the efficient management of underground infrastructure in real-time.},
	address = {New York, NY, USA},
	articleno = {97},
	author = {Jurado, J. M. and Graciano, A. and Ortega, L. and Feito, F. R.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140004},
	isbn = {9781450354905},
	keywords = {Virtual Reality, Real-Time Visualization and Interaction, 3D Underground Models, 3D GIS},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Web-based GIS application for real-time interaction of underground infrastructure through virtual reality},
	url = {https://doi.org/10.1145/3139958.3140004},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140007,
	abstract = {Humans are animals of habit, e.g., people follow typical and/or familiar paths in their daily routines. With that in mind we investigate the problem where a user, traveling on his/her preferred path, needs to visit one of many available points-of-interest while (1) minimizing his/her total travel distance and also (2) minimizing the detour distance incurred to reach the chosen point-of-interest. We call this new problem the "Best-Compromise In-Route Nearest Neighbor" query in order to emphasize that a route cannot typically optimize both criteria at the same time, but rather find a compromise between them. In fact, the competing nature of these two criteria resembles the notion of skyline queries. In that context, we propose a solution based on using suitable upper-bounds to both cost criteria to prune uninteresting paths. It returns all linearly non-dominated paths that are optimal under any given linear combination of the two competing criteria. Our experiments using real data sets of different sizes show that our proposal can be orders of magnitude faster than a straightforward alternative.},
	address = {New York, NY, USA},
	articleno = {41},
	author = {Ahmadi, Elham and Costa, Camila F. and Nascimento, Mario A.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140007},
	isbn = {9781450354905},
	keywords = {Road Networks, Linear Skyline, In-Route Queries},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Best-Compromise In-Route Nearest Neighbor Queries},
	url = {https://doi.org/10.1145/3139958.3140007},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140008,
	abstract = {Urban gathering events such as social protests, sport games, and traffic congestions bring significant challenges to urban management. Identifying gathering events timely is thus an important problem for city administrators and stakeholders. Previous techniques on gathering event detection are mostly descriptive, i.e., using realtime on-site observations (e.g., taxi drop-offs, traffic volume) to detect the gathering events that have already emerged. In this paper we propose a predictive approach to identify future gathering events through destination prediction of incomplete trajectories. Our approach consists of two parts, i.e., destination prediction and event forecasting. For destination prediction, we relax the Markov property assumed in most of the related work and address the consequent high-memory-cost challenge by proposing a novel Via Location Grouping (VIGO) approach for destination prediction. For event forecasting, we design an online prediction mechanism that learns from both historical and recent trajectories to address the non-stationarity of urban trip patterns. Gathering events are forecast based on projected arrivals in each location and time. A case study on real taxi data in Shenzhen, China shows that our proposed approach can correctly and timely predict gathering events. Extensive experiments show that the proposed VIGO approach achieves higher accuracy than related work for destination prediction and saves more than 82\% memory cost over a baseline approach. The event forecasting based on VIGO is effective and fast enough for continuous event forecasting at one-minute frequency.},
	address = {New York, NY, USA},
	articleno = {34},
	author = {Vahedian, Amin and Zhou, Xun and Tong, Ling and Li, Yanhua and Luo, Jun},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140008},
	isbn = {9781450354905},
	keywords = {Trajectory Mining, Gathering Events, Destination Prediction},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Forecasting Gathering Events through Continuous Destination Prediction on Big Trajectory Data},
	url = {https://doi.org/10.1145/3139958.3140008},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140011,
	abstract = {With the development of position tracking technologies and the increasing usage of mobile devices, the analysis of moving objects, such as pedestrians, vehicles, drones, and hurricanes has become an important topic in various applications including intelligent transportation, disaster management, and urban planning. Many of existing studies have focused on managing and analyzing only time-varying locations of point-based objects. However, real-world moving phenomena are space-time continua occupying volumes, having an area at a time; even more, they contain dynamic attributes depending on time and space, such as the velocity of vehicles or the average of wind speed of hurricanes. In this demonstration, we introduce a comprehensive data format to represent various types of temporal geometries and dynamic properties of moving objects based on OGC® Moving Features. Moreover, we present a visual extension of Cesium to visualize moving objects in a space-time cube by cooperating with a data server that manages moving objects in a Cassandra database via RESTful APIs. This demonstration presents how to analyze a correlation between typhoon trajectories and geo-tagged Twitter messages with our systems.},
	address = {New York, NY, USA},
	articleno = {85},
	author = {Kim, Kyoung-Sook and Kim, Dongmin and Jeong, Hyemi and Ogawa, Hirotaka},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140011},
	isbn = {9781450354905},
	keywords = {Space-time Cube, REST, OGC® Moving Features, JSON, Geovisualization, Cesium, Cassandra},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Stinuum: A Holistic Visual Analysis of Moving Objects with Open Source Software},
	url = {https://doi.org/10.1145/3139958.3140011},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140020,
	abstract = {Geometric primitives defined by OGC and ISO standards, implemented in most modern spatially-enabled database management systems (DBMS), are unable to capture the semantics of richer representation types, as found in current geographic data models. Moreover, relational DBMSs do not directly extend referential integrity mechanisms to cover spatial relationships and to support spatial integrity constraints. Rather, they usually assume that all spatial integrity checking will be carried out by the application, during the data entry process. This is not practical if the DBMS supports many applications, and can lead to redundant and inconsistent work. This paper presents AST-PostGIS, an extension for PostgreSQL/PostGIS that incorporates advanced spatial data types and implements spatial integrity constraints. The extension reduces the distance between the conceptual and the physical designs of spatial databases, by providing richer representations for geo-object and geo-field geometries. It also offers procedures to assert the consistency of spatial relationships during data updates. Such procedures can also be used before enforcing spatial integrity constraints for the first time. We illustrate the use of AST-PostGIS on an urban geographic database design problem, mapping its conceptual schema to the physical implementation in extended SQL.},
	address = {New York, NY, USA},
	articleno = {33},
	author = {Lizardo, Lu\'{\i}s Eduardo Oliveira and Davis, Clodoveu Augusto},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140020},
	isbn = {9781450354905},
	keywords = {Spatial integrity Constraints, Spatial databases, Spatial data modeling, OMT-G, Geographic Information Systems},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {A PostGIS extension to support advanced spatial data types and integrity constraints},
	url = {https://doi.org/10.1145/3139958.3140020},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140025,
	abstract = {City-scale 3D models represent an important component in the analysis of urban design, and the analysis of urban systems such as energy and transportation systems. Using manual methods to build large-scale 3D models is a time-consuming and an expensive process. Thus, advancing 3D modeling automation literature can greatly contribute to and complement smart-city initiatives. In this paper, we demonstratean efficient and fully-automatic 3D modeling pipeline that utilizes low resolution noisy LiDAR dataset to create city-scale 3D model.},
	address = {New York, NY, USA},
	articleno = {86},
	author = {Albeaik, Saleh and Alrished, Mohamad and Aldawood, Salma and Alsubaiee, Sattam and Alfaris, Anas},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140025},
	isbn = {9781450354905},
	keywords = {LiDAR Data, Large-Scale Urban Modeling, 3D Modeling},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Virtual Cities: 3D Urban Modeling from Low Resolution LiDAR Data},
	url = {https://doi.org/10.1145/3139958.3140025},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140031,
	abstract = {With the rapid growth of publicly available GPS traces, robust and efficient automatic road network reconstruction has become a crucial task in GIS data analysis and applications. In [20], an effective and robust road network reconstruction algorithm was developed based on the discrete Morse theory, which has the state-of-the-art performance in automatic road-network reconstruction. Based on a discrete Morse-based graph reconstruction framework, we provide two improvements of the previous algorithm [20]: (1) we further simplify it and obtain a better empirical time performance; and (2) we develop a simple but effective editing strategy that helps adding missing road segments in the output reconstruction.},
	address = {New York, NY, USA},
	articleno = {58},
	author = {Dey, Tamal K. and Wang, Jiayuan and Wang, Yusu},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140031},
	isbn = {9781450354905},
	keywords = {Topological method, Morse theory, Map generation, GPS traces},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Improved Road Network Reconstruction using Discrete Morse Theory},
	url = {https://doi.org/10.1145/3139958.3140031},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140033,
	abstract = {Finding a shortest path between two nodes in a graph is a well-studied problem whose applicability in practice crucially relies on the choice of the applied cost function. Especially, for the key application of vehicle routing the cost function may consist of more than one optimization criterion (e.g., distance, travel time, etc.). Finding a good balance between these criteria is a challenging and essential task. We present an approach that learns that balance from existing GPS-tracks. The core of our approach is to find a balance factor α for a given set of GPS-tracks such that the tracks can be decomposed into a minimum number of optimal paths with respect to α.In an experimental evaluation on real-world GPS-tracks of bicyclists we show that our approach yields an appropriate balance factor in a reasonable amount of time.},
	address = {New York, NY, USA},
	articleno = {59},
	author = {Oehrlein, Johannes and Niedermann, Benjamin and Haunert, Jan-Henrik},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140033},
	isbn = {9781450354905},
	keywords = {Trajectory Mining, Shortest Paths, Bicriterial Optimization},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Inferring the Parametric Weight of a Bicriteria Routing Model from Trajectories},
	url = {https://doi.org/10.1145/3139958.3140033},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140037,
	abstract = {The visualization of spatial data becomes increasingly important in science, business and many other domains. In geography, data often corresponds to a large number of point observations that should be displayed on a constrained screen with limited resolution. This causes, however, a loss of information due to an overloaded and occluded visualization. In this paper we present a new visualization algorithm that avoids this problem by aggregating point data into a set of non-overlapping circles that capture all important information. Our algorithm based on a quadtree computes the circles in linear time with respect to the number of points.},
	address = {New York, NY, USA},
	articleno = {73},
	author = {Beilschmidt, Christian and Fober, Thomas and Mattig, Michael and Seeger, Bernhard},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140037},
	isbn = {9781450354905},
	keywords = {Spatial visualization, Point aggregation, Big spatial point data},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {A Linear-Time Algorithm for the Aggregation and Visualization of Big Spatial Point Data},
	url = {https://doi.org/10.1145/3139958.3140037},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140039,
	abstract = {MinMax routing is the task of finding a feasible route between two points such that the maximum distance between any point in the path and a set of points-of-interest is minimal. In this demonstration paper we focus on the case of routing electric vehicles and where points-of-interest are charging stations. In this scenario, the MinMax route guarantees that for any arbitrary point in such a route, the maximum distance that has to be traveled in order to reach a charging station is minimal, being thus a "safer" route. We propose a solution to the MinMax routing problem and implement a web-based prototype that produces practical MinMax routes in realistic sized networks, e.g., the state of California, in sub-second processing time.},
	address = {New York, NY, USA},
	articleno = {89},
	author = {de Oliveira, Martin Ichilevici and Nascimento, Mario A.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140039},
	isbn = {9781450354905},
	keywords = {Trip planning queries, Safe routing, Electric vehicles routing},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {MinMax Routing: The Case for Safer Routes for Electric Vehicles},
	url = {https://doi.org/10.1145/3139958.3140039},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140040,
	abstract = {This paper proposes a novel data-driven model (BESTED), based on spatial Bayesian network with incorporated exponential smoothing mechanism, for predicting precipitation time series on daily basis. In BESTED, the spatial Bayesian network helps to efficiently model the influence of spatially distributed variables. Moreover, the incorporated exponential smoothing mechanism aids in tuning the network inferred values to compensate for the unknown factors, influencing the precipitation rate. Empirical study has been carried out to predict the daily precipitation in West Bengal, India, for the year 2015. The experimental result demonstrates the superiority of the proposed BESTED model, compared to the other benchmarks and state-of-the-art techniques.},
	address = {New York, NY, USA},
	articleno = {55},
	author = {Das, Monidipa and Ghosh, Soumya K.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140040},
	isbn = {9781450354905},
	keywords = {Time series, Spatio-temporal prediction, Spatial Bayesian network, Precipitation, Exponential smoothing},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {BESTED: An Exponentially Smoothed Spatial Bayesian Analysis Model for Spatio-temporal Prediction of Daily Precipitation},
	url = {https://doi.org/10.1145/3139958.3140040},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140044,
	abstract = {Class ambiguity refers to the phenomenon whereby samples with similar features belong to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. SEL problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost (finding an optimal zone partition is NP-hard). Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble), and thus cannot effectively minimize class ambiguity. In contrast, our spatial ensemble framework explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
	address = {New York, NY, USA},
	articleno = {23},
	author = {Jiang, Zhe and Li, Yan and Shekhar, Shashi and Rampi, Lian and Knight, Joseph},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140044},
	isbn = {9781450354905},
	keywords = {spatial heterogeneity, spatial ensemble, local models, class ambiguity, Spatial classification},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity: A Summary of Results},
	url = {https://doi.org/10.1145/3139958.3140044},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140048,
	abstract = {Prior work in river forecasting has focused on applying regression models to gage and discharge prediction since these are naturally continuous dynamical functions. On the other hand, with discretized data, classifiers can be adopted to solve this problem by predicting a conditional probability distribution. Predicting this distribution is important in at least two ways: (1) the variance of the distribution can indicate the confidence of the predicted expected values, and (2) the distribution can be used for computing the probability that the gage or discharge exceeds or falls below some threshold. This paper presents a concrete river forecasting framework with classifiers including probabilistic graphical models (PGMs) and artificial neural network classifiers (ANNCs). The proposed framework is applied on real data for the Guadalupe river basin (Texas) thereby enabling a detailed comparison among various manners of forecasting studied, along with a set of guidelines for their best use.},
	address = {New York, NY, USA},
	articleno = {83},
	author = {Ding, Ruizhou and Marculescu, Diana},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140048},
	isbn = {9781450354905},
	keywords = {spatial-temporal modeling, River forecasting},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Leveraging Classification Models for River Forecasting},
	url = {https://doi.org/10.1145/3139958.3140048},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140051,
	abstract = {Nowadays, growing effort has been put to develop spatio-temporal clustering approaches that are capable of discovering interesting patterns in large spatio-temporal data streams. In this paper, we propose a 3-phase serial, density-contour based clustering algorithm called ST-COPOT, which can identify spatio-temporal cluster at multiple levels of density granularity. ST-COPOT takes the point cloud data as input and divides it into batches, next, it employs a non-parametric kernel density estimation approach and contouring algorithms to obtain spatial clusters; at last, spatio-temporal clusters are formed by identifying continuing relationships between spatial clusters in consecutive batches. Moreover, a novel data structure called contour polygon tree is introduced as a compact representation of the spatial clusters obtained for each batch for different density thresholds, and a family of novel distance functions that operate on contour polygon trees are proposed to identify continuing clusters. The experimental results on NYC taxi trips data show that ST-COPOT can effectively discover interesting spatio-temporal patterns in taxi pickup location streams.},
	address = {New York, NY, USA},
	articleno = {84},
	author = {Zhang, Yongli and Eick, Christoph F.},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140051},
	isbn = {9781450354905},
	keywords = {spatio-temporal data stream, spatio-temporal clustering, contour polygon tree, Spatio-temporal point cloud data},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {ST-COPOT: Spatio-temporal Clustering with Contour Polygon Trees},
	url = {https://doi.org/10.1145/3139958.3140051},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140059,
	abstract = {Geo-fencing is a location based service that allows sending of messages to users who enter/exit a specified geographical area, known as a geo-fence. Today, it has become one of the popular location based mobile marketing strategies. However, the process of designing geo-fences is presently manual, i.e. a retailer must specify the location and the radius of area around it to setup the geo-fences. Moreover, this process does not consider the user's preference towards the targeted product/service and thus, can compromise his/her experience of the app that sends these communications. We attempt to solve this problem by presenting a novel end-to-end system for automated design of affinity based smart geo-fences. Affinity towards a product/service refers to the user's interest in a product/service. Our unique formulation to estimate affinity, using historical app usage data, is sensitive to a user's location and thus, the affinity is termed as location sensitive product affinity (LSPA). The geo-fence logic tries to capture contiguous groups of locations where the affinity high. Experiments on real world e-commerce dataset reveals that geo-fences designed by our approach performs significantly better at accurately targeting the users who are interested in a product. We thus show that, using historical app usage data, geo-fences can be designed in an automated manner and can help enterprises target interested users with better accuracy as compared to the present industry practices.},
	address = {New York, NY, USA},
	articleno = {39},
	author = {Garg, Ankur and Choudhary, Sunav and Bajaj, Payal and Agrawal, Sweta and Kedia, Abhishek and Agrawal, Shubham},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140059},
	isbn = {9781450354905},
	keywords = {Spatial Data Mining, Geo-fencing},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Smart Geo-fencing with Location Sensitive Product Affinity},
	url = {https://doi.org/10.1145/3139958.3140059},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140063,
	abstract = {We present an algorithm for the following problem: Given a dataset D: = {T1,..., Tn} of data trajectories and a set Q: = {Q1,..., Qm} of query trajectories, each of which with a distance parameter ϵi ≥ 0, report, for each query trajectory Qi, all data trajectories within a Fr\'{e}chet distance of at most ϵi. As computing the Fr\'{e}chet distance is known to be computationally demanding, our algorithm uses a filter-and-refinement approach to reduce the number of query/data candidate pairs for which the Fr\'{e}chet distance needs to be computed exactly. As usually, we first use a hash-based range searching data structure to filter out candidate pairs whose minimum bounding rectangles are too far away. We then make extensive use of geometric properties of the Fr\'{e}chet distance to prune further candidate pairs in a series of further steps of the filter phase. In the refinement phase, i.e., when exactly computing the Fr\'{e}chet distance, we keep track of the boundary of the reachable space in the free space diagram to speed up the computation. Our algorithm is capable of using multiple threads in parallel; this is used to overlay the filter and refinement steps as well as the reporting of the output.},
	address = {New York, NY, USA},
	articleno = {100},
	author = {D\"{u}tsch, Fabian and Vahrenhold, Jan},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140063},
	isbn = {9781450354905},
	keywords = {Range searching, Fr\'{e}chet distance, Batched processing},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {A Filter-and-Refinement-Algorithm for Range Queries Based on the Fr\'{e}chet Distance (GIS Cup)},
	url = {https://doi.org/10.1145/3139958.3140063},
	year = {2017},
}

@inproceedings{10.1145/3139958.3140064,
	abstract = {Consider a set P of trajectories (polygonal lines in R2), and a query given by a trajectory Q and a threshold ϵ &gt; 0. To answer the query we wish to find all trajectories P ∈ P such that δF(P, Q) ≤ ϵ, where δF denotes the Fr\'{e}chet distance. We present an approach to efficiently answer a large number of queries for the same set P. Key ingredients are (a) precomputing a spatial hash that allows us to quickly find trajectories that have endpoints near Q; (b) precomputing simplifications on all trajectories in P; (c) using the simplifications and optimizations of the decision algorithm to efficiently decide δF(P, Q) ≤ ϵ for most P ∈ P.},
	address = {New York, NY, USA},
	articleno = {101},
	author = {Buchin, Kevin and Diez, Yago and van Diggelen, Tom and Meulemans, Wouter},
	booktitle = {Proceedings of the 25th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3139958.3140064},
	isbn = {9781450354905},
	keywords = {trajectories, range searching, Fr\'{e}chet distance},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '17},
	title = {Efficient trajectory queries under the Fr\'{e}chet distance (GIS Cup)},
	url = {https://doi.org/10.1145/3139958.3140064},
	year = {2017},
}

@article{10.1145/3141772,
	abstract = {The main achievements of spatio-temporal modelling in the field of Geographic Information Science that spans the past three decades are surveyed. This article offers an overview of: (i) the origins and history of Temporal Geographic Information Systems (T-GIS); (ii) relevant spatio-temporal data models proposed; (iii) the evolution of spatio-temporal modelling trends; and (iv) an analysis of the future trends and developments in T-GIS. It also presents some current theories and concepts that have emerged from the research performed, as well as a summary of the current progress and the upcoming challenges and potential research directions for T-GIS. One relevant result of this survey is the proposed taxonomy of spatio-temporal modelling trends, which classifies 186 modelling proposals surveyed from more than 1,450 articles.},
	address = {New York, NY, USA},
	articleno = {30},
	author = {Siabato, Willington and Claramunt, Christophe and Ilarri, Sergio and Manso-Callejo, Miguel Angel},
	doi = {10.1145/3141772},
	issn = {0360-0300},
	issue_date = {March 2019},
	journal = {ACM Comput. Surv.},
	keywords = {time geography, temporal models, temporal GIS, survey, spatio-temporal databases, literature review, Spatio-temporal models},
	month = {apr},
	number = {2},
	numpages = {41},
	publisher = {Association for Computing Machinery},
	title = {A Survey of Modelling Trends in Temporal GIS},
	url = {https://doi.org/10.1145/3141772},
	volume = {51},
	year = {2018},
}

@inproceedings{10.1145/3148011.3148025,
	abstract = {The resources published on the Web of data are often described by spatial references such as coordinates. The common data linking approaches are mainly based on the hypothesis that spatially close resources are more likely to represent the same thing. However, this assumption is valid only when the spatial references that are compared have been produced with the same positional accuracy, and when they actually represent the same spatial characteristic of the resources captured in an unambiguous way. Otherwise, spatial distance-based matching algorithms may produce erroneous links. In this article, we first suggest to formalize and acquire the knowledge about the spatial references, namely their positional accuracy, their geometric modeling, their level of detail, and the vagueness of the spatial entities they represent. We then propose an interlinking approach that dynamically adapts the way spatial references are compared, based on this knowledge.},
	address = {New York, NY, USA},
	articleno = {12},
	author = {Feliachi, Abdelfettah and Abadie, Nathalie and Hamdi, Fay\c{c}al},
	booktitle = {Proceedings of the 9th Knowledge Capture Conference},
	doi = {10.1145/3148011.3148025},
	isbn = {9781450355537},
	keywords = {linked data, instances matching, Spatial references},
	location = {<conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {K-CAP '17},
	title = {An Adaptive Approach for Interlinking Georeferenced Data},
	url = {https://doi.org/10.1145/3148011.3148025},
	year = {2017},
}

@inproceedings{10.1145/3148044.3148045,
	abstract = {The age-old practice of cartography has undergone fundamental shifts with the advent of the digital age. Today's digital maps are often crowd-sourced, allow interactive route planning, and may contain live updates, such as traffic congestion state. In this paper, we take the concept of maps one step further by introducing a new generation of maps, which contain the additional functionality of showing multi-resolution spatio-temporal events, extracted dynamically from social media streams. Building such maps requires developing a scalable and efficient system to deal with a variety of unstructured data streams, applying sentiment analysis and multi-dimensional clustering techniques to extract relevant Events of Interest (EoI) at different map scales, and inferring the spatio-temporal scope of detected events. This paper presents a novel system that extracts events from social data at different levels of spatio-temporal granularities. The system implements a hierarchical in-memory spatio-temporal indexing scheme to support efficient access to data streams, as well as for memory flushing purposes. Data streams are first processed to extract events at a local scale. Next, we determine the proper spatio-temporal scope and the level of abstraction for detected events at a global scale. This allows us to show live multi-resolution events in correspondence to the scale of the view -- when viewing at a city scale, we see events of higher significance, while zooming in to a neighborhood highlights events of a more local interest.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Rehman, Faizan Ur and Afyouni, Imad and Lbath, Ahmed and Basalamah, Saleh},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Analytics for Local Events and News},
	doi = {10.1145/3148044.3148045},
	isbn = {9781450355001},
	keywords = {Spatio-Temporal Scope, Multi-Resolution, Event-Enriched Maps, Crowdsourced Data},
	location = {Redondo Beach, CA, USA},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {LENS'17},
	title = {Understanding the Spatio-Temporal Scope of Multi-scale Social Events},
	url = {https://doi.org/10.1145/3148044.3148045},
	year = {2017},
}

@inproceedings{10.1145/3148150.3148157,
	abstract = {Organizations can be identified by a myriad of terms apart from their official names. While abbreviations remain a common "short-name" to reference organizations, the prevalence of other short-names has risen in conjunction with social networks. When a user enters a short-name as a locational search query, it remains a challenge to infer the relationship between the short-name and the organization it ostensibly represents. For a number of organizations around the Washington D.C., Maryland, and Virginia area, we first generate a list of possible short-names for each of them. We then search through their tweets to build a corpus of short-names associated with each organization. By measuring our list against the corpus, we can identify potential short-names, and return the location of the organization.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Wajid, Faizan and Wei, Hong and Samet, Hanan},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Recommendations for Location-Based Services and Social Networks},
	doi = {10.1145/3148150.3148157},
	isbn = {9781450354998},
	keywords = {Twitter, Toponym Recognition, Short Names, GeoNames},
	location = {Redondo Beach, CA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {LocalRec'17},
	title = {Identifying Short-Names for Place Entities from Social Networks},
	url = {https://doi.org/10.1145/3148150.3148157},
	year = {2017},
}

@inproceedings{10.1145/3148150.3148159,
	abstract = {In this paper an application is developed that functions similar to a recommender system and allows to find appropriate OpenStreetMap (OSM) tags by querying co-occurring keys and tags, as well as similar sets of tags in the database. A user may enter key(s) or key-value pair(s), even using wildcard substitution for both, in order to find keys or key-value pairs that are used in combination with the entered ones. Moreover, the top-k matching tag sets are also presented. The results are then top-k ranked, based on the frequency of the occurrence of each distinct set in the database. This information may enable a user to find the most comprehensive and best fitting tag set for an OSM element. This assumption is examined in an evaluation where the precision and recall metrics for both approaches are computed and compared. Our approach helps discovering combinations of tags and their usage frequency in contrast to common recommender systems that focus on classifying or clustering elements and finding the most accurate (single) class or cluster rather than sets of tags.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Silbernagl, Doris and Krismer, Nikolaus and Augsten, Nikolaus and Specht, G\"{u}nther},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Recommendations for Location-Based Services and Social Networks},
	doi = {10.1145/3148150.3148159},
	isbn = {9781450354998},
	keywords = {Top-k Retrieval, Set Similarity, Recommendation, OpenStreetMap},
	location = {Redondo Beach, CA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {LocalRec'17},
	title = {Recommending OSM Tags To Improve Metadata Quality},
	url = {https://doi.org/10.1145/3148150.3148159},
	year = {2017},
}

@inproceedings{10.1145/3148160.3148168,
	abstract = {Spatio-temporal data from Moving Objects is often available as a live data stream and needs to be processed accordingly. The trajectories update while the objects are moving. Even though the data arrives in a streaming manner, significant delays between location updates are possible, resulting in delayed or less accurate results of continuous queries on the trajectories. That can be an important issue when using the queries for real-world decisions, e. g., with Automatic Identification System (AIS) data for maritime navigation. Additionally, short-time predictions can be useful to get early warnings for critical situations. Previous work does not cover this problem for streaming applications, as existing systems are mainly Moving Object Databases, which are not optimized for streaming data. In this work, we describe how spatio-temporal inter and extrapolation can be integrated into Data Stream Management Systems and which challenges have to be solved doing so.},
	address = {New York, NY, USA},
	author = {Brandt, Tobias and Grawunder, Marco},
	booktitle = {Proceedings of the 8th ACM SIGSPATIAL Workshop on GeoStreaming},
	doi = {10.1145/3148160.3148168},
	isbn = {9781450354929},
	keywords = {Trajectories, Moving Objects, Maritime, Data Stream Management Systems},
	location = {Redondo Beach, CA, USA},
	numpages = {8},
	pages = {49–56},
	publisher = {Association for Computing Machinery},
	series = {IWGS'17},
	title = {Moving Object Stream Processing With Short-Time Prediction},
	url = {https://doi.org/10.1145/3148160.3148168},
	year = {2017},
}

@inproceedings{10.1145/3149092.3149096,
	abstract = {Digital maps have been playing an important role in our everyday life. They are in our car navigation systems, smart phones and smart watches etc. And we have many online map providers like Google, Bing and Here. However, traditional ways of surveying and mapping of roads by human beings working on the ground usually takes a lot of time and labor work. Thus we are proposing here an automatic solution by utilizing aerial images with geographical information. Road maps are automatically generated by: First processing aerial images using a modified deep neural network based on GoogleNet to identify the most likely road part. Then the road parts are assigned with their corresponding geographical coordinates. Finally the image data and geographical data are fusion together in Geotiff format, which might be used for map updating, car navigation and autonomous vehicles.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Ye, Yun and Yilmaz, Alper},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on High-Precision Maps and Intelligent Applications for Autonomous Vehicles},
	doi = {10.1145/3149092.3149096},
	isbn = {9781450354974},
	keywords = {image processing, geotiff images, digital maps, deep neural network, aerial image classification},
	location = {Redondo Beach, California},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {AutonomousGIS '17},
	title = {An automatic pipeline for mapping roads from aerial images},
	url = {https://doi.org/10.1145/3149092.3149096},
	year = {2017},
}

@inproceedings{10.1145/3149572.3149600,
	abstract = {Working on a construction site is always risky because of its dynamic nature. Despite numerous efforts to reduce fatalities on construction sites, fatal accidents continue to occur. The advancements in location acquisition and mobile communication techniques offer various means to monitor construction sites using mobility data. Information derived from such data offers tremendous opportunities to track and analyze the mobility of moving objects, that can contributes to improve safety management systems. However, such systems requires preprocessed positioning data of moving objects as raw data captured from location acquisition devices such as Global Positioning System (GPS) device is generally available as discrete points and do not hold enough information to understand the mobility. In this article, we have conducted a review of applications that use mobility data known as trajectories. After a detailed literature review, an application of trajectories carrying spatio-temporal information for worker safety on construction sites is discussed. The application collects GPS data and reconstructs workers' trajectories using different data processing algorithms. Once trajectory data is cleaned and processed, it can further be enriched with semantic and contextual information to enable the desired interpretation of workers' movements on construction sites and ultimately attempts to improve work zone safety by reducing fatalities. However, the scope of this paper is kept limited to data processing of GPS trajectories.},
	address = {New York, NY, USA},
	author = {Arslan, Muhammad and Cruz, Christophe and Roxin, Ana-Maria and Ginhac, Dominique},
	booktitle = {Proceedings of the 9th International Conference on Information Management and Engineering},
	doi = {10.1145/3149572.3149600},
	isbn = {9781450353373},
	keywords = {mobility, fatal accidents, construction sites, Health and Safety (H&amp;S)},
	location = {Barcelona, Spain},
	numpages = {6},
	pages = {211–216},
	publisher = {Association for Computing Machinery},
	series = {ICIME 2017},
	title = {Using Spatio-temporal Trajectories to Monitor Construction Sites for Safety Management},
	url = {https://doi.org/10.1145/3149572.3149600},
	year = {2017},
}

@inproceedings{10.1145/3149808.3149813,
	abstract = {Volunteered Geographic Information (VGI) projects, such as Open-StreetMap (OSM) enable the public to contribute to the collection of spatial data. In OSM, users may deviate from spatial feature annotation guidelines and create new tags (i.e. key=value pairs), even if recommended tags exist. This is problematic, as undocumented tags have no set meaning, and they potentially contribute to the dataset heterogeneity and thus reduce usability. This paper proposes an unsupervised approach to identify equivalent documented attribute keys to the used undocumented keys. Based on their extensional definitions through their values, co-occurring keys and geometries of the features they annotate, the semantic similarity of OSM keys is evaluated. The approach has been tested on the OSM dataset for the state of Victoria, Australia. Results have been evaluated against a set of manually detected equivalent keys and show that the method is plausible, but may fail if some assumptions about tag use are not enforced, e.g., semantically unique tags.},
	address = {New York, NY, USA},
	author = {Majic, Ivan and Winter, Stephan and Tomko, Martin},
	booktitle = {Proceedings of the 1st Workshop on Artificial Intelligence and Deep Learning for Geographic Knowledge Discovery},
	doi = {10.1145/3149808.3149813},
	isbn = {9781450354981},
	keywords = {volunteered geographic information, spatial database, semantic similarity, openstreetmap, data quality, data cleaning, VGI, OSM},
	location = {Los Angeles, California},
	numpages = {9},
	pages = {24–32},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '17},
	title = {Finding equivalent keys in openstreetmap: semantic similarity computation based on extensional definitions},
	url = {https://doi.org/10.1145/3149808.3149813},
	year = {2017},
}

@inproceedings{10.1145/3149808.3149816,
	abstract = {With large amounts of digital map archives becoming available, the capability to automatically extracting information from historical maps is important for many domains that require long-term geographic data, such as understanding the development of the landscape and human activities. In the previous work, we built a system to automatically recognize geographic features in historical maps using Convolutional Neural Networks (CNN). Our system uses contemporary vector data to automatically label examples of the geographic feature of interest in historical maps as training samples for the CNN model. The alignment between the vector data and geographic features in maps controls if the system can generate representative training samples, which has a significant impact on recognition performance of the system. Due to the large number of training data that the CNN model needs and tens of thousands of maps needed to be processed in an archive, manually aligning the vector data to each map in an archive is not practical. In this paper, we present an algorithm that automatically aligns vector data with geographic features in historical maps. Existing alignment approaches focus on road features and imagery and are difficult to generalize for other geographic features. Our algorithm aligns various types of geographic features in document images with the corresponding vector data. In the experiment, our alignment algorithm increased the correctness and completeness of the extracted railroad and river vector data for about 100\% and 20\%, respectively. For the performance of feature recognition, the aligned vector data had a 100\% improvement on the precision while maintained a similar recall.},
	address = {New York, NY, USA},
	author = {Duan, Weiwei and Chiang, Yao-Yi and Knoblock, Craig A. and Jain, Vinil and Feldman, Dan and Uhl, Johannes H. and Leyk, Stefan},
	booktitle = {Proceedings of the 1st Workshop on Artificial Intelligence and Deep Learning for Geographic Knowledge Discovery},
	doi = {10.1145/3149808.3149816},
	isbn = {9781450354981},
	keywords = {vector data alignment, map processing, historical documents},
	location = {Los Angeles, California},
	numpages = {10},
	pages = {45–54},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '17},
	title = {Automatic alignment of geographic features in contemporary vector data and historical maps},
	url = {https://doi.org/10.1145/3149808.3149816},
	year = {2017},
}

@inproceedings{10.1145/3149858.3149859,
	abstract = {Our project involves building a platform able to retrieve, map and analyze the occurrences of place names in fictional novels published between 1800 and 1914 and whose action occurs wholly or partly in Paris. We describe a proof of concept using queries made via the TXM textual analysis platform for the extraction of street names. Then, we propose a fully automatic process using the named entity recognition (NER) components of the PERDIDO platform. This paper describes some encouraging initial results obtained by combining NLP approaches (NER methods) with textometric tools for the automated geoparsing of street names.},
	address = {New York, NY, USA},
	author = {Moncla, Ludovic and Gaio, Mauro and Joliveau, Thierry and Lay, Yves-Fran\c{c}ois Le},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Geospatial Humanities},
	doi = {10.1145/3149858.3149859},
	isbn = {9781450354967},
	keywords = {Named Entity Recognition, Geoparsing, Geographical Information Retrieval, Digital Humanities},
	location = {Redondo Beach, CA, USA},
	numpages = {8},
	pages = {1–8},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '17},
	title = {Automated Geoparsing of Paris Street Names in 19th Century Novels},
	url = {https://doi.org/10.1145/3149858.3149859},
	year = {2017},
}

@inproceedings{10.1145/3149858.3149866,
	abstract = {The digital geohumanities---and geographic computation generally--- have advanced greatly by representing phenomena within geographic coordinate systems. More specifically, most visualizations and analyses only proceed once data are rendered into a single coordinate system via geolocation and one or more projections. But does it follow that geographic computation should require all phenomena to be represented in Euclidean or spherical geometry in a singular, absolute, Newtonian space?We suggest an approach to pluralizing the spaces available to geographic computation. We both supplement the technical architecture for projections and subtly reframe the purpose and meaning of projections. What we term numerical, generalized projections thereby become more central to GISystems. We suggest how existing libraries might be modified with minimal disruption (taking the widespread and foundational proj.4 library as example). We also envision modifications to existing OGC technical specifications for projections and coordinate systems. Finally, in conversation with the interpretative practice and nuanced spatialities of the digital geohumanities and critical geography, we further extend generalized projections to encompass spatial multiplicity, fragmented spaces, wormholes, and an expanded role for interruptions.This will facilitate: 1) interpretative approaches to scholarship and diverse constructions of space common in the humanities; 2) computational engagement with the ontological and epistemological commitments to relational space of critical human geography; and 3) scientific efforts to understand complex systems in the spaces and times that emerge from those systems' dynamics, revisiting a desire common in early quantitative geography; and 4) the desire for a broad basis of understanding geographic information in GIScience.},
	address = {New York, NY, USA},
	author = {Bergmann, Luke R. and O'Sullivan, David},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Geospatial Humanities},
	doi = {10.1145/3149858.3149866},
	isbn = {9781450354967},
	keywords = {relational space, literary cartography, geographical imaginations, geographic information, generalized map projections},
	location = {Redondo Beach, CA, USA},
	numpages = {8},
	pages = {31–38},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '17},
	title = {Computing with many spaces: Generalizing projections for the digital geohumanities and GIScience},
	url = {https://doi.org/10.1145/3149858.3149866},
	year = {2017},
}

@article{10.1145/3150525,
	abstract = {We introduce the concept of using a flow diagram to compactly represent the segmentation of a large number of state sequences according to a set of criteria. We argue that this flow diagram representation gives an intuitive summary that allows the user to detect patterns within the segmentations. In essence, our aim is to generate a flow diagram with a minimum number of nodes that models a segmentation of the states in the input sequences. For a small number of state sequences we present efficient algorithms to compute a minimal flow diagram. For a large number of state sequences, we show that it is unlikely that efficient algorithms exist. Specifically, the problem is W[1]-hard if the number of state sequences is taken as a parameter. We introduce several heuristics for this problem. We argue about the usefulness of the flow diagram by applying the algorithms to two problems in sports analysis, and evaluate the performance of our algorithms on a football dataset and synthetic data.},
	address = {New York, NY, USA},
	articleno = {1.7},
	author = {Buchin, Kevin and Buchin, Maike and Gudmundsson, Joachim and Horton, Michael and Sijben, Stef},
	doi = {10.1145/3150525},
	issn = {1084-6654},
	issue_date = {2017},
	journal = {ACM J. Exp. Algorithmics},
	keywords = {state sequence, sports analytics, network analysis, group segmentation, football, experimental, Flow networks},
	month = {dec},
	numpages = {23},
	publisher = {Association for Computing Machinery},
	title = {Compact Flow Diagrams for State Sequences},
	url = {https://doi.org/10.1145/3150525},
	volume = {22},
	year = {2017},
}

@inproceedings{10.1145/3150919.3150924,
	abstract = {Distance computation between polylines or trajectories is a key point to assess similarity between geometrical objects. This paper describes a new optimized algorithm to compute discrete Fr\'{e}chet distance which aims to lower computation time and improve precision. This algorithm is applied to GPS trajectories. It includes a filtering, pruning and an enhancement process. Thanks to this algorithm, big data trajectory repositories can be mined. This process is validated on a large trajectory dataset.},
	address = {New York, NY, USA},
	author = {Devogele, Thomas and Etienne, Laurent and Esnault, Maxence and Lardy, Florian},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3150919.3150924},
	isbn = {9781450354943},
	keywords = {trajectory mining, similarity measure, linear distance, filtering process, GPS dataset, Fr\'{e}chet distance},
	location = {Redondo Beach, CA, USA},
	numpages = {9},
	pages = {11–19},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '17},
	title = {Optimized Discrete Fr\'{e}chet Distance between trajectories},
	url = {https://doi.org/10.1145/3150919.3150924},
	year = {2017},
}

@inproceedings{10.1145/3151547.3151553,
	abstract = {The widely used Huff model is designed to estimate the spatial probability distribution of shopping centre patronage based on a shopping centre's attractiveness and the cost of a customer's travel. Here, we calibrate the Huff model for the city of Shenzhen, China, using GPS taxi trajectory data for one million taxi journeys. Using Geographical Weighted Regression to fit the model, we show that there is significant geographical variation in best estimates of the Huff parameters of attractiveness and cost. To explain this variation, we use open-source house price sales' data as a proxy for customers' wealth in each region. Regression results demonstrate a significant linear relationship between localised house prices and the Huff model parameter of attractiveness, suggesting that wealthy customers are more sensitive to shopping centre attractiveness than customers with less wealth. We present this as a novel discovery.},
	address = {New York, NY, USA},
	author = {Gong, Shuhui and Cartlidge, John and Yue, Yang and Qiu, Guoping and Li, Qingquan and Xin, Jingyu},
	booktitle = {Proceedings of the 10th ACM SIGSPATIAL Workshop on Computational Transportation Science},
	doi = {10.1145/3151547.3151553},
	isbn = {9781450354912},
	keywords = {Taxi data, Shopping behavior, OLS, House price data, GWR},
	location = {Redondo Beach, CA, USA},
	numpages = {6},
	pages = {30–35},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'17},
	title = {Geographical Huff Model Calibration using Taxi Trajectory Data},
	url = {https://doi.org/10.1145/3151547.3151553},
	year = {2017},
}

@inproceedings{10.1145/3151547.3151554,
	abstract = {The on-demand economy has attracted significant attention in recent years, with a rapid growth in on-demand services ranging from ride-hailing to package delivery and grocery pickup. However, real-world spatio-temporal data that can be used for evaluating research on on-demand brokers design and supply-demand regulation are either not publicly available or are very limited in their spatial coverage. Research efforts in generating synthetic spatio-temporal datasets such as traffic generators have only focused on one side of the business model, particularly the demand side, and thus are not convenient for studying market variations such as the problem of supply-demand imbalance. In addition, many of these generators do not accurately reflect real-world data characteristics. In this paper, we propose a supply and demand aware framework for generating synthetic datasets for the purpose of designing on-demand spatial service brokers, while also capturing real-world data characteristics by leveraging multiple publicly available data sources. We also present an evaluation of the quality and performance of our proposed framework.},
	address = {New York, NY, USA},
	author = {Ali, Reem Y. and Li, Yan and Shekhar, Shashi and Athavale, Shounak and Marsman, Eric},
	booktitle = {Proceedings of the 10th ACM SIGSPATIAL Workshop on Computational Transportation Science},
	doi = {10.1145/3151547.3151554},
	isbn = {9781450354912},
	keywords = {synthetic data with real-world characteristics, synthetic data generation, supply and demand aware, spatial service broker, on-demand services, on-demand brokers},
	location = {Redondo Beach, CA, USA},
	numpages = {6},
	pages = {36–41},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'17},
	title = {Supply and Demand Aware Synthetic Data Generation for On-demand Traffic with Real-world Characteristics},
	url = {https://doi.org/10.1145/3151547.3151554},
	year = {2017},
}

@inproceedings{10.1145/3151759.3151825,
	abstract = {Emergency is an event that can threaten and disrupt a someone's life. Emergency situation occur suddenly with unexpected place and time. Different emergency situations may need different emergency unit type, either polices, ambulances or fire fighters. Most countries have implemented single number to handle emergency situation where the users will get the adequate emergency units as they need. To identify the coverage of each emergency unit for each emergency type, a multi-layered network Voronoi diagram is used. Then, finding nearest emergency unit and alternate units from a certain location can be determined by using Kth-Nearest Neighbor algorithm applied in predefined multi-layered network Voronoi diagram..},
	address = {New York, NY, USA},
	author = {Nuhrintama, Ashari Fachrizal and Adhinugraha, Kiki Maulana},
	booktitle = {Proceedings of the 19th International Conference on Information Integration and Web-Based Applications \&amp; Services},
	doi = {10.1145/3151759.3151825},
	isbn = {9781450352994},
	keywords = {network voronoi diagram, multi layered NVD, kth NN, emergency services},
	location = {Salzburg, Austria},
	numpages = {5},
	pages = {380–384},
	publisher = {Association for Computing Machinery},
	series = {iiWAS '17},
	title = {Kth NN query with multi layered network voronoi diagram for first aid emergency system},
	url = {https://doi.org/10.1145/3151759.3151825},
	year = {2017},
}

@inproceedings{10.1145/3151759.3151838,
	abstract = {Urban planning data are of big importance to both general public and domain experts like civil engineers, architects or urban planning specialists. Typically, the data are scattered within many datasets containing both geographical knowledge and taxonomical knowledge. Features representing same objects in two different datasets often do not have any connection except geometry. Complex queries over multiple datasets are rather complicated and data have to be preformed, due to the distributed nature of such data, as well as their complexity. We present a case study on ontology modeling of urban planning data of the City of Prague. We discuss the ontological and spatial nature of the domain, followed by the design and formalization of the context-sensitive Urban ontology together with GeoSPARQL queries showing the usage of the ontology for dataset exploration and data search.},
	address = {New York, NY, USA},
	author = {Med, Michal and K\v{r}emen, Petr},
	booktitle = {Proceedings of the 19th International Conference on Information Integration and Web-Based Applications \&amp; Services},
	doi = {10.1145/3151759.3151838},
	isbn = {9781450352994},
	keywords = {urban planning, spatial ontologies, GeoSPARQL},
	location = {Salzburg, Austria},
	numpages = {5},
	pages = {457–461},
	publisher = {Association for Computing Machinery},
	series = {iiWAS '17},
	title = {Context-based ontology for urban data integration},
	url = {https://doi.org/10.1145/3151759.3151838},
	year = {2017},
}

@inproceedings{10.1145/3152178.3152182,
	abstract = {This paper performs a quantitative comparison of open-source data available on the Internet for the fine-grain mapping of land use. Three points of interest (POI) data sources--Google Places, Bing Maps, and the Yellow Pages--and one volunteered geographic information data source--Open Street Map (OSM)--are compared with each other at the parcel level for San Francisco with respect to a proposed fine-grain land-use taxonomy. The sources are also compared to coarse-grain authoritative data which we consider to be the ground truth. Results show limited agreement among the data sources as well as limited accuracy with respect to the authoritative data even at coarse class granularity. We conclude that POI and OSM data do not appear to be sufficient alone for fine-grain land-use mapping.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Deng, Xueqing and Newsam, Shawn},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
	doi = {10.1145/3152178.3152182},
	isbn = {9781450354950},
	keywords = {volunteered geographic information, points of interest, Land use},
	location = {Redondo Beach, CA, USA},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {UrbanGIS'17},
	title = {Quantitative Comparison of Open-Source Data for Fine-Grain Mapping of Land Use},
	url = {https://doi.org/10.1145/3152178.3152182},
	year = {2017},
}

@inproceedings{10.1145/3152178.3152183,
	abstract = {Several geospatial problems like urban subsurface analysis involve data from multiple domains and nature. For instance, an urban infrastructure analysis must take into account not only the urban elements but their geological environment. This requires the definition of hybrid schemes and algorithms that keep the dimensionality and domain of the data, while allowing the joint management of both representations in a combined way. In addition, a proper 3D visualization method that shows those heterogeneous data together can be very useful for geoscientific and GIS professionals. In this paper, we present the foundations of a real-time 3D visualization framework capable of rendering field and vector data, as well as a set of operations that can solve many problems for engaging data from different domains. We propose the use of the Stack-Based Representation of Terrains for field data which provides a whole 3D representation of volumetric terrains while allowing an efficient memory usage. The resulting hybrid framework can help geoscientists and engineers to analyze 3D complex geospatial data and make decisions at a glance.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Graciano, Alejandro and Rueda, Antonio J. and Ortega, Lidia and Feito, Francisco R.},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
	doi = {10.1145/3152178.3152183},
	isbn = {9781450354950},
	keywords = {Stack-Based Representation, Hybrid GIS, Geovisualization, Geomodelling, 3D GIS},
	location = {Redondo Beach, CA, USA},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {UrbanGIS'17},
	title = {Towards a hybrid framework for the visualization and analysis of 3D spatial data},
	url = {https://doi.org/10.1145/3152178.3152183},
	year = {2017},
}

@inproceedings{10.1145/3152178.3152194,
	abstract = {The city of Curitiba in southern Brazil is considered to be the cradle of the Bus Rapid Transit (BRT) system. Curitiba has a population of around 1.9 million people and has a higher development index than Brazil in general. A master plan approved in the middle of the 1960's has guided the development of the city in a Transit Oriented Development (TOD) direction by zoning for high development densities close to the five BRT trunk lines in so-called structural development in Curitiba. The objective was to examine if the BRT system could have been a motivator for property development, and if so, to what extend. This paper presents a perspective to examine property development: Timing of Development, as the relationship between the number of years after construction of BRT lines buildings were constructed, and the distances of these buildings from the BRT lines. Results from the entire BRT system showed that a greater "time lag" of property development following BRT development also meant that the property in question was located further away from a BRT line, suggesting that areas close to the BRT were popular.},
	address = {New York, NY, USA},
	articleno = {16},
	author = {Bergman, Klara and Franklin, Joel and Gadda, Tatiana and Kozievitch, N\'{a}dia P.},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL Workshop on Smart Cities and Urban Analytics},
	doi = {10.1145/3152178.3152194},
	isbn = {9781450354950},
	keywords = {Timing of Development, TOD, Curitiba, BRT},
	location = {Redondo Beach, CA, USA},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {UrbanGIS'17},
	title = {The Relationship between Timing of Development and Bus Rapid Transit},
	url = {https://doi.org/10.1145/3152178.3152194},
	year = {2017},
}

@inproceedings{10.1145/3152341.3152348,
	abstract = {Social media data which capture long-term personal travel activities as a set of space-time points (time series) become widely used for human mobility study. The space-time points representing individual activities are massive and need aggregation upon time dimension (besides space dimension) to show temporal mobility patterns. During the temporal aggregations, time series are sliced into different temporal layers, and the aggregation results could be impacted by four parameters, including layer size (time interval of each tempoal layer), start placement (the start time of the first layer), amount of overlap between two consecutive layers, and time series extent (temporal scope of the datasets for aggregation). Different parameterizations result in different mobility patterns, known as the "Modifiable Temporal Unit Problem" (MTUP; on the analogy of the "Modifiable Areal Unit Problem" or MAUP). While the general effects of MTUP are well examined in previous studies, MTUP is often ignored in trajectory reconstructions using sparse social media data. To fill this research gap, this paper will explore the impact of different temporal aggregation schemas (parameterizations) on the discovery of human mobility patterns using geo-tagged tweets within a 3D geospatial analytical system. The case study reveals that MTUP is significant during the process of detecting an individual's daily representative (regular) trajectories based on sparse online footprints. Comprehensive analysis on multiple aggregation results with different parameters could improve understanding of an individual's regular daily travel patterns. The interactive analytical system and visualization methods proposed by this study could minimize MTUP impact and help avoid false arguments.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Liu, Xinyi and Huang, Qunying and Li, Zhenlong and Wu, Meiliu},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Prediction of Human Mobility},
	doi = {10.1145/3152341.3152348},
	isbn = {9781450355018},
	keywords = {social media, online trajectory, human mobility, MTUP},
	location = {Redondo Beach, CA, USA},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {PredictGIS'17},
	title = {The impact of MTUP to explore online trajectories for human mobility studies},
	url = {https://doi.org/10.1145/3152341.3152348},
	year = {2017},
}

@inproceedings{10.1145/3152341.3152349,
	abstract = {This work proposes a deep neural network approach known as the column-structured deep neural network (COL-DNN-R) for predicting crowd density in an indoor environment using historical Wi-Fi traces of individual visitors. With a structure designed to minimize feature engineering, COL-DNN accepts raw features such as crowd density, opening and closing hours and peak visitor counts for extracting features. The extracted features are used by a regression model R for predicting the crowd densities. Standard regression models such as MLP, RF and SVM can be used as R. Experiments are performed to investigate the effect of feature representation and model structure on the prediction accuracy. Experiment results show the best prediction accuracy is obtained using features extracted by COL-DNN and using MLP as the regression model, i.e., R = MLP.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Sudo, Akihito and Teng, Teck-Hou and Lau, Hoong Chuin and Sekimoto, Yoshihide},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Prediction of Human Mobility},
	doi = {10.1145/3152341.3152349},
	isbn = {9781450355018},
	keywords = {Indoor Crowd Prediction, Feature Extraction, Deep Neural Network},
	location = {Redondo Beach, CA, USA},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {PredictGIS'17},
	title = {Predicting Indoor Crowd Density using Column-Structured Deep Neural Network},
	url = {https://doi.org/10.1145/3152341.3152349},
	year = {2017},
}

@inproceedings{10.1145/3152465.3152470,
	abstract = {This paper presents a GIS based dynamic model of fire spread using heterogeneous cellular automation simulation, given a set of stochastic ignition points and initial environmental settings. Modeling forests and concrete buildings in different types of cells, we have developed the algorithm for fire spread in hybrid urban scenario. As a consequence, our method can model the fire spread process of fire spread from forest areas to urban areas and vice versa in a single experiment. The model is built upon B-S standardized emergency management protocols, which we have customized from the Web Processing Service (WPS). And the standardized emergency management protocols can manage a set of different disaster models in a common structure. The structure of the simulation platform is established to dynamically visualize the fire spread process. And a comparison of experimental results with different wind velocity is carried out to illustrate the model.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Li, Zhanghua and Wang, Fei and Zheng, Xiaocui and Jiang, Wenyu and Meng, Qingxiang and Liu, Binbin},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
	doi = {10.1145/3152465.3152470},
	isbn = {9781450354936},
	keywords = {standardization, heterogeneous cellular automation, emergency management, GIS, Fire spread model},
	location = {Redondo Beach, CA, USA},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {EM-GIS '17},
	title = {GIS Based Dynamic Modeling of Fire Spread with Heterogeneous Cellular Automation Model and Standardized Emergency Management Protocol},
	url = {https://doi.org/10.1145/3152465.3152470},
	year = {2017},
}

@inproceedings{10.1145/3152465.3152471,
	abstract = {Rapid emergency resource supply is one of the most issue in effective emergency response and management for crisis and disasters. To support the reasonable emergency resource supply under emergency conditions, emergency material scheduling should take full consideration of time cost, economic cost and the priorities of the resources demand. In this paper, we propose emergency resource scheduling model (ERSM) to determine the optimal route for emergency resource scheduling based on Geographic Information Systems (GIS). The ERSM evaluate optimal emergency rescue route under different emergency conditions based on Analytic Hierarchy Process (AHP) method, which introduced multiple indicators including the traffic road condition, actual travel distance, urgency of disaster relief spot. Meanwhile the solomon insertion-type heuristic algorithm (SIHA) is proposed to identify the optimal route of emergency resource scheduling. Then ArcGIS software with network analysis extension is adopted to display optimal delivery routes. ERSM provides the emergency mangers with optimal route of emergency resource for quick and efficient responses after disaster.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Feng, Guoliang and Su, Guofeng and Sun, Zhanhui},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
	doi = {10.1145/3152465.3152471},
	isbn = {9781450354936},
	keywords = {Solomon insertion-type heuristic algorithm, Geographic Information Systems, Emergency resource scheduling model, Emergency management, Analytic Hierarchy Process},
	location = {Redondo Beach, CA, USA},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {EM-GIS '17},
	title = {Optimal route of emergency resource scheduling based on GIS},
	url = {https://doi.org/10.1145/3152465.3152471},
	year = {2017},
}

@inproceedings{10.1145/3155902.3155906,
	abstract = {There are many historical itineraries that describe routes as a list of settlements and the travel distances along the way. They are an important source of information for various kinds of research in the humanities, providing insights into for example the development of human mobility and historical road networks.In this paper, we develop an approach for aligning these itineraries with a modern gazetteer (database of places). We combine textual information (historical toponyms) and spatial information (travel distances) into a Hidden Markov model. Naively calculating a maximum likelihood explanation is slow, but careful algorithm engineering achieves high performance suitable for user interaction.We demonstrate the practical potential of our approach by geo-referencing 48 itineraries (containing 691 stops) from two important historical guidebooks published in 1563 and 1597: our approach is fast and accurate. Additionally, we show how to use sensitivity analysis to power an efficient user interface for quality assurance.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Budig, Benedikt and van Dijk, Thomas C.},
	booktitle = {Proceedings of the 11th Workshop on Geographic Information Retrieval},
	doi = {10.1145/3155902.3155906},
	isbn = {9781450353380},
	keywords = {Toponyms, Optimization, Itinerary Resolution, Historical Itineraries, Hidden Markov Models, Deep Georeferencing, Algorithms},
	location = {Heidelberg, Germany},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {GIR'17},
	title = {Journeys of the Past: A Hidden Markov Approach to Georeferencing Historical Itineraries},
	url = {https://doi.org/10.1145/3155902.3155906},
	year = {2017},
}

@inproceedings{10.1145/3155902.3155907,
	abstract = {This paper is concerned with automatic georeferencing of river networks from raster images such as aerial photos or maps. Determining reasonable assignments between a given network of rivers derived from a textual description and an image is subject to high combinatorial complexity and uncertainty. We investigate the application of spatial reasoning in automatic georeferencing.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Wolter, Diedrich and Blank, Daniel and Henrich, Andreas},
	booktitle = {Proceedings of the 11th Workshop on Geographic Information Retrieval},
	doi = {10.1145/3155902.3155907},
	isbn = {9781450353380},
	keywords = {spatial reasoning, georeferencing river networks, GIS},
	location = {Heidelberg, Germany},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {GIR'17},
	title = {Georeferencing River Networks Using Spatial Reasoning},
	url = {https://doi.org/10.1145/3155902.3155907},
	year = {2017},
}

@inproceedings{10.1145/3155902.3155908,
	abstract = {Historical itineraries, often accessible as lists or tables describing places visited in sequence, are abundant resources and also important objects of study for humanities scholars. This article advances a novel method for automatically geocoding tabular itineraries, combining approximate string matching with a cost optimization algorithm based on dynamic programming. Experiments with a dataset of historical itineraries, with ground-truth geocoding annotations provided by domain experts and leveraging also the GeoNames gazetteer, attest to the effectiveness of the proposed method. The obtained results show that while approximate string matching can already achieve very low median errors, with many toponyms matching exactly against GeoNames entries, the combination with cost optimization can significantly improve results in terms of the average distance towards the correct disambiguations.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Santos, Rui and Murrieta-Flores, Patricia and Martins, Bruno},
	booktitle = {Proceedings of the 11th Workshop on Geographic Information Retrieval},
	doi = {10.1145/3155902.3155908},
	isbn = {9781450353380},
	keywords = {toponym matching, geographic information retrieval, dynamic programming, digital humanities, automated geocoding},
	location = {Heidelberg, Germany},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {GIR'17},
	title = {An Automated Approach for Geocoding Tabular Itineraries},
	url = {https://doi.org/10.1145/3155902.3155908},
	year = {2017},
}

@inproceedings{10.1145/3155902.3155913,
	abstract = {We investigate gazetteer matching between natural features in Switzerland. We produce a gold standard dataset for a subset of features from 8 natural feature types in GeoNames aligned with their corresponding match(es) in SwissNames3D, an authoritative gazetteer. Based on this dataset, we comment on feature type alignments between the two resources and on type-specific differences to take into consideration for the matching task. We present preliminary results of rule-based matching and plans for future work.},
	address = {New York, NY, USA},
	articleno = {11},
	author = {Acheson, Elise and Villette, Julia and Volpi, Michele and Purves, Ross S.},
	booktitle = {Proceedings of the 11th Workshop on Geographic Information Retrieval},
	doi = {10.1145/3155902.3155913},
	isbn = {9781450353380},
	keywords = {matching, gazetteers, feature types, entity resolution},
	location = {Heidelberg, Germany},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {GIR'17},
	title = {Gazetteer matching for natural features in Switzerland},
	url = {https://doi.org/10.1145/3155902.3155913},
	year = {2017},
}

@article{10.1145/3156667,
	abstract = {Which venue is a tweet posted from? We call this a fine-grained geolocation problem. Given an observed tweet, the task is to infer its discrete posting venue, e.g., a specific restaurant. This recovers the venue context and differs from prior work, which geolocats tweets to location coordinates or cities/neighborhoods.First, we conduct empirical analysis to uncover venue and user characteristics for improving geolocation. For venues, we observe spatial homophily, in which venues near each other have more similar tweet content (i.e., text representations) compared to venues further apart. For users, we observe that they are spatially focused and more likely to visit venues near their previous visits. We also find that a substantial proportion of users post one or more geocoded tweet(s), thus providing their location history data. We then propose geolocation models that exploit spatial homophily and spatial focus characteristics plus posting time information. Our models rank candidate venues of test tweets such that the actual posting venue is ranked high. To better tune model parameters, we introduce a learning-to-rank framework. Our best model significantly outperforms state-of-the-art baselines. Furthermore, we show that tweets without any location-indicative words can be geolocated meaningfully as well.},
	address = {New York, NY, USA},
	articleno = {26},
	author = {Chong, Wen-Haw and Lim, Ee-Peng},
	doi = {10.1145/3156667},
	issn = {1046-8188},
	issue_date = {July 2018},
	journal = {ACM Trans. Inf. Syst.},
	keywords = {spatial homophily, spatial focus, learning to rank, Tweet geolocation},
	month = {feb},
	number = {3},
	numpages = {34},
	publisher = {Association for Computing Machinery},
	title = {Exploiting User and Venue Characteristics for Fine-Grained Tweet Geolocation},
	url = {https://doi.org/10.1145/3156667},
	volume = {36},
	year = {2018},
}

@inproceedings{10.1145/3158233.3159319,
	abstract = {The general image processing technique only processes the image itself, but in the field of geo-mapping reconnaissance, geographic data also need to be dealt with. For example, scouts often need to know the location of a target. In this paper, an image and geographic data integration approach is proposed. This method takes the geographic data as multi-channel floating-point matrixes participating in the image processing algorithm to handle. To verify the effectiveness of the proposed algorithm, a geographic data verification method for the image and geographic data integration is also introduced. The experimental results show that the proposed method can effectively process the image with geographic data and show the application for image stitching.},
	address = {New York, NY, USA},
	author = {Liang, Zhongyan and Guo, Qiaojin and Xu, Jian and Hu, Jie},
	booktitle = {Proceedings of the 2017 2nd International Conference on Communication and Information Systems},
	doi = {10.1145/3158233.3159319},
	isbn = {9781450353489},
	keywords = {UAV, Image Stitching, Image Mosaicking, Geographic data},
	location = {Wuhan, China},
	numpages = {5},
	pages = {268–272},
	publisher = {Association for Computing Machinery},
	series = {ICCIS 2017},
	title = {A UAV Image and Geographic Data Integration Processing Method and Its Applications},
	url = {https://doi.org/10.1145/3158233.3159319},
	year = {2017},
}

@inproceedings{10.1145/3159450.3162374,
	address = {New York, NY, USA},
	author = {Erkan, Ali and Barr, John},
	booktitle = {Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
	doi = {10.1145/3159450.3162374},
	isbn = {9781450351034},
	keywords = {underrepresented groups, spatial data processing, interdisciplinarity, geographic information systems, GIS},
	location = {Baltimore, Maryland, USA},
	numpages = {1},
	pages = {1053},
	publisher = {Association for Computing Machinery},
	series = {SIGCSE '18},
	title = {Geographic Information Systems (GIS): Opportunities of Spatial Data Processing for Computer Science Education (Abstract Only)},
	url = {https://doi.org/10.1145/3159450.3162374},
	year = {2018},
}

@article{10.1145/3161171,
	abstract = {With the rapid growth in smartphone usage, it has been more and more important to understand the patterns of mobile data consumption by users. In this paper, we present an empirical study of the correlation between user mobility and app usage patterns. In particular, we focus on users' moving speed as the key mobility metric, and try to answer the following question: are there any notable relations between moving speed and the app usage patterns? Our study is based on a real-world, large-scale dataset of 2G phone network data request records. A critical challenge was that the raw data records are rather coarse-grained. More specifically, unlike GPS traces, the exact locations of users were not readily available. We inferred users' approximate locations according to their interactions with nearby cell towers, whose locations were known. We proposed a novel method to filter out noises and perform reliable speed estimation. We verify our methodology with out of sample data and show its improvement in speed estimation accuracy. We then examined several aspects of mobile data usage patterns, including the data volume, the access frequency, and the app categories, to reveal the correlation between these patterns and users' moving speed. Experimental results based on our large-scale real-world datasets revealed that users under different mobility categories not only have different smartphone usage motivations but also have different ways of using their smartphones.},
	address = {New York, NY, USA},
	articleno = {153},
	author = {Lu, Zheng and Feng, Yunhe and Zhou, Wenjun and Li, Xiaolin and Cao, Qing},
	doi = {10.1145/3161171},
	issue_date = {December 2017},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	keywords = {trajectory inference, smartphone app usage pattern, cellular-data, User mobility},
	month = {jan},
	number = {4},
	numpages = {21},
	publisher = {Association for Computing Machinery},
	title = {Inferring Correlation between User Mobility and App Usage in Massive Coarse-grained Data Traces},
	url = {https://doi.org/10.1145/3161171},
	volume = {1},
	year = {2018},
}

@article{10.1145/3162076,
	abstract = {In 2015, the top 10 largest amusement park corporations saw a combined annual attendance of over 400 million visitors. Daily average attendance in some of the most popular theme parks in the world can average 44,000 visitors per day. These visitors ride attractions, shop for souvenirs, and dine at local establishments; however, a critical component of their visit is the overall park experience. This experience depends on the wait time for rides, the crowd flow in the park, and various other factors linked to the crowd dynamics and human behavior. As such, better insight into visitor behavior can help theme parks devise competitive strategies for improved customer experience. Research into the use of attractions, facilities, and exhibits can be studied, and as behavior profiles emerge, park operators can also identify anomalous behaviors of visitors which can improve safety and operations. In this article, we present a visual analytics framework for analyzing crowd dynamics in theme parks. Our proposed framework is designed to support behavioral analysis by summarizing patterns and detecting anomalies. We provide methodologies to link visitor movement data, communication data, and park infrastructure data. This combination of data sources enables a semantic analysis of who, what, when, and where, enabling analysts to explore visitor-visitor interactions and visitor-infrastructure interactions. Analysts can identify behaviors at the macro level through semantic trajectory clustering views for group behavior dynamics, as well as at the micro level using trajectory traces and a novel visitor network analysis view. We demonstrate the efficacy of our framework through two case studies of simulated theme park visitors.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Steptoe, Michael and Kr\"{u}ger, Robert and Garcia, Rolando and Liang, Xing and Maciejewski, Ross},
	doi = {10.1145/3162076},
	issn = {2160-6455},
	issue_date = {March 2018},
	journal = {ACM Trans. Interact. Intell. Syst.},
	keywords = {trajectory analysis, semantic trajectories, behavior, Visual analytics},
	month = {feb},
	number = {1},
	numpages = {27},
	publisher = {Association for Computing Machinery},
	title = {A Visual Analytics Framework for Exploring Theme Park Dynamics},
	url = {https://doi.org/10.1145/3162076},
	volume = {8},
	year = {2018},
}

@inproceedings{10.1145/3167020.3167026,
	abstract = {Maintaining infrastructures (e.g., roadway) is a critical issue for local governments. Data from physical devices and reports from citizens through social networks are helpful to observe conditions of infrastructures. This paper proposes a framework called SOLA for integrating and analysing data from multiple sources including streaming data and static data for roadway management. The framework integrates data from multiple sources in the way of stream OLAP architecture, and analyses the integrated data in terms of OLAP analysis. This paper applies the framework to support roadway managements of local governments, and develops the application called SOLAR. SOLAR aims at providing historical views of roadway patrols as well as roadway statuses for assisting in determining roadway patrolling schedules. The real-world use case on a city exhibits the applicability of SOLAR with positive feedbacks from city officers. SOLA is a promising framework for big data analysis and smart city applications, as the number, amount, and speed of generating data increase in the era of big data and smart city.},
	address = {New York, NY, USA},
	author = {Komamizu, Takahiro and Amagasa, Toshiyuki and Shaikh, Salman Ahmed and Shiokawa, Hiroaki and Kitagawa, Hiroyuki},
	booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
	doi = {10.1145/3167020.3167026},
	isbn = {9781450348959},
	keywords = {Stream OLAP, Spatio-temporal OLAP, Roadway management, Real-time OLAP, Data integration},
	location = {Bangkok, Thailand},
	numpages = {7},
	pages = {35–41},
	publisher = {Association for Computing Machinery},
	series = {MEDES '17},
	title = {SOLA: Stream OLAP-based Analytical Framework for Roadway Maintenance},
	url = {https://doi.org/10.1145/3167020.3167026},
	year = {2017},
}

@inproceedings{10.1145/3167020.3167029,
	abstract = {This paper presents a method of searching for modified maps that are similar to a user's query. Although there are many different reasons for using maps, a similarity search method can support user decision-making. Modified maps are simplified maps that have been changed in some way. They are useful for reflecting a user's specific purpose; however, there is no existing method for searching for modified maps. In this paper, we propose a system that suggests a list of similar modified maps by analyzing the features of a modified map input by the user.},
	address = {New York, NY, USA},
	author = {Narikawa, Kentaro and Kitayama, Daisuke and Sumiya, Kazutoshi},
	booktitle = {Proceedings of the 9th International Conference on Management of Digital EcoSystems},
	doi = {10.1145/3167020.3167029},
	isbn = {9781450348959},
	keywords = {Search method, Multiple features, Modified maps, Geographical information},
	location = {Bangkok, Thailand},
	numpages = {8},
	pages = {57–64},
	publisher = {Association for Computing Machinery},
	series = {MEDES '17},
	title = {Design of Multiple Modified Features Based on a Map Analysis of Geographical Information},
	url = {https://doi.org/10.1145/3167020.3167029},
	year = {2017},
}

@inproceedings{10.1145/3167132.3167225,
	abstract = {Several methods for trajectory classification build models exploring trajectory global features, such as the average and the standard deviation of speed and acceleration, but for some applications these features may not be the best to determine the class. Other works explore local features, applying trajectory partition and discretization, that lose important movement information that could discriminate the class. In this work we propose a new method, called Movelets, to discover relevant subtrajectories without the need of a predefined criteria for either trajectory partition or discretization. We extend the concept of time series shapelets for trajectories, and to the best of our knowledge, this work is the first to use shapelets in the trajectory domain. We evaluated the proposed approach with several categories of datasets, including hurricanes, vehicles, animals, and transportation means, and show with extensive experiments that our method largely outperformed state of the art works, indicating that Movelets is very promising for trajectory classification.},
	address = {New York, NY, USA},
	author = {Ferrero, Carlos Andres and Alvares, Luis Otavio and Zalewski, Willian and Bogorny, Vania},
	booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3167132.3167225},
	isbn = {9781450351911},
	keywords = {trajectory shapelets, trajectory classification, spatio-temporal data analysis, relevant subtrajectories, movelets},
	location = {Pau, France},
	numpages = {8},
	pages = {849–856},
	publisher = {Association for Computing Machinery},
	series = {SAC '18},
	title = {MOVELETS: exploring relevant subtrajectories for robust trajectory classification},
	url = {https://doi.org/10.1145/3167132.3167225},
	year = {2018},
}

@inproceedings{10.1145/3167132.3167228,
	abstract = {The recent initiatives to digitize cultural heritage resources and publish them on the Web have renewed interest in historical maps for the diachronic analysis of territories in GIS applications. However, such analyses should not be done without a good understanding of the possibilities and limitations of geographical information provided by historical maps, i.e. their quality. One of the major concerns regarding historical maps quality is their positional planimetric accuracy which highly depends on survey techniques used at the time. As these techniques are not always thoroughly known and as ground truth is most of the time not sufficiently available, direct absolute evaluation approaches have been proposed to assess historical maps positional planimetric accuracy. In this article, we follow the intuition that the most widely adopted georeferencing-based approach for assessing the positional planimetric accuracy of historical maps can be adapted to provide an evaluation of the error caused by the survey process in cases like Paris atlases where the georeferencing transformation can be estimated with ground control points based on geodetic features and where the projection of the map can be approximated by a well known projected coordinate reference system. We apply this tuned approach on the Verniquet atlas and evaluate the validity of our hypothesis about projection approximation.},
	address = {New York, NY, USA},
	author = {Dum\'{e}nieu, Bertrand and Abadie, Nathalie and Perret, Julien},
	booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3167132.3167228},
	isbn = {9781450351911},
	keywords = {planimetric accuracy assessment, historical maps},
	location = {Pau, France},
	numpages = {8},
	pages = {876–883},
	publisher = {Association for Computing Machinery},
	series = {SAC '18},
	title = {Assessing the planimetric accuracy of Paris atlases from the late 18th and 19th centuries},
	url = {https://doi.org/10.1145/3167132.3167228},
	year = {2018},
}

@inproceedings{10.1145/3168390.3168421,
	abstract = {The Earth's land surface is always changing or deforming from time to time. This phenomenon occurs by natural, human factors or combination of both. Land deformation leads to destructive disaster and caused a great loss. A method for measuring and mapping land surface deformation, especially in urban area, is urgently needed. Earth's surface deformation mapping using radar imagery from Synthetic Aperture Radar (SAR) remote sensing satellites have been actively developed since its image acquisition capabilities that can be performed in almost any conditions. SAR sensor signals can pass through clouds to overcome the difficulties in analyzing cloud-covered areas, such as in tropical area. In this research, we proposed land surface deformation mapping using PS-InSAR method on ALOS/PALSAR imageries. We analyzed the deformation rate from 10 ALOS/PALSAR scenes dated from June 10, 2007 to December 13, 2008. Results of the analysis showed that during the period, land surface deformation rate in almost all urban areas in Bandung, West Java, Indonesia is increasing. The highest land subsidence rate occurred in Cimahi and Bojong districts and recorded at 13.5 cm per year, respectively. Land subsidence in urban areas Bandung is caused by excessive use of ground water, new settlements, and changed land cover, especially in the industrial and settlement area.},
	address = {New York, NY, USA},
	author = {Sudiana, Dodi and Antoni and Rokhmatuloh and Sumantyo, Josaphat Tetuko Sri},
	booktitle = {Proceedings of the 2017 International Conference on Computer Science and Artificial Intelligence},
	doi = {10.1145/3168390.3168421},
	isbn = {9781450353922},
	keywords = {Synthetic Aperture Radar, PS-InSAR, Land deformation, ALOS/PALSAR},
	location = {Jakarta, Indonesia},
	numpages = {6},
	pages = {84–89},
	publisher = {Association for Computing Machinery},
	series = {CSAI '17},
	title = {Land Surface Deformation Mapping Method using PS-InSAR on ALOS/PALSAR Data in Bandung Region},
	url = {https://doi.org/10.1145/3168390.3168421},
	year = {2017},
}

@inproceedings{10.1145/3170521.3170536,
	abstract = {This paper proposes Zero-Mean and Unity-Mean (ZU-Mean) features based device localization methods for internet of things (IoT). These features do not depend on the hardwares and/or specifications of the devices being used. Moreover, the zero-mean and unity-mean features mitigate the additive and multiplicative noise, respectively. Extensive real experiments are conducted in two different sites (residential and mall areas) using WiFi received signal strength (RSS) for five weeks. The performance of the proposed methods is better than the absolute RSS based method. We also highlight that the absolute RSS feature cannot be used in calibration-free method and hence, it is not suitable for diverse devices in IoT networks. Additionally, the proposed low-cost method is computationally efficient as compared to the existing methods in the literature.},
	address = {New York, NY, USA},
	articleno = {15},
	author = {Kumar, Sudhir and Das, Sajal K},
	booktitle = {Proceedings of the Workshop Program of the 19th International Conference on Distributed Computing and Networking},
	doi = {10.1145/3170521.3170536},
	isbn = {9781450363976},
	keywords = {localization, internet of things, fingerprinting},
	location = {Varanasi, India},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {Workshops ICDCN '18},
	title = {ZU-mean: fingerprinting based device localization methods for IoT in the presence of additive and multiplicative noise},
	url = {https://doi.org/10.1145/3170521.3170536},
	year = {2018},
}

@inproceedings{10.1145/3175587.3175599,
	abstract = {Monitoring of vast water bodies, including internal and external waters, is an important issue which is commonly performed by remote sensing as the most economical technology. In this field, the concentration of chlorophyll-a, as a critical water quality index, has attracted most research attentions. In this paper, wavelet neural network are proposed for the estimation of chlorophyll-a concentration in Caspian Sea from multi-date MODIS product MYDOCGA.These networks are evaluated from both aspects of estimation accuracy as well as response stability and are also compared to the classical perceptron neural networks (PNN). In addition, different features are examined as the network input parameters including all the 9 MODIS product MYDOCGA bands, different subsets of these bands and also PCA(Principal Component Analysis) bands in different number. The results, which are obtained and validated based to 55 filed observed samples, proves the effectiveness of WNN (Wavelet Neural Network) in comparison to classical neural networks. The best RMSE=0.07 of these networks reveals that remote sensing can accurately replace field observations to produce thematic maps of water quality parameters provided that appropriate processing techniques are applied.},
	address = {New York, NY, USA},
	author = {Haghparast, Melika and Mokhtarzade, Mehdi and Gholamalifard, Mehdi},
	booktitle = {Proceedings of the 4th International Conference on Bioinformatics Research and Applications},
	doi = {10.1145/3175587.3175599},
	isbn = {9781450353823},
	keywords = {wavelet neural network, water quality parameters, chlorophyll_a concentration, Remote sensing},
	location = {Barcelona, Spain},
	numpages = {6},
	pages = {46–51},
	publisher = {Association for Computing Machinery},
	series = {ICBRA '17},
	title = {A Wavelet-Neural Network for the Estimation of Chlorophyll-a Concentration in Caspian Sea},
	url = {https://doi.org/10.1145/3175587.3175599},
	year = {2017},
}

@inproceedings{10.1145/3178876.3186027,
	abstract = {Toponym Resolution, the task of assigning a location mention in a document to a geographic referent (i.e., latitude/longitude), plays a pivotal role in analyzing location-aware content. However, the ambiguities of natural language and a huge number of possible interpretations for toponyms constitute insurmountable hurdles for this task. In this paper, we study the problem of toponym resolution with no additional information other than a gazetteer and no training data. We demonstrate that a dearth of large enough annotated data makes supervised methods less capable of generalizing. Our proposed method estimates the geographic scope of documents and leverages the connections between nearby place names as evidence to resolve toponyms. We explore the interactions between multiple interpretations of mentions and the relationships between different toponyms in a document to build a model that finds the most coherent resolution. Our model is evaluated on three news corpora, two from the literature and one collected and annotated by us; then, we compare our methods to the state-of-the-art unsupervised and supervised techniques. We also examine three commercial products including Reuters OpenCalais, Yahoo! YQL Placemaker, and Google Cloud Natural Language API. The evaluation shows that our method outperforms the unsupervised technique as well as Reuters OpenCalais and Google Cloud Natural Language API on all three corpora; also, our method shows a performance close to that of the state-of-the art supervised method and outperforms it when the test data has 40\% or more toponyms that are not seen in the training data.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Kamalloo, Ehsan and Rafiei, Davood},
	booktitle = {Proceedings of the 2018 World Wide Web Conference},
	doi = {10.1145/3178876.3186027},
	isbn = {9781450356398},
	keywords = {unsupervised disambiguation, toponym resolution, spatial hierarchies, geolocation extraction, context-bound hypotheses},
	location = {Lyon, France},
	numpages = {10},
	pages = {1287–1296},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '18},
	title = {A Coherent Unsupervised Model for Toponym Resolution},
	url = {https://doi.org/10.1145/3178876.3186027},
	year = {2018},
}

@inproceedings{10.1145/3178876.3186078,
	abstract = {Many applications that use geographical databases (a.k.a. gazetteers) rely on the accuracy of the information in the database. However, poor data quality is an issue when data is integrated from multiple sources with different quality constraints and sometimes with little information about the sources. One major consequence of this is that the geographical scope of a location and/or its position may not be known or may not be accurate. In this paper, we study the problem of detecting the scope of locations in a geographical database and its applications in identifying inconsistencies and improving the quality of a gazetteer. We develop novel strategies, including probabilistic and geometric approaches, to accurately derive the geographical scope of places based on the spatial hierarchy of a gazetteer as well as other public information (such as area) that may be available. We show how the boundary information derived here can be useful in identifying inconsistencies, enhancing the location hierarchy and improving the applications that rely on gazetteers. Our experimental evaluation on two public-domain gazetteers reveals that the proposed approaches significantly outperform, in terms of the accuracy of the geographical bounding boxes, a baseline that is based on the parent-child relationship of a gazetteer. Among applications, we show that the boundary information derived here can move more than 20\% of locations in a public gazetteer to better positions in the hierarchy and that the accuracy of those moves is over 90\%.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Singh, Sanket Kumar and Rafiei, Davood},
	booktitle = {Proceedings of the 2018 World Wide Web Conference},
	doi = {10.1145/3178876.3186078},
	isbn = {9781450356398},
	keywords = {geotagging, geographical scoping, gazetteer improvement},
	location = {Lyon, France},
	numpages = {10},
	pages = {1663–1672},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '18},
	title = {Strategies for Geographical Scoping and Improving a Gazetteer},
	url = {https://doi.org/10.1145/3178876.3186078},
	year = {2018},
}

@inproceedings{10.1145/3183713.3183738,
	abstract = {With the proliferation of mobile devices, large collections of geospatial data are becoming available, such as geo-tagged photos. Map rendering systems play an important role in presenting such large geospatial datasets to end users. We propose that such systems should support the following desirable features: representativeness, visibility constraint, zooming consistency, and panning consistency. The first two constraints are fundamental challenges to a map exploration system, which aims to efficiently select a small set of representative objects from the current region of user's interest, and any two selected objects should not be too close to each other for users to distinguish in the limited space of a screen. We formalize it as the Spatial Object Selection (SOS) problem, prove that it is an NP-hard problem, and develop a novel approximation algorithm with performance guarantees. \% To further support interactive exploration of geospatial data on maps, we propose the Interactive SOS (ISOS) problem, in which we enrich the SOS problem with the zooming consistency and panning consistency constraints. The objective of ISOS is to provide seamless experience for end-users to interactively explore the data by navigating the map. We extend our algorithm for the SOS problem to solve the ISOS problem, and propose a new strategy based on pre-fetching to significantly enhance the efficiency. Finally we have conducted extensive experiments to show the efficiency and scalability of our approach.},
	address = {New York, NY, USA},
	author = {Guo, Tao and Feng, Kaiyu and Cong, Gao and Bao, Zhifeng},
	booktitle = {Proceedings of the 2018 International Conference on Management of Data},
	doi = {10.1145/3183713.3183738},
	isbn = {9781450347037},
	keywords = {sampling, map exploration, geospatial data visualization},
	location = {Houston, TX, USA},
	numpages = {16},
	pages = {567–582},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '18},
	title = {Efficient Selection of Geospatial Data on Maps for Interactive and Visualized Exploration},
	url = {https://doi.org/10.1145/3183713.3183738},
	year = {2018},
}

@inproceedings{10.1145/3184558.3186962,
	abstract = {In this paper, we use semantic technologies for enriching trajectory data in the automotive industry for offline analysis. We proposed to re-use a combination of existing ontologies and we designed a Vehicle Signal Specification ontology to provide an environment in which we developed an application that analyzes the variations of signal values and enables to infer the "driving smoothness'' that we represent as additional annotations of semantic trajectories.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Klotz, Benjamin and Troncy, Rapha\"{e}l and Wilms, Daniel and Bonnet, Christian},
	booktitle = {Companion Proceedings of the The Web Conference 2018},
	doi = {10.1145/3184558.3186962},
	isbn = {9781450356404},
	keywords = {vss, ssn, sosa, semantic trajectories, ontology, car signal},
	location = {Lyon, France},
	numpages = {4},
	pages = {135–138},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '18},
	title = {Generating Semantic Trajectories Using a Car Signal Ontology},
	url = {https://doi.org/10.1145/3184558.3186962},
	year = {2018},
}

@inproceedings{10.1145/3184558.3191623,
	abstract = {We present SAVITR, a system that leverages the information posted on the Twitter microblogging site to monitor and analyse emergency situations. Given that only a very small percentage of microblogs are geo-tagged, it is essential for such a system to extract locations from the text of the microblogs. We employ natural language processing techniques to infer the locations mentioned in the microblog text, in an unsupervised fashion and display it on a map-based interface. The system is designed for efficient performance, achieving an F-score of 0.81, and is approximately two orders of magnitude faster than other available tools for location extraction.},
	address = {Republic and Canton of Geneva, CHE},
	author = {Dutt, Ritam and Hiware, Kaustubh and Ghosh, Avijit and Bhaskaran, Rameshwar},
	booktitle = {Companion Proceedings of the The Web Conference 2018},
	doi = {10.1145/3184558.3191623},
	isbn = {9781450356404},
	keywords = {microblogs, location extraction, geonames, emergencies},
	location = {Lyon, France},
	numpages = {7},
	pages = {1643–1649},
	publisher = {International World Wide Web Conferences Steering Committee},
	series = {WWW '18},
	title = {SAVITR: A System for Real-time Location Extraction from Microblogs during Emergencies},
	url = {https://doi.org/10.1145/3184558.3191623},
	year = {2018},
}

@article{10.1145/3190345,
	abstract = {Data analytics has an ever increasing impact on tackling various societal challenges. In this article, we investigate how data from several heterogeneous online sources can be used to discover insights and make predictions about the spatial distribution of crime in large urban environments. A series of important research questions is addressed, following a purely data-driven approach and methodology. First, we examine how useful different types of data are for the task of crime levels prediction, focusing especially on how prediction accuracy can be improved by combining data from multiple information sources. To that end, we not only investigate prediction accuracy across all individual areas studied, but also examine how these predictions affect the accuracy of identified crime hotspots. Then, we look into individual features, aiming to identify and quantify the most important factors. Finally, we drill down to different crime types, elaborating on how the prediction accuracy and the importance of individual features vary across them. Our analysis involves six different datasets, from which more than 3,000 features are extracted, filtered, and used to learn models for predicting crime rates across 14 different crime categories. Our results indicate that combining data from multiple information sources can significantly improve prediction accuracy. They also highlight which features affect prediction accuracy the most, as well as for which particular crime categories the predictions are more accurate.},
	address = {New York, NY, USA},
	articleno = {12},
	author = {Belesiotis, Alexandros and Papadakis, George and Skoutas, Dimitrios},
	doi = {10.1145/3190345},
	issn = {2374-0353},
	issue_date = {December 2017},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {data analytics, crime prediction, Spatial crime distribution},
	month = {apr},
	number = {4},
	numpages = {31},
	publisher = {Association for Computing Machinery},
	title = {Analyzing and Predicting Spatial Crime Distribution Using Crowdsourced and Open Data},
	url = {https://doi.org/10.1145/3190345},
	volume = {3},
	year = {2018},
}

@article{10.1145/3191746,
	abstract = {Rapidly developing location acquisition technologies have provided us with big GPS trajectory data, which offers a new means of understanding people's daily behaviors as well as urban dynamics. With such data, predicting human mobility at the city level will be of great significance for transportation scheduling, urban regulation, and emergency management. In particular, most urban human behaviors are related to a small number of important regions, referred to as Regions-of-Interest (ROIs). Therefore, in this study, a deep ROI-based modeling approach is proposed for effectively predicting urban human mobility. Urban ROIs are first discovered from historical trajectory data, and urban human mobility is designated using two types of ROI labels (ISROI and WHICHROI). Then, urban mobility prediction is modeled as a sequence classification problem for each type of label. Finally, a deep-learning architecture built with recurrent neural networks is designed as an effective sequence classifier. Experimental results demonstrate that the superior performance of our proposed approach to the baseline models and several real-world practices show the applicability of our approach to real-world urban computing problems.},
	address = {New York, NY, USA},
	articleno = {14},
	author = {Jiang, Renhe and Song, Xuan and Fan, Zipei and Xia, Tianqi and Chen, Quanjun and Chen, Qi and Shibasaki, Ryosuke},
	doi = {10.1145/3191746},
	issue_date = {March 2018},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	keywords = {urban computing, human mobility, deep learning, big data},
	month = {mar},
	number = {1},
	numpages = {29},
	publisher = {Association for Computing Machinery},
	title = {Deep ROI-Based Modeling for Urban Human Mobility Prediction},
	url = {https://doi.org/10.1145/3191746},
	volume = {2},
	year = {2018},
}

@article{10.1145/3191786,
	abstract = {Urban anomalies, such as abnormal movements of crowds and accidents, may result in loss of life or property if not handled properly. It would be of great value for governments if anomalies can be automatically alerted in their early stage. However, detecting anomalies in urban area has two main challenges. First, the criteria to determine an anomaly on different occasions (e.g. rainy days vs. sunny days, or holidays vs. workdays) and in different places (e.g. tourist attractions vs. office areas) are distinctly different, as these occasions and places have their own definitions on normal patterns. Second, urban anomalies often exhibit complex forms (e.g. road closure may cause decrease in taxi flow and increase in bike flow). We need an algorithm that not only models the anomaly degree of individual data source but also the combination of changes in multiple data sources. In this paper, we propose a two-step method to tackle those challenges. In the first step, we use a similarity-based algorithm to estimate an anomaly score for each individual data source in each region and time slot based on the values of historically similar regions. Those scores are fed into the second step, where we propose an algorithm based on one-class Support Vector Machine to capture rare patterns occurred in multiple data sources, nearby regions or time slots, and give a final, integrated anomaly score for each region. Evaluations based on both synthetic and real world datasets show the advantages of our method beyond baseline techniques such as distance-based, probability-based methods.},
	address = {New York, NY, USA},
	articleno = {54},
	author = {Zhang, Huichu and Zheng, Yu and Yu, Yong},
	doi = {10.1145/3191786},
	issue_date = {March 2018},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	keywords = {urban computing, spatio-temporal data mining, multiple data sources, anomaly detection},
	month = {mar},
	number = {1},
	numpages = {18},
	publisher = {Association for Computing Machinery},
	title = {Detecting Urban Anomalies Using Multiple Spatio-Temporal Data Sources},
	url = {https://doi.org/10.1145/3191786},
	volume = {2},
	year = {2018},
}

@article{10.1145/3193835,
	abstract = {In the past decade, positioning system-enabled devices such as smartphones have become most prevalent. This functionality brings the increasing popularity of location-based services in business as well as daily applications such as navigation, targeted advertising, and location-based social networking. Continuous spatial queries serve as a building block for location-based services. As an example, an Uber driver may want to be kept aware of the nearest customers or service stations. Continuous spatial queries require updates to the query result as the query or data objects are moving. This poses challenges to the query efficiency, which is crucial to the user experience of a service. A large number of approaches address this efficiency issue using the concept of safe region. A safe region is a region within which arbitrary movement of an object leaves the query result unchanged. Such a region helps reduce the frequency of query result update and hence improves query efficiency. As a result, safe region-based approaches have been popular for processing various types of continuous spatial queries. Safe regions have interesting theoretical properties and are worth in-depth analysis. We provide a comparative study of safe region-based approaches. We describe how safe regions are computed for different types of continuous spatial queries, showing how they improve query efficiency. We compare the different safe region-based approaches and discuss possible further improvements.},
	address = {New York, NY, USA},
	articleno = {64},
	author = {Qi, Jianzhong and Zhang, Rui and Jensen, Christian S. and Ramamohanarao, Kotagiri and HE, Jiayuan},
	doi = {10.1145/3193835},
	issn = {0360-0300},
	issue_date = {May 2019},
	journal = {ACM Comput. Surv.},
	keywords = {moving query, moving object, continuous range query, Continuous kNN query},
	month = {may},
	number = {3},
	numpages = {39},
	publisher = {Association for Computing Machinery},
	title = {Continuous Spatial Query Processing: A Survey of Safe Region Based Techniques},
	url = {https://doi.org/10.1145/3193835},
	volume = {51},
	year = {2018},
}

@inproceedings{10.1145/3194658.3194665,
	abstract = {Understanding how ambulance incidents are spatially distributed can shed light to the epidemiological dynamics of geographic areas and inform healthcare policy design. Here we analyze a longitudinal dataset of more than four million ambulance calls across a region of twelve million residents in the North West of England. With the aim to explain geographic variations in ambulance call frequencies, we employ a wide range of data layers including open government datasets describing population demographics and socio-economic characteristics, as well as geographic activity in online services such as Foursquare. Working at a fine level of spatial granularity we demonstrate that daytime population levels and the deprivation status of an area are the most important variables when it comes to predicting the volume of ambulance calls at an area. Foursquare check-ins on the other hand complement these government sourced indicators, offering a novel view to population nightlife and commercial activity locally. We demonstrate how check-in activity can provide an edge when predicting certain types of emergency incidents in a multi-variate regression model.},
	address = {New York, NY, USA},
	author = {Noulas, Anastasios and Moffatt, Colin and Hristova, Desislava and Gon\c{c}alves, Bruno},
	booktitle = {Proceedings of the 2018 International Conference on Digital Health},
	doi = {10.1145/3194658.3194665},
	isbn = {9781450364935},
	keywords = {human mobility, health geography, digital epidemiology},
	location = {Lyon, France},
	numpages = {10},
	pages = {100–109},
	publisher = {Association for Computing Machinery},
	series = {DH '18},
	title = {Foursquare to the Rescue: Predicting Ambulance Calls Across Geographies},
	url = {https://doi.org/10.1145/3194658.3194665},
	year = {2018},
}

@inproceedings{10.1145/3194658.3194683,
	abstract = {The devastating consequences of neonates infected with the Zika virus makes it necessary to fight and stop the spread of this virus and its vectors (Aedes mosquitoes). An essential part of the fight against mosquitoes is the use of mobile technology to support routine surveillance and risk assessment by community health workers (health agents). In addition, to improve early warning systems, the public health authorities need to forecast more accurately where an outbreak of the virus and its vector is likely to occur. The ZIKΛ system aims to develop a novel comprehensive framework that combines e-learning to empower health agents, community-based participatory surveillance, and forecasting of occurrences and distribution of the Zika virus and its vectors in real time. This system is currently being implemented in Brazil, in the cities of Campina Grande, Recife, Jaboat\~{a}o dos Guararapes, and Olinda, the State of Pernambuco and Paraiba with the highest prevalence of the Zika virus disease. In this paper, we present the ZIKA system which helps health agents to learn new techniques and good practices to improve the surveillance of the virus and offer a real time distribution forecast of the virus and the vector. The forecast model is recalibrated in real time with information coming from health agents, governmental institutions, and weather stations to predict the areas with higher risk of a Zika virus outbreak in an interactive map. This mapping and alert system will help governmental institutions to make fast decisions and use their resources more efficiently to stop the spread of the Zika virus. The ZIKA app was developed and built in Ionic which allows for easy cross-platform rendering for both iOS and Android. The system presented in the current paper is one of the first systems combining public health surveillance, citizen-driven participatory reporting and weather data-based prediction. The implementation of the ZIKA system will reduce the devastating consequences of Zika virus in neonates and improve the life quality of vulnerable people in Brazil.},
	address = {New York, NY, USA},
	author = {Beltr\'{a}n, Juan D. and Boscor, Andrei and dos Santos, Wellington P. and Massoni, Tiago and Kostkova, Patty},
	booktitle = {Proceedings of the 2018 International Conference on Digital Health},
	doi = {10.1145/3194658.3194683},
	isbn = {9781450364935},
	keywords = {zika virus, surveillance, forecasting, e-learning, big data},
	location = {Lyon, France},
	numpages = {5},
	pages = {90–94},
	publisher = {Association for Computing Machinery},
	series = {DH '18},
	title = {ZIKA: A New System to Empower Health Workers and Local Communities to Improve Surveillance Protocols by E-learning and to Forecast Zika Virus in Real Time in Brazil},
	url = {https://doi.org/10.1145/3194658.3194683},
	year = {2018},
}

@inproceedings{10.1145/3197026.3197035,
	abstract = {Street names are not only used across the world as part of addresses, but also reveal a lot about a country's identity. Thus, they are subject to analysis in the fields of geography and social science. There, typically, a manual analysis limited to a small region is performed, e.g., focusing on the renaming of streets in a city after a political change in a country. Surprisingly, there have been hardly any automatic, large-scale studies of street names so far, although this might lead to interesting insights regarding the distribution of particular street name phenomena. In this paper, we present an automated, world-wide analysis of street names with date references. Such temporal streets are frequently used to commemorate important events and thus particularly interesting to study. After applying a multilingual temporal tagger to discover such street names, we analyze their temporal and geographic distributions on different levels of granularity. Furthermore, we present an approach to automatically harvest potential explanations why streets in specific regions refer to particular dates. Despite the challenges of the tasks, our evaluation demonstrates the feasibility of the street extraction and the explanation harvesting.},
	address = {New York, NY, USA},
	author = {Str\"{o}tgen, Jannik and Andrade, Rosita and Gupta, Dhruv},
	booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
	doi = {10.1145/3197026.3197035},
	isbn = {9781450351782},
	keywords = {temporal tagging, street name analysis, explanation harvesting, computational history, collective memory},
	location = {Fort Worth, Texas, USA},
	numpages = {10},
	pages = {79–88},
	publisher = {Association for Computing Machinery},
	series = {JCDL '18},
	title = {Putting Dates on the Map: Harvesting and Analyzing Street Names with Date Mentions and their Explanations},
	url = {https://doi.org/10.1145/3197026.3197035},
	year = {2018},
}

@inproceedings{10.1145/3197026.3203899,
	abstract = {The identification and extraction of the events that news articles report on is a commonly performed task in the analysis workflow of various projects that analyze news articles. However, due to the lack of universally usable and publicly available methods for news articles, many researchers must redundantly implement methods for event extraction to be used within their projects. Answers to the journalistic five W and one H questions (5W1H) describe the main event of a news story, i.e., who did what, when, where, why, and how. We propose Giveme5W1H, an open-source system that uses syntactic and domain-specific rules to extract phrases answering the 5W1H. In our evaluation, we find that the extraction precision of 5W1H phrases is p=0.64, and p=0.79 for the first four W questions, which discretely describe an event.},
	address = {New York, NY, USA},
	author = {Hamborg, Felix and Breitinger, Corinna and Schubotz, Moritz and Lachnit, Soeren and Gipp, Bela},
	booktitle = {Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries},
	doi = {10.1145/3197026.3203899},
	isbn = {9781450351782},
	keywords = {reporter's questions, news event detection, journalist's questions, 5w1h question answering, 5w1h extraction, 5w qa},
	location = {Fort Worth, Texas, USA},
	numpages = {2},
	pages = {339–340},
	publisher = {Association for Computing Machinery},
	series = {JCDL '18},
	title = {Extraction of Main Event Descriptors from News Articles by Answering the Journalistic Five W and One H Questions},
	url = {https://doi.org/10.1145/3197026.3203899},
	year = {2018},
}

@inproceedings{10.1145/3197091.3205835,
	abstract = {In this paper, we propose the use of simple KML files to increase student engagement when learning file I/O. We also report on our trial study where we had one group of students learn I/O functions using conventional exercises while we had another working with KML-based files that could trivially be viewed in Google Earth. Our assessment of student engagement indicated that students in the experiment group registered significantly higher in the usefulness and interest dimensions.},
	address = {New York, NY, USA},
	author = {Vidal, Elizabeth and Erkan, Ali},
	booktitle = {Proceedings of the 23rd Annual ACM Conference on Innovation and Technology in Computer Science Education},
	doi = {10.1145/3197091.3205835},
	isbn = {9781450357074},
	keywords = {KML, File I/O, CS2},
	location = {Larnaca, Cyprus},
	numpages = {2},
	pages = {369–370},
	publisher = {Association for Computing Machinery},
	series = {ITiCSE 2018},
	title = {A novel introduction to file I/O using KML and Google Earth},
	url = {https://doi.org/10.1145/3197091.3205835},
	year = {2018},
}

@inproceedings{10.1145/3200947.3201052,
	abstract = {The widespread use of Social Media creates great opportunities for businesses to take advantage of. By combining clever techniques, companies can develop powerful data analysis systems to understand their customers. This paper presents a real time sentiment analysis and location inference system, showcased via AirSent, an R-based application designed to assist airline carriers in measuring their passengers' satisfaction. AirSent can download, classify and locate tweets within seconds, presenting the results in interactive maps.},
	address = {New York, NY, USA},
	articleno = {21},
	author = {Michailidis, Dimitrios and Stylianou, Nikolaos and Vlahavas, Ioannis},
	booktitle = {Proceedings of the 10th Hellenic Conference on Artificial Intelligence},
	doi = {10.1145/3200947.3201052},
	isbn = {9781450364331},
	keywords = {Social Media Analytics, Sentiment Analysis, Real Time Classification, Location Inference},
	location = {Patras, Greece},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SETN '18},
	title = {Real Time Location Based Sentiment Analysis on Twitter: The AirSent System},
	url = {https://doi.org/10.1145/3200947.3201052},
	year = {2018},
}

@article{10.1145/3204455,
	abstract = {This article proposes a method to find intersections at which cars tend to deviate from the optimal route based on global positioning system (GPS) tracking data under the assumption that such deviations indicate that car navigation systems (CNSs) and road signage are not readily available. If the intended route is known, deviations can be enumerated by comparing the intended route with the vehicle’s actual route as observed by a GPS; however, the intended route is unknown and can differ from the route suggested by a CNS. To identify intersections with high deviation rates without knowing intended routes, we exhaustively sampled subsequences from each vehicular GPS track, and detected deviations from the optimal route for the subsequences. Although the detected deviations are not always caused by driver confusion, accumulating such erroneous detection results would yield a meaningful difference in the number of accumulated deviations at each intersection. We applied the proposed method to 3,843 GPS tracks collected from visitor drivers in the city of Kyoto. Thresholding the estimated deviation rate yielded 39 intersections from 14,543 candidates. The results show a certain level of correlation between obtained deviations and rerouting locations from actual CNS data. We also found several intersections where faulty route suggestions are provided by CNSs.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Fujino, Takumi and Hashimoto, Atsushi and Kasahara, Hidekazu and Mori, Mikihiko and Iiyama, Masaaki and Minoh, Michihiko},
	doi = {10.1145/3204455},
	issn = {2374-0353},
	issue_date = {March 2018},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {road design, motor trip, GPS track analysis},
	month = {jun},
	number = {1},
	numpages = {21},
	publisher = {Association for Computing Machinery},
	title = {Detecting Deviations from Intended Routes Using Vehicular GPS Tracks},
	url = {https://doi.org/10.1145/3204455},
	volume = {4},
	year = {2018},
}

@inproceedings{10.1145/3206505.3206516,
	abstract = {In Geographical Information search, map visualization can challenge the user because results can consist of a large set of heterogeneous items, increasing visual complexity. We propose a novel visualization model to address this issue. Our model represents results as markers, or as geometric objects, on 2D/3D layers, using stylized and highly colored shapes to enhance their visibility. Moreover, the model supports interactive information filtering in the map by enabling the user to focus on different data categories, using transparency sliders to tune the opacity, and thus the emphasis, of the corresponding data items. A test with users provided positive results concerning the efficacy of the model.},
	address = {New York, NY, USA},
	articleno = {38},
	author = {Ardissono, Liliana and Delsanto, Matteo and Lucenteforte, Maurizio and Mauro, Noemi and Savoca, Adriano and Scanu, Daniele},
	booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
	doi = {10.1145/3206505.3206516},
	isbn = {9781450356169},
	keywords = {visual information filtering, search results visualization, opacity tuning, 2D/3D geographical maps},
	location = {Castiglione della Pescaia, Grosseto, Italy},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {AVI '18},
	title = {Map-based visualization of 2D/3D spatial data via stylization and tuning of information emphasis},
	url = {https://doi.org/10.1145/3206505.3206516},
	year = {2018},
}

@inproceedings{10.1145/3206505.3206566,
	abstract = {The presentation of search results in GIS can expose the user to cluttered geographical maps, challenging the identification of relevant information. In order to address this issue, we propose a visualization model supporting interactive information filtering on 2D/3D maps. Our model is based on the introduction of transparency sliders that enable the user to tune the opacity, and thus the emphasis, of data categories in the map. In this way, he or she can focus the maps on the most relevant types of information for the task to be performed. A test with users provided positive results concerning the efficacy of our model.},
	address = {New York, NY, USA},
	articleno = {56},
	author = {Ardissono, Liliana and Delsanto, Matteo and Lucenteforte, Maurizio and Mauro, Noemi and Savoca, Adriano and Scanu, Daniele},
	booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
	doi = {10.1145/3206505.3206566},
	isbn = {9781450356169},
	keywords = {visual information filtering, search results visualization, opacity tuning, 2D/3D geographical maps},
	location = {Castiglione della Pescaia, Grosseto, Italy},
	numpages = {3},
	publisher = {Association for Computing Machinery},
	series = {AVI '18},
	title = {Transparency-based information filtering on 2D/3D geographical maps},
	url = {https://doi.org/10.1145/3206505.3206566},
	year = {2018},
}

@inproceedings{10.1145/3206505.3206575,
	abstract = {The rapid evolution of the Internet of Things (IoT) and Big Data technology has been generating a large amount and variety of sensing contents, including numeric measured values (e.g., timestamps, geolocations, or sensor logs) and multimedia (e.g., images, audios, and videos). In analyzing and understanding heterogeneous types of IoT-generated contents better, data visualization is an essential component of exploratory data analyses to facilitate information perception and knowledge extraction. This study introduces a holistic approach of storing, processing, and visualizing IoT-generated contents to support context-aware spatiotemporal insight by combining deep learning techniques with a geographical map interface. Visualization is provided under an interactive web-based user interface to help the an efficient visual exploration considering both time and geolocation by easy spatiotemporal query user interface1.},
	address = {New York, NY, USA},
	articleno = {70},
	author = {Lee, Jun and Kim, Kyoung-Sook and Lee, Ryong and Lee, Sang-Hwan},
	booktitle = {Proceedings of the 2018 International Conference on Advanced Visual Interfaces},
	doi = {10.1145/3206505.3206575},
	isbn = {9781450356169},
	keywords = {spatiotemporal analysis, object detection, geovisualization, deep learning, IoT-generated contents},
	location = {Castiglione della Pescaia, Grosseto, Italy},
	numpages = {3},
	publisher = {Association for Computing Machinery},
	series = {AVI '18},
	title = {Visual insight of spatiotemporal IoT-generated contents},
	url = {https://doi.org/10.1145/3206505.3206575},
	year = {2018},
}

@inproceedings{10.1145/3209281.3209300,
	abstract = {There is significant national interest in tackling issues surrounding the needs of vulnerable children and adults. At the same time, UK local authorities face severe financial challenges as a result of decreasing financial settlements and increasing demands from growing urban populations. This research employs state-of-the-art data analytics and visualisation techniques to analyse six years of local government social care data for the city of Birmingham, the UK's second most populated city. This analysis identifies: (i) service cost profiles over time; (ii) geographical dimensions to service demand and delivery; (iii) patterns in the provision of services, and (iv) the extent to which data value and data protection interact. The research accesses data held by the local authority to discover patterns and insights that may assist in the understanding of service demand, support decision making and resource management, whilst protecting and safeguarding its most vulnerable citizens. The use of data in this manner could also inform the approach a local authority has to its data, its capture and use, and the potential for supporting data-led management and service improvements.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Chotvijit, Sarunkorn and Thiarai, Malkiat and Jarvis, Stephen},
	booktitle = {Proceedings of the 19th Annual International Conference on Digital Government Research: Governance in the Data Age},
	doi = {10.1145/3209281.3209300},
	isbn = {9781450365260},
	keywords = {spatio-temporal analysis, social care, service provision, local authority, data analytics, Birmingham},
	location = {Delft, The Netherlands},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {dg.o '18},
	title = {Big data analytics in social care provision: spatial and temporal evidence from Birmingham},
	url = {https://doi.org/10.1145/3209281.3209300},
	year = {2018},
}

@inproceedings{10.1145/3209978.3209989,
	abstract = {This paper presents a new trajectory search engine called Torch for querying road network trajectory data. Torch is able to efficiently process two types of typical queries (similarity search and Boolean search), and support a wide variety of trajectory similarity functions. Additionally, we propose a new similarity function LORS in Torch to measure the similarity in a more effective and efficient manner. Indexing and search in Torch works as follows. First, each raw vehicle trajectory is transformed to a set of road segments (edges) and a set of crossings (vertices) on the road network. Then a lightweight edge and vertex index called LEVI is built. Given a query, a filtering framework over LEVI is used to dynamically prune the trajectory search space based on the similarity measure imposed. Finally, the result set (ranked or Boolean) is returned. Extensive experiments on real trajectory datasets verify the effectiveness and efficiency of Torch.},
	address = {New York, NY, USA},
	author = {Wang, Sheng and Bao, Zhifeng and Culpepper, J. Shane and Xie, Zizhe and Liu, Qizhi and Qin, Xiaolin},
	booktitle = {The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
	doi = {10.1145/3209978.3209989},
	isbn = {9781450356572},
	keywords = {trajectory search, road network, map matching, inverted index, effectiveness evaluation, compression},
	location = {Ann Arbor, MI, USA},
	numpages = {10},
	pages = {535–544},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '18},
	title = {Torch: A Search Engine for Trajectory Data},
	url = {https://doi.org/10.1145/3209978.3209989},
	year = {2018},
}

@inproceedings{10.1145/3209978.3210109,
	abstract = {We propose a location prediction method for tweets based on the geographical probability distribution of their terms over a region. In our method, the probabilities are calculated using Kernel Density Estimation (KDE), where the bandwidth of the kernel function for each term is determined separately according to the location indicativeness of the term. Prediction for a new tweet is performed by combining the probability distributions of its terms weighted by their information gain ratio. The method we propose relies on statistical approaches without requiring any parameter tuning. Experiments conducted on three tweet sets from different regions of the world indicate significant improvement in prediction accuracy compared to the state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Ozdikis, Ozer and Ramampiaro, Heri and N\o{}rv\r{a}g, Kjetil},
	booktitle = {The 41st International ACM SIGIR Conference on Research \&amp; Development in Information Retrieval},
	doi = {10.1145/3209978.3210109},
	isbn = {9781450356572},
	keywords = {tweet localization, location prediction, kernel density estimation},
	location = {Ann Arbor, MI, USA},
	numpages = {4},
	pages = {1149–1152},
	publisher = {Association for Computing Machinery},
	series = {SIGIR '18},
	title = {Locality-adapted Kernel Densities for Tweet Localization},
	url = {https://doi.org/10.1145/3209978.3210109},
	year = {2018},
}

@inproceedings{10.1145/3210284.3220506,
	abstract = {In this paper, we propose scalable algorithms allowing primo to infer a map of vessels' trajectories and secundo to predict future locations of a vessel on sea. Our system is based on Apache Spark -a fast and scalable engine for large-scale data processing. The training dataset is event-based. Each event depicts the GPS position of the vessel at a timestamp. We propose and implement a workflow computing trips' patterns, with GPS locations of each trip summarized using GeoHashing. The latter is an efficient encoding of a geographic location into a short string of letters and digits. In order to perform prediction queries efficiently, we propose (i) a geohash positional index which maps each geohash to a list of pairs (trip-pattern-identifier, offset of the geohash in the geohash sequence of the trip-pattern), (ii) a departure-port index which maps each departure port to a list of trip-patterns' identifiers, as well as (iii) a pairwise geohash sequence alignment allowing to score the similarity of two geohash-sequences using queen-spatial neighborhood.},
	address = {New York, NY, USA},
	author = {Moussa, Rim},
	booktitle = {Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
	doi = {10.1145/3210284.3220506},
	isbn = {9781450357821},
	keywords = {Streaming, Spatial data mining, Sequential Patterns, Map inference},
	location = {Hamilton, New Zealand},
	numpages = {4},
	pages = {213–216},
	publisher = {Association for Computing Machinery},
	series = {DEBS '18},
	title = {Scalable Maritime Traffic Map Inference and Real-time Prediction of Vessels' Future Locations on Apache Spark},
	url = {https://doi.org/10.1145/3210284.3220506},
	year = {2018},
}

@inproceedings{10.1145/3219104.3219143,
	abstract = {Tools developed for the Geospatial Data Analysis Building Blocks (GABBs) project that are available on MyGeoHub.org have been used for a geospatial experience for middle school students at Turned Onto Technology \&amp; Leadership (TOTAL) summer camps at Purdue University during 2016 and 2017. Thirty to thirty-six middle school students from diverse backgrounds and from around the United States used one such tool, MultiSpec Online to determine the area of a flood which occurred in June 2008 in southern Illinois and Indiana using Landsat 5 multispectral satellite data. The students also got hands-on experience using the GeoBuilder tool to find locations within Indiana with the highest reported rain events.The students received introductions to remote sensing, Geobuilder and MultiSpec Online during a one-day session and the following day they participated in a 75-minute active learning session to work through the exercises. The GABBs team worked closely with the students in multiple sessions helping them with technical questions and at the same time, collecting valuable feedback in how to improve the tools.At the end of the session for the second year, the students completed a four-question evaluation form to provide structured feedback. It was clear that the students were much more excited about the active learning session than the introduction (presentation) session.},
	address = {New York, NY, USA},
	articleno = {46},
	author = {Biehl, Larry and Zhao, Lan and Ellis, Carolyn and Kalyanam, Rajesh and Campbell, Robert and Song, Carol},
	booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
	doi = {10.1145/3219104.3219143},
	isbn = {9781450364461},
	keywords = {Cyberinfrastructure, science gateway},
	location = {Pittsburgh, PA, USA},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {PEARC '18},
	title = {Introducing Research Concepts to Middle School Students via a Geospatial Science Gateway},
	url = {https://doi.org/10.1145/3219104.3219143},
	year = {2018},
}

@inproceedings{10.1145/3219104.3219150,
	abstract = {Science gateways provide easy access to domain-specific tools and data. The field of Geographic Information Science and Systems (GIS) uses myriad tools and datasets, which raises challenges in designing a science gateway to meet users' diverse research and teaching needs. We describe a new science gateway called the GISandbox that is designed meet the needs of researchers and educators leveraging geospatial computing, which is situated at the nexus of GIS and computational science. The GISandbox is built on Jupyter Notebooks to create an easy, open, and flexible platform for geospatial computing. Jupyter Notebooks is a widely used interactive computing environment running in the browser that integrates live code, narrative, equations and images. We extend the Jupyter Notebook platform to enable users to run interactive notebooks on the cloud resource Jetstream or computationally-intensive notebooks on the Bridges supercomputer located at the Pittsburgh Supercomputing Center. A novel Job Management platform allows the user to easily submit a Jupyter Notebook for batch execution on Bridges (and eventually Comet), monitor the SLURM job, and retrieve output files. GISandbox Virtual Machines are created in Jetstream's Atmosphere interface and then deployed and configured using a series of Ansible scripts, which allow us to create an easily reproducible and scalable system. This paper outlines our vision for GISandbox, the current implementation, with a discussion looking toward the future and how the GISandbox could be used in other domains.},
	address = {New York, NY, USA},
	articleno = {44},
	author = {Shook, Eric and Vento, Davide Del and Zonca, Andrea and Wang, Jun},
	booktitle = {Proceedings of the Practice and Experience on Advanced Research Computing},
	doi = {10.1145/3219104.3219150},
	isbn = {9781450364461},
	keywords = {GIS, Geographic Information Systems, Science Gateway},
	location = {Pittsburgh, PA, USA},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {PEARC '18},
	title = {GISandbox: A Science Gateway for Geospatial Computing},
	url = {https://doi.org/10.1145/3219104.3219150},
	year = {2018},
}

@inproceedings{10.1145/3219819.3219900,
	abstract = {Vehicle travel time estimation or estimated time of arrival (ETA) is one of the most important location-based services (LBS). It is becoming increasingly important and has been widely used as a basic service in navigation systems and intelligent transportation systems. This paper presents a novel machine learning solution to predict the vehicle travel time based on floating-car data. First, we formulate ETA as a pure spatial-temporal regression problem based on a large set of effective features. Second, we adapt different existing machine learning models to solve the regression problem. Furthermore, we propose a Wide-Deep-Recurrent (WDR) learning model to accurately predict the travel time along a given route at a given departure time. We then jointly train wide linear models, deep neural networks and recurrent neural networks together to take full advantages of all three models. We evaluate our solution offline with millions of historical vehicle travel data. We also deploy the proposed solution on Didi Chuxing's platform, which services billions of ETA requests and benefits millions of customers per day. Our extensive evaluations show that our proposed deep learning algorithm significantly outperforms the state-of-the-art learning algorithms, as well as the solutions provided by leading industry LBS providers.},
	address = {New York, NY, USA},
	author = {Wang, Zheng and Fu, Kun and Ye, Jieping},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3219819.3219900},
	isbn = {9781450355520},
	keywords = {estimated time of arrival, location-based services, wide-deep-recurrent learning},
	location = {London, United Kingdom},
	numpages = {9},
	pages = {858–866},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {Learning to Estimate the Travel Time},
	url = {https://doi.org/10.1145/3219819.3219900},
	year = {2018},
}

@inproceedings{10.1145/3219819.3219929,
	abstract = {Cultural activity is an inherent aspect of urban life and the success of a modern city is largely determined by its capacity to offer generous cultural entertainment to its citizens. To this end, the optimal allocation of cultural establishments and related resources across urban regions becomes of vital importance, as it can reduce financial costs in terms of planning and improve quality of life in the city, more generally. In this paper, we make use of a large longitudinal dataset of user location check-ins from the online social network WeChat to develop a data-driven framework for cultural planning in the city of Beijing. We exploit rich spatio-temporal representations on user activity at cultural venues and use a novel extended version of the traditional latent Dirichlet allocation model that incorporates temporal information to identify latent patterns of urban cultural interactions. Using the characteristic typologies of mobile user cultural activities emitted by the model, we determine the levels of demand for different types of cultural resources across urban areas. We then compare those with the corresponding levels of supply as driven by the presence and spatial reach of cultural venues in local areas to obtain high resolution maps that indicate urban regions with lack of cultural resources, and thus give suggestions for further urban cultural planning and investment optimisation.},
	address = {New York, NY, USA},
	author = {Zhou, Xiao and Noulas, Anastasios and Mascolo, Cecilia and Zhao, Zhongxiang},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3219819.3219929},
	isbn = {9781450355520},
	keywords = {pattern mining, spatial accessibility, spatio-temporal analysis, topic modeling, urban computing},
	location = {London, United Kingdom},
	numpages = {10},
	pages = {1069–1078},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {Discovering Latent Patterns of Urban Cultural Interactions in WeChat for Modern City Planning},
	url = {https://doi.org/10.1145/3219819.3219929},
	year = {2018},
}

@inproceedings{10.1145/3219819.3220031,
	abstract = {With the proliferation of mobile devices and location-based services, rich geo-tagged data is becoming prevalent and this offer great opportunities to understand different geographical regions (e.g., shopping areas). However, the huge number of regions with complicated spatial information are expensive for people to explore and understand. To solve this issue, we study the problem of searching similar regions given a user specified query region. The problem is challenging in both similarity definition and search efficiency. To tackle the two challenges, we propose a novel solution equipped by (1) a deep learning approach to learning the similarity that considers both object attributes and the relative locations between objects; and (2) an efficient branch and bound search algorithm for finding top-N similar regions. Moreover, we propose an approximation method to further improve the efficiency by slightly sacrificing the accuracy. Our experiments on three real world datasets demonstrate that our solution improves both the accuracy and search efficiency by a significant margin compared with the state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Liu, Yiding and Zhao, Kaiqi and Cong, Gao},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3219819.3220031},
	isbn = {9781450355520},
	keywords = {metric learning, similarity search, spatial data},
	location = {London, United Kingdom},
	numpages = {10},
	pages = {1850–1859},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {Efficient Similar Region Search with Deep Metric Learning},
	url = {https://doi.org/10.1145/3219819.3220031},
	year = {2018},
}

@inproceedings{10.1145/3219819.3220053,
	abstract = {Flood extent mapping plays a crucial role in disaster management and national water forecasting. Unfortunately, traditional classification methods are often hampered by the existence of noise, obstacles and heterogeneity in spectral features as well as implicit anisotropic spatial dependency across class labels. In this paper, we propose geographical hidden Markov tree, a probabilistic graphical model that generalizes the common hidden Markov model from a one dimensional sequence to a two dimensional map. Partial order class dependency is incorporated in the hidden class layer with a reverse tree structure. We also investigate computational algorithms for reverse tree construction, model parameter learning and class inference. Extensive evaluations on both synthetic and real world datasets show that proposed model outperforms multiple baselines in flood mapping, and our algorithms are scalable on large data sizes.},
	address = {New York, NY, USA},
	author = {Xie, Miao and Jiang, Zhe and Sainju, Arpan Man},
	booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3219819.3220053},
	isbn = {9781450355520},
	keywords = {geographical hidden markov tree, spatial classification},
	location = {London, United Kingdom},
	numpages = {10},
	pages = {2545–2554},
	publisher = {Association for Computing Machinery},
	series = {KDD '18},
	title = {Geographical Hidden Markov Tree for Flood Extent Mapping},
	url = {https://doi.org/10.1145/3219819.3220053},
	year = {2018},
}

@inproceedings{10.1145/3220228.3220233,
	abstract = {Road accidents are one of the major concerns for humanity today, since it is the source of many diseases and injuries. However, to make adequate road safety actions and policies, it is required get road accident information and analyses them. In the present work, we propose a Cloud based system for road safety called RoSa-Cloud. The system provides a cloud service offering access to road accident information from any web site and any client device (phone, tablet, laptop, etc.). We provided a prototype of RoSa-Cloud system and developed an android mobile application called RoSa App. The mobile application is very useful to police service to avoid entering precisions about accident location, since all modern smart phones supports GPS localization and by consequence the mobile application can get and record the precise GPS coordinates.},
	address = {New York, NY, USA},
	author = {Kaci, Abdellah and Nacef, Abdelhakim and Henni, Abderazzak},
	booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3220228.3220233},
	isbn = {9781450364454},
	keywords = {web services, road safety, mobility, cloud computing},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {132–136},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '18},
	title = {Mobile cloud system for road safety},
	url = {https://doi.org/10.1145/3220228.3220233},
	year = {2018},
}

@inproceedings{10.1145/3220228.3220236,
	abstract = {Geospatial Big Data analytics are changing the way that businesses operate in many industries. Although a good number of research works have reported in the literature on geospatial data analytics and real-time data processing of large spatial data streams, only a few have addressed the full geospatial big data analytics project lifecycle and geospatial data science project lifecycle. Big data analysis differs from traditional data analysis primarily due to the volume, velocity and variety characteristics of the data being processed. One of a motivation of introducing new framework is to address these big data analysis challenges. Geospatial data science projects differ from most traditional data analysis projects because they could be complex and in need of advanced technologies in comparison to the traditional data analysis projects. For this reason, it is essential to have a process to govern the project and ensure that the project participants are competent enough to carry on the process. To this end, this paper presents, new geospatial big data mining and machine learning framework for geospatial data acquisition, data fusion, data storing, managing, processing, analysing, visualising and modelling and evaluation. Having a good process for data analysis and clear guidelines for comprehensive analysis is always a plus point for any data science project. It also helps to predict required time and resources early in the process to get a clear idea of the business problem to be solved.},
	address = {New York, NY, USA},
	author = {Saraee, Mo and Silva, Charith},
	booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3220228.3220236},
	isbn = {9781450364454},
	keywords = {machine learning, geospatial big data, data science, data mining, big data},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {98–102},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '18},
	title = {A new data science framework for analysing and mining geospatial big data},
	url = {https://doi.org/10.1145/3220228.3220236},
	year = {2018},
}

@inproceedings{10.1145/3220228.3220237,
	abstract = {Vineyard parcels delimitation is a preliminary but important task to support zoning activities, which can be burdensome and time-consuming when manually performed. In spite of being desirable to overcome such issue, the implementation of a semi-/fully automatic delimitation approach can meet serious development challenges when dealing with vineyards like the ones that prevail in Douro Region (north-east of Portugal), mainly due to the great diversity of parcel/row formats and several factors that can hamper detection as, for example, interrupted rows and inter-row vegetation. Thereby, with the aim of addressing vineyard parcels detection and delimitation in Douro Region, a preliminary method based on segmentation and morphological operations upon highresolution aerial imagery is proposed. This method was tested in a data set collected from vineyards located at the University of Tr'as-os-Montes and Alto Douro(Vila Real, Portugal). The presence of some of the previously mentioned challenging conditions - namely interrupted rows and inter-row grassing - in a few parcels contributed to lower the overall detection accuracy, pointing out the need for future improvements. Notwithstanding, encouraging preliminary results were achieved.},
	address = {New York, NY, USA},
	author = {Ad\~{a}o, Telmo and P\'{a}dua, Lu\'{\i}s and Hruundefinedka, Jon\'{a}undefined and Marques, Pedro and Peres, Emanuel and Sousa, Joaquim Jo\~{a}o and Cunha, Ant\'{o}nio and de Sousa, Ant\'{o}nio M. R. and Morais, Raul},
	booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3220228.3220237},
	isbn = {9781450364454},
	keywords = {zoning, vineyard parcel, digitalimage processing, automatic vine parcelling, UAV, UAS, RGB},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {67–71},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '18},
	title = {A pilot digital image processing approach for detecting vineyard parcels in Douro region through high-resolution aerial imagery},
	url = {https://doi.org/10.1145/3220228.3220237},
	year = {2018},
}

@inproceedings{10.1145/3220228.3220239,
	abstract = {This paper is devoted to the study of representation problem and use of experience in planning and implementing logistics projects in the field of e-business using geoinformation models. Geoinformation models can be presented as a description of the site of the territory on which the logistics project is implemented. The description consists of maps, charts and plans, as well as links to non-cartographic sources of information. The role of the quality criterion is considered as the reliability of the generated solutions. The reliability of the project is means the practical possibility of its implementation with minimal losses. Factors determining reliability in this sense are considered. The figurative representation of knowledge about the precedents of logistics projects is analyzed. Mapping of permissible transformations of the components of a logistics project in the form of cartographic objects is a distinctive feature of a figurative representation. Most often they are polygonal objects of location of logistics centers and transportation trajectories. The possibility of transferring an allowable transformation into another space-time domain with observance of topological constraints is considered as a way of preserving the meaning of the project. A method of transforming images into a given region of the territory is described. An example of a completed project with an estimate of reliability is given.},
	address = {New York, NY, USA},
	author = {Belyakov, Stanislav and Bozhenyuk, Alexander and Belyakova, Marina and Zubkov, Sergey},
	booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3220228.3220239},
	isbn = {9781450364454},
	keywords = {logistics projects, intelligent systems, geoinformation modeling},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {108–112},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '18},
	title = {Models experience presentation for logistics projects implementation based on geoinformation models},
	url = {https://doi.org/10.1145/3220228.3220239},
	year = {2018},
}

@inproceedings{10.1145/3220228.3220241,
	abstract = {Advances in Unmanned Aerial Systems (UAS) allowed them to become both flexible and cost-effective. When combined with computer vision data processing techniques they are a good way to obtain high-resolution imagery and 3D information. As such, UAS can be advantageous both for agriculture and forestry areas, where the need for data acquisition at specific times and within a specific time frame is crucial, enabling the extraction of several measurements from different crop types. In this study a low-cost UAS was used to survey an area mainly composed by chestnut trees (Castanea sativa Mill.). Flights were performed at different heights (ranging from 30 to 120 m), in single and double grid flight patterns, and photogrammetric processing was then applied. The obtained information consists of orthophoto mosaics and digital elevation models which enable the measurement of individual tree's parameters such as tree crown diameter and tree height. Results demonstrate that despite its lower spatial resolution, data from single grid flights carried out at higher heights provided more reliable results than data acquired at lower flight heights. Higher number of images acquired in double grid flights also improved the results. Overall, the obtained results are encouraging, presenting a R2 higher than 0.9 and an overall root mean square error of 44 cm.},
	address = {New York, NY, USA},
	author = {P\'{a}dua, Lu\'{\i}s and Marques, Pedro and Ad\'{a}o, Telmo and Hru\v{s}ka, Jon\'{a}\v{s} and Peres, Emanuel and Morais, Raul and Sousa, Ant\'{o}nio and Sousa, Joaquim J.},
	booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3220228.3220241},
	isbn = {9781450364454},
	keywords = {unmanned aerial systems, remote sensing, photogrammetric processing, forestry, chestnut trees, canopy height models},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {87–91},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '18},
	title = {UAS-based imagery and photogrammetric processing for tree height and crown diameter extraction},
	url = {https://doi.org/10.1145/3220228.3220241},
	year = {2018},
}

@inproceedings{10.1145/3220228.3220242,
	abstract = {In agricultural applications hyperspectral imaging is used in cases where differences in spectral reflectance of the examined objects are small. However, the large amount of data generated by hyperspectral sensors requires advance processing methods. Machine learning approaches may play an important role in this task. They are known for decades, but they need high volume of data to compute accurate results. Until recently, the availability of hyperspectral data was a big drawback. It was first used in satellites, later in manned aircrafts and data availability from those platforms was limited because of logistics complexity and high price. Nowadays, hyperspectral sensors are available for unmanned aerial vehicles, which enabled to reach a high volume of data, thus overcoming these issues. This way, the aim of this paper is to present the status of the usage of machine learning approaches in the hyperspectral data processing, with a focus on agriculture applications. Nevertheless, there are not many studies available applying machine learning approach to hyperspectral data for agricultural applications. This apparent limitation was in fact the inspiration for making this survey. Preliminary results using UAV-based data are presented, showing the suitability of machine learning techniques in remote sensed data.},
	address = {New York, NY, USA},
	author = {Hru\v{s}ka, Jon\'{a}\v{s} and Ad\~{a}o, Telmo and P\'{a}dua, Lu\'{\i}s and Marques, Pedro and Cunha, Ant\'{o}nio and Peres, Emanuel and Sousa, Ant\'{o}nio and Morais, Raul and Sousa, Joaquim J.},
	booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3220228.3220242},
	isbn = {9781450364454},
	keywords = {remote sensing, machine learning, hyperspectral data, deep learning, agriculture},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {137–141},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '18},
	title = {Machine learning classification methods in hyperspectral data processing for agricultural applications},
	url = {https://doi.org/10.1145/3220228.3220242},
	year = {2018},
}

@inproceedings{10.1145/3220228.3220243,
	abstract = {The cost-effectiveness of unmanned aerial systems (UAS) makes them suitable platforms to survey cultural heritage sites. Developments in photogrammetry provide methods capable to generate accurate 3D models out of 2D aerial images. Considering the involved technologies, the purpose of this paper is to document the Chapel of Espir\'{\i}to Santo: a very relevant monument for Vila Real (Portugal) that is currently located at the campus of the University of Tr\'{a}s-os-Montes and Alto Douro. The UAS based aerial imagery survey approach is presented along with photogrammetric process to build chapel's 3D model. Moreover, two photogrammetric software were compared - Pix4Dmapper Pro and Agisoft Photoscan - in terms of modelling accuracy and functionalities ease of use.},
	address = {New York, NY, USA},
	author = {P\'{a}dua, Lu\'{\i}s and Ad\~{a}o, Telmo and Hru\v{s}ka, Jon\'{a}\v{s} and Marques, Pedro and Sousa, Ant\'{o}nio and Morais, Raul and Louren\c{c}o, Jos\'{e} Martinho and Sousa, Joaquim J. and Peres, Emanuel},
	booktitle = {Proceedings of the International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3220228.3220243},
	isbn = {9781450364454},
	keywords = {photogrammetry, cultural heritage, UAV, 3D models},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {72–76},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '18},
	title = {UAS-based photogrammetry of cultural heritage sites: a case study addressing Chapel of Esp\'{\i}rito Santo and photogrammetric software comparison},
	url = {https://doi.org/10.1145/3220228.3220243},
	year = {2018},
}

@inproceedings{10.1145/3225058.3225105,
	abstract = {In recent times, geospatial datasets are growing in terms of size, complexity and heterogeneity. High performance systems are needed to analyze such data to produce actionable insights in an efficient manner. For polygonal a.k.a vector datasets, operations such as I/O, data partitioning, communication, and load balancing becomes challenging in a cluster environment. In this work, we present MPI-Vector-IO 1, a parallel I/O library that we have designed using MPI-IO specifically for partitioning and reading irregular vector data formats such as Well Known Text. It makes MPI aware of spatial data, spatial primitives and provides support for spatial data types embedded within collective computation and communication using MPI message-passing library. These abstractions along with parallel I/O support are useful for parallel Geographic Information System (GIS) application development on HPC platforms.Performance evaluation is done on Lustre and GPFS filesystems. MPI-Vector-IO scales well with MPI processes and file size and achieves bandwidth up to 22 GB/s for common spatial data access patterns. We observed that independent file read functions performed better than collective functions in MPI-IO for contiguous access pattern on Lustre. In general, the I/O is improved by one to two orders of magnitude over real-world datasets using up to 1152 CPU cores. Spatial Join query is used as an exemplar to demonstrate an end-to-end application using MPI-Vector-IO.},
	address = {New York, NY, USA},
	articleno = {13},
	author = {Puri, Satish and Paudel, Anmol and Prasad, Sushil K.},
	booktitle = {Proceedings of the 47th International Conference on Parallel Processing},
	doi = {10.1145/3225058.3225105},
	isbn = {9781450365109},
	keywords = {HPC, Message Passing Interface, Parallel IO, Spatial Data, Spatial Join},
	location = {Eugene, OR, USA},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	series = {ICPP '18},
	title = {MPI-Vector-IO: Parallel I/O and Partitioning for Geospatial Vector Data},
	url = {https://doi.org/10.1145/3225058.3225105},
	year = {2018},
}

@inproceedings{10.1145/3229345.3229358,
	abstract = {Open Government Data has been made available by public institutions in Brazil and the world, and can add value to various sectors of society. Open data is also linked to smart cities, and hence important in this context, as it is the first step towards public transparency. Despite the wide range of Open Government Data, interpreting such data sets is a non-trivial task, due to the massive amount of raw data. This stimulates the search for techniques and methodologies that allow the interpretation of implicit information and deduction of new knowledge. One of the approaches used for these tasks involves the use of data visualizations. In addition to data visualizations classically used in descriptive statistics for data analysis, such as line or bar charts, many web sites have been used map visualization techniques. This type of visualization is important, since visualization of georeferenced data combined with other types of information can aid its interpretation. However, for data visualization construction on maps, it is necessary that the objects to be visualized are georeferenced. There are standards for turning such data available, however they are diverse, which may make it difficult for a single tool to display views from different sources. This work aims to present a framework that defines data standards for constructing data visualizations on maps of various types. Based on this framework, a tool was implemented to facilitate the creation of different map views, both by developers of open data portals and by users who analyze such data.},
	address = {New York, NY, USA},
	articleno = {12},
	author = {Santos, Victor and Camara, Pedro and Bernardini, Flavia and Viterbo, Jose and Jorge, Douglas},
	booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
	doi = {10.1145/3229345.3229358},
	isbn = {9781450365598},
	keywords = {Transparency, Open Government Data, Information Visualization in Maps},
	location = {Caxias do Sul, Brazil},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {SBSI '18},
	title = {A framework for constructing open data map visualizations},
	url = {https://doi.org/10.1145/3229345.3229358},
	year = {2018},
}

@inproceedings{10.1145/3229345.3229367,
	abstract = {In agriculture, land use knowledge is very important for the analysis of agricultural, to estimate production and propose crop production forecast models, environmental monitoring and sustainable planning. Therefore, to coffee agribusiness the mapping of cultivated areas is essential. In the southern region of Minas Gerais, this mapping has been performed using geotechnologies. However, this strategy has some limitations resulting in maps of land use with classification errors. In this work, the proposed solution to minimize this problem is the combination of remote sensing with citizen science. For this purpose, a mobile application, named Demarcaf\'{e}, has been developed. The Demarcaf\'{e} allows citizens (non-researchers) to collaborate with this mapping. Hence, the technology assumes the role of facilitator of this connection. The Demarcaf\'{e} has been evaluated and the results suggest that the tool can represent an important advance in this mapping.},
	address = {New York, NY, USA},
	articleno = {19},
	author = {Souza, Vanessa C. O. and Castro, Yasmin B. and Paula, Melise M. V. and Volpato, Margarete M. L.},
	booktitle = {Proceedings of the XIV Brazilian Symposium on Information Systems},
	doi = {10.1145/3229345.3229367},
	isbn = {9781450365598},
	keywords = {geotechnologies, collaborative systems, coffee, citizen science},
	location = {Caxias do Sul, Brazil},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {SBSI '18},
	title = {Demarcaf\'{e}: a proposal to support the mapping of coffee areas using citizen science},
	url = {https://doi.org/10.1145/3229345.3229367},
	year = {2018},
}

@inproceedings{10.1145/3230348.3230379,
	abstract = {The standardization of spatial information is the basis of spatial database. The spatial data of the coal mine belong to the underground space data. The mining of coal mine is a process of movement, so the standardization of the spatial information of the coal mine is difficult. The paper enumerates the progress of the standardization of space information in various countries. Based on ISO19100 standards and China national standards, the principles of coal mine spatial information classification and coding is put forward. A set of classification and coding system for spatial information of coal mine is put forward. The system uses Hybrid Classification method. Classification and coding of coal mine spatial information are carried out by using classification code and identification code. The results of classification and coding are suitable for the actual situation.},
	address = {New York, NY, USA},
	author = {Zhenghao, Yu and Lei, Song},
	booktitle = {Proceedings of the 2018 1st International Conference on Internet and E-Business},
	doi = {10.1145/3230348.3230379},
	isbn = {9781450363754},
	keywords = {Standardization issue, Hybrid Classification, Code, Coal Mine Space Information},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {162–165},
	publisher = {Association for Computing Machinery},
	series = {ICIEB '18},
	title = {Research on Standardization of Space Information in Coal Mine},
	url = {https://doi.org/10.1145/3230348.3230379},
	year = {2018},
}

@article{10.1145/3232852,
	abstract = {Given a query record, record matching is the problem of finding database records that represent the same real-world object. In the easiest scenario, a database record is completely identical to the query. However, in most cases, problems do arise, for instance, as a result of data errors or data integrated from multiple sources or received from restrictive form fields. These problems are usually difficult, because they require a variety of actions, including field segmentation, decoding of values, and similarity comparisons, each requiring some domain knowledge.In this article, we study the problem of matching records that contain address information, including attributes such as Street-address and City. To facilitate this matching process, we propose a domain-specific procedure to, first, enrich each record with a more complete representation of the address information through geocoding and reverse-geocoding and, second, to select the best similarity measure per each address attribute that will finally help the classifier to achieve the best f-measure. We report on our experience in selecting geocoding services and discovering similarity measures for a concrete but common industry use-case.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Koumarelas, Ioannis and Kroschk, Axel and Mosley, Clifford and Naumann, Felix},
	doi = {10.1145/3232852},
	issn = {1936-1955},
	issue_date = {June 2018},
	journal = {J. Data and Information Quality},
	keywords = {Address matching, address normalization, address parsing, conditional functional dependencies, duplicate detection, geocoding, geographic information systems, random forest, record linkage, similarity measures},
	month = {sep},
	number = {2},
	numpages = {16},
	publisher = {Association for Computing Machinery},
	title = {Experience: Enhancing Address Matching with Geocoding and Similarity Measure Selection},
	url = {https://doi.org/10.1145/3232852},
	volume = {10},
	year = {2018},
}

@article{10.1145/3234505,
	abstract = {This article concerns the definition and identification of qualitative spatial relationships for the full and partial enclosure of spatial regions. The article precisely defines three relationships between regions—“surrounds,” “engulfs,” and “envelops”—highlighting the correspondence to similar definitions in the literature. An efficient algorithm capable of identifying these qualitative spatial relations in a network of dynamic (mobile) geosensor nodes is developed and tested. The algorithms are wholly decentralized, and operate in-network with no centralized control. The algorithms are also “coordinate-free,” able to operate in distributed spatial computing environments where coordinate locations are expensive to capture or otherwise unavailable. Experimental evaluation of the algorithms designed demonstrates the efficiency of the approach. Although the algorithm communication complexity is dominated by an overall worst-case O(n2) leader election algorithm, the experiments show in practice an average-case complexity approaching linear, O(n1.1).},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Both, Alan and Duckham, Matt and Worboys, Michael F.},
	doi = {10.1145/3234505},
	issn = {2374-0353},
	issue_date = {June 2018},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Geosensor networks, Voronoi regions, decentralized algorithms, moving objects, qualitative spatial reasoning},
	month = {aug},
	number = {2},
	numpages = {21},
	publisher = {Association for Computing Machinery},
	title = {Identifying Surrounds and Engulfs Relations in Mobile and Coordinate-Free Geosensor Networks},
	url = {https://doi.org/10.1145/3234505},
	volume = {4},
	year = {2018},
}

@article{10.1145/3234692,
	abstract = {Road networks are essential nowadays, especially for people travelling to large, unfamiliar cities. Moreover, cities are constantly growing and road networks need periodic updates to provide reliable information. We propose an automatic method to generate the road network using a GPS trajectory dataset. The method, called CellNet, works by first detecting the intersections (junctions) using a clustering-based technique and then creating the road segments in-between. We compare CellNet against conceptually different alternatives using Chicago and Joensuu datasets. The results show that CellNet provides better accuracy and is less sensitive to parameter setup. The size of the generated road network is only 25\% of the networks produced by other methods. This implies that the network provided by CellNet has much less redundancy.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Mariescu-Istodor, Radu and Fr\"{a}nti, Pasi},
	doi = {10.1145/3234692},
	issn = {2374-0353},
	issue_date = {September 2018},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {GPS routes, clustering, grid, road network, similarity},
	month = {sep},
	number = {3},
	numpages = {22},
	publisher = {Association for Computing Machinery},
	title = {CellNet: Inferring Road Networks from GPS Trajectories},
	url = {https://doi.org/10.1145/3234692},
	volume = {4},
	year = {2018},
}

@inproceedings{10.1145/3234695.3236363,
	abstract = {Navigation assistive technologies aim to improve the mobility of blind or visually impaired people. In particular, turn-by-turn navigation assistants provide sequential instructions to enable autonomous guidance towards a destination. A problem frequently addressed in the literature is to obtain accurate position and orientation of the user during such guidance. An orthogonal challenge, often overlooked in the literature, is how precisely navigation instructions are followed by users. In particular, imprecisions in following rotation instructions lead to rotation errors that can significantly affect navigation. Indeed, a relatively small error during a turn is amplified by the following frontal movement and can lead the user towards incorrect or dangerous paths. In this contribution, we study rotation errors and their effect on turn-by-turn guidance for individuals with visual impairments. We analyze a dataset of indoor trajectories of 11 blind participants guided along three routes through a multi-story shopping mall using NavCog, a turn-by-turn smartphone navigation assistant. We find that participants extend rotations by 17º on average. The error is not proportional to the expected rotation; instead, it is accentuated for "slight turns" (22.5º-60º), while "ample turns" (60º-120º) are consistently approximated to 90º. We generalize our findings as design considerations for engineering navigation assistance in real-world scenarios.},
	address = {New York, NY, USA},
	author = {Ahmetovic, Dragan and Oh, Uran and Mascetti, Sergio and Asakawa, Chieko},
	booktitle = {Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility},
	doi = {10.1145/3234695.3236363},
	isbn = {9781450356503},
	keywords = {navigation assistive technologies, orientation and mobility, visual impairments and blindness},
	location = {Galway, Ireland},
	numpages = {7},
	pages = {333–339},
	publisher = {Association for Computing Machinery},
	series = {ASSETS '18},
	title = {Turn Right: Analysis of Rotation Errors in Turn-by-Turn Navigation for Individuals with Visual Impairments},
	url = {https://doi.org/10.1145/3234695.3236363},
	year = {2018},
}

@article{10.1145/3237186,
	abstract = {Route planning is a challenging problem for urban computing that usually involves the processing of a huge amount of data and collaborative user feedback. Traditionally, route planning services are street-based, that is, even paths for a pedestrian are suggested in terms of streets. However, such models are not suitable for users with certain disabilities. To address this problem, we have performed a requirement analysis with a group of wheelchair-users and their companions to understand their urban mobility experience. Given that perspective, we describe in this article a sidewalk-based model to accommodate the needs for a wheelchair route planning service. The model is mathematically defined as a graph, where the vertices are the city block corners and the edges are the sidewalks or crosswalks. The edge costs are derived from important accessibility features, such as distance, path inclination, and existence and maintenance conditions of curb ramps, crosswalks, and sidewalks. The model has been designed so that user feedback is considered to help updating the model when accessibility issues are detected, by wheelchair-users and companions, or solved, by the department of city planning. We also present a route planning algorithm that provides a set of alternative routes based on accessibility conditions, and a shortcut recommender algorithm to support accessibility-related decision making by the department of city planning. Experiments, by using PgRouting and PostGIS with open data, are reported for a Brazilian city neighborhood to validate the model and the route planning service.},
	address = {New York, NY, USA},
	articleno = {18},
	author = {Barczyszyn, Guilherme L. and Camenar, Let\'{\i}cia M. De O. and Nascimento, Diego De F. Do and Kozievitch, N\'{a}dia P. and Silva, Ricardo D. Da and Almeida, Leonelo D. A. and Santi, Juliana De and Minetto, Rodrigo},
	doi = {10.1145/3237186},
	issn = {1936-7228},
	issue_date = {September 2018},
	journal = {ACM Trans. Access. Comput.},
	keywords = {Accessible shortest path, geographic information systems, shortcut recommendation, sidewalk-based route planning, user collaboration},
	month = {aug},
	number = {3},
	numpages = {26},
	publisher = {Association for Computing Machinery},
	title = {A Collaborative System for Suitable Wheelchair Route Planning},
	url = {https://doi.org/10.1145/3237186},
	volume = {11},
	year = {2018},
}

@article{10.1145/3239163,
	abstract = {Locating the original region of tampered features is a challenging task for existing 2D vector map fragile watermarking methods. This article presents a 2D vector map fragile watermarking framework that locates not only the current but also the original region of tampered feature groups. In particular, we propose dividing the features of the host vector map into groups, and embedding a watermark consisting of location-bits and check-bits into each group at the sender side. At the receiver side, by comparing the extracted and calculated check-bits, one can identify tampered groups and locate their current regions. Then the location-bits extracted from the mapping groups are used to indicate the original regions of the tampered groups. To demonstrate and analyze the applicability of this framework, we instantiate it by proposing a simulated annealing (SA)-based group division method, a group mapping method, a minimum encasing rectangle (MER) based location-bits generation method and a check-bits generation method, and use an existing reversible data hiding method for watermark embedding. The experimental results show that the proposed framework can locate all the regions influenced by tampering, and the SA-based group division method can get a better region location ability.},
	address = {New York, NY, USA},
	articleno = {12},
	author = {Wang, Nana and Kankanhalli, Mohan},
	doi = {10.1145/3239163},
	issn = {2374-0353},
	issue_date = {December 2018},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {2D vector map, Fragile watermarking, authentication, region location, tamper detection},
	month = {sep},
	number = {4},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {2D Vector Map Fragile Watermarking with Region Location},
	url = {https://doi.org/10.1145/3239163},
	volume = {4},
	year = {2018},
}

@inproceedings{10.1145/3240876.3240909,
	abstract = {With the rapid development of Internet, more and more new events break first in social media. Some studies are now carried out on detecting events in real-time on social media like Twitter. Inferring the geolocation of events is an important problem for better understanding the events. In this work, we develop a multi-level method to infer the event location. We leverage coordinates, text content and geographical knowledge into geolocation inference algorithm to tackle this problem. In addition we construct a World City data set and propose a location prediction model to infer single tweet location as a supplement to the algorithm.},
	address = {New York, NY, USA},
	articleno = {26},
	author = {Ying, Yue and Peng, Chen and Dong, Chao and Li, Yang and Feng, Yan},
	booktitle = {Proceedings of the 10th International Conference on Internet Multimedia Computing and Service},
	doi = {10.1145/3240876.3240909},
	isbn = {9781450365208},
	keywords = {Twitter, event geolocation, geolocation inferring},
	location = {Nanjing, China},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {ICIMCS '18},
	title = {Inferring event geolocation based on Twitter},
	url = {https://doi.org/10.1145/3240876.3240909},
	year = {2018},
}

@inproceedings{10.1145/3241403.3241439,
	abstract = {Earth Observation data archives are currently growing at unprecedented speeds. New satellites add petabytes of data every year. At the same time, the amount of data provided by Earth-based in-situ networks is growing at enormous rates. These developments have led to a change in data processing paradigms. Data is not down-loaded and processed locally anymore, but applications are sent to the data. This paper demonstrates an Big Data architecture that allows for interoperable solutions across data providers, integrators, and users. The availability of a mature domain architecture as provided by the Open Geospatial Consortium provides a solid base and allows for uniform microservice handling. The makro-and microarchitecture described herein uses self-contained Docker-images to allow for transparent microservices, horizontal scale-out, and high reliability and maintainability thanks to decoupled and self-sustained execution elements.},
	address = {New York, NY, USA},
	articleno = {34},
	author = {Simonis, Ingo},
	booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
	doi = {10.1145/3241403.3241439},
	isbn = {9781450364836},
	keywords = {container, microservices, software architecture},
	location = {Madrid, Spain},
	numpages = {3},
	publisher = {Association for Computing Machinery},
	series = {ECSA '18},
	title = {Container-based architecture to optimize the integration of microservices into cloud-based data-intensive application scenarios},
	url = {https://doi.org/10.1145/3241403.3241439},
	year = {2018},
}

@inproceedings{10.1145/3242840.3242878,
	abstract = {This research draws inspiration from the desire to help sustain and boost the production of abaca in Catanduanes being the topmost abaca producing province in the Philippines through an information campaign in a website with the application of data visualization tools and techniques. Result of this research will draw attention about the usefulness and benefits that could be derived through the production of abaca in uplifting the socio-economic status and standard of living of farmers. Likewise, this research would also support the rural development program of the government on embracing climate-resilient ecological farming practices to adjust to climate change. Sustainability of the abaca production in the province means ensuring vibrancy of economy especially for the abaca farmers. Demographic data about abaca production were obtained from the abaca concerned government offices in Catanduanes. Documentary analysis, Interview, Website development and evaluation were the methodologies employed in this investigation. Likewise, a survey questionnaire was utilized to gather data about the usability of the website from the persons from government agencies that are concerned on abaca and website development. Three (3) criteria were used to evaluate the usability of the website - usefulness, ease of use and satisfaction. Frequency count and weighted mean were the statistics used in analyzing the responses and the respondents have generally evaluated the developed website as "Strongly Agree".},
	address = {New York, NY, USA},
	author = {Tapado, Belen M. and Palaoag, Thelma D.},
	booktitle = {Proceedings of the 2nd International Conference on Algorithms, Computing and Systems},
	doi = {10.1145/3242840.3242878},
	isbn = {9781450365093},
	keywords = {Catanduanes abaca, abaca, abaca production, abaca website, data visualization},
	location = {Beijing, China},
	numpages = {6},
	pages = {65–70},
	publisher = {Association for Computing Machinery},
	series = {ICACS '18},
	title = {Data Visualization of Abaca Production in Catanduanes},
	url = {https://doi.org/10.1145/3242840.3242878},
	year = {2018},
}

@inproceedings{10.1145/3243082.3264671,
	abstract = {The smart city concept envisions the use of a plethora of systems providing different data and services to assist public agents in making strategic decisions for the city based on reliable information. However, the high heterogeneity and lack of standardization of these systems and their data challenge the development of value-added applications, which can be enriched with geographic information to allow for more effective actions over the real-world urban space. This paper introduces Smart Geo Layers (SGeoL), a geographic-layered data middleware platform conceived to integrate data provided by heterogeneous sources in a smart city environment. Besides easing such an integration, SGeoL allows correlating data with geographic information and it provides functionalities such as data aggregation, visualization, query, and analysis. This paper also presents the use of SGeoL in the development of a urban planning application as well as results of some computational experiments aimed to assess the performance of the platform.},
	address = {New York, NY, USA},
	author = {Souza, Arthur and Pereira, Jorge and Batista, Thais and Cavalcante, Everton and Cacho, N\'{e}lio and Lopes, Frederico and Almeida, Andr\'{e}},
	booktitle = {Proceedings of the 24th Brazilian Symposium on Multimedia and the Web},
	doi = {10.1145/3243082.3264671},
	isbn = {9781450358675},
	keywords = {Smart cities, data integration, data middleware, geographic information systems},
	location = {Salvador, BA, Brazil},
	numpages = {4},
	pages = {411–414},
	publisher = {Association for Computing Machinery},
	series = {WebMedia '18},
	title = {A Geographic-Layered Data Middleware for Smart Cities},
	url = {https://doi.org/10.1145/3243082.3264671},
	year = {2018},
}

@inproceedings{10.1145/3265007.3265017,
	abstract = {This paper deals with a generation method of TINs from regular rectangular dissections for an effective and efficient visualization of DEM data. The method consists of two steps. The first step consists of the application of a known resolution reduction method based on octgrids for regular rectangular dissections. The second step consists of the application of a known triangulation method to those resolution reduced rectangular dissections. TINs derived from octgrids are called oct-TINs. Effectiveness and efficiency of oct-TINs generated by the above method are also shown by examples.},
	address = {New York, NY, USA},
	author = {Suzuki, Masanori and Anzai, Koushi and Goto, Takaaki and Miyadera, Youzou and Yaku, Takeo},
	booktitle = {Proceedings of the 6th ACM/ACIS International Conference on Applied Computing and Information Technology},
	doi = {10.1145/3265007.3265017},
	isbn = {9781450365741},
	keywords = {terrain maps, rectangular dissections, octgrids, Visualization, TINs},
	location = {Kunming, China},
	numpages = {3},
	pages = {32–34},
	publisher = {Association for Computing Machinery},
	series = {ACIT '18},
	title = {Oct-TINs: A Data Structure of Triangular Irregular Networks for Terrain Map Visualization},
	url = {https://doi.org/10.1145/3265007.3265017},
	year = {2018},
}

@inproceedings{10.1145/3265007.3265018,
	abstract = {This paper deals with triangular irregular networks generated from rectangular dissections. This paper introduces a steepest descent valley line detection method and a steepest ascent ridge line detection method for terrain maps represented by triangular irregular networks,},
	address = {New York, NY, USA},
	author = {Goto, Takaaki and Suzuki, Masanori and Tsuchida, Kensei and Yaku, Takeo},
	booktitle = {Proceedings of the 6th ACM/ACIS International Conference on Applied Computing and Information Technology},
	doi = {10.1145/3265007.3265018},
	isbn = {9781450365741},
	keywords = {triangular irregular networks, terrain map, steepest descent, steepest ascent, ridge detection, Valley detection},
	location = {Kunming, China},
	numpages = {3},
	pages = {35–37},
	publisher = {Association for Computing Machinery},
	series = {ACIT '18},
	title = {Feature Extraction of Triangular Irregular Networks with the Drop of Water Principle},
	url = {https://doi.org/10.1145/3265007.3265018},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274899,
	abstract = {Given a utility network and one or more starting points that define where analysis should begin, the problem of analyzing utility networks entails assembling a subset of network elements that meet some specified criteria. Analyzing utility network data has several applications and provides tremendous business value to utilities. For example, analysis may answer questions about the current state of the network (e.g., what valves need to be closed to shut off water flow to a location of a pipe leak), help to design future facilities (e.g., how many houses are fed by a transformer and can the transformer supply another house without overloading its capacity?), and help to organize business practices (e.g., create circuit maps for work crews to facilitate damage assessment after an ice storm). Analyzing utility networks is a challenging problem due to 1) the size of the data, which could have many tens of millions of network elements per utility, and billions of elements at the nationwide or continental scale, 2) modeling and analyzing utility assets at high fidelity (level of detail), and 3) the different analysis requirements across utility domains (e.g., water, wastewater, sewer, district heating, gas, electric, fiber, and telecom). This paper describes the trace framework for utility network analysis that has been implemented in ArcGIS Pro 2.1/ArcGIS Enterprise 10.6. The trace framework features algorithms in a services-based architecture for addressing analysis tasks across a wide array of utility domains. Previous approaches have focused on solving specific problems in specific domains whereas the trace framework provides a more general, scalable solution. We present experiments that demonstrate the scalability of the trace framework and a case study that highlights its value in performing a wide variety of analytics on utility networks.},
	address = {New York, NY, USA},
	author = {Oliver, Dev and Hoel, Erik G.},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274899},
	isbn = {9781450358897},
	keywords = {GIS, graph algorithms, graphs and networks, spatial databases, utility networks},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {249–258},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {A trace framework for analyzing utility networks: a summary of results (industrial paper)},
	url = {https://doi.org/10.1145/3274895.3274899},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274903,
	abstract = {Shortest path computation is a fundamental problem in road networks. However, in many real-world scenarios, determining solely the shortest path is not enough. In this paper, we study the problem of finding k-Dissimilar Paths with Minimum Collective Length (kDPwML), which aims at computing a set of paths from a source s to a target t such that all paths are pairwise dissimilar by at least θ and the sum of the path lengths is minimal. We introduce an exact algorithm for the kDPwML problem, which iterates over all possible s - t paths while employing two pruning techniques to reduce the prohibitively expensive computational cost. To achieve scalability we also define the much smaller set of the simple single-via paths, and we adapt two algorithms for kDPwML queries to iterate over this set. Our experimental analysis on real road networks shows that iterating over all paths is impractical, while iterating over the set of simple single-via paths can lead to scalable solutions with only a small trade-off in the quality of the results.},
	address = {New York, NY, USA},
	author = {Chondrogiannis, Theodoros and Bouros, Panagiotis and Gamper, Johann and Leser, Ulf and Blumenthal, David B.},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274903},
	isbn = {9781450358897},
	keywords = {alternative routing, path similarity, route planning},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {404–407},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Finding k-dissimilar paths with minimum collective length},
	url = {https://doi.org/10.1145/3274895.3274903},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274910,
	abstract = {Geographic information systems (GIS) customarily encode spatial information using geometric objects (points, polylines and polygons) and their locations. But people frequently use qualitative relations, such as topological relations (e.g., connection or overlap) or cardinal direction relations (e.g. North or Southeast), to describe spatial scenes. While topological relations have been integrated into modern GIS, direction relations have remained isolated from GIS and are not available for user interaction. Instead, a user must visually infer them from map depictions.This work uses the problem of generating and interpreting cardinal direction labels that describe the direction between a region and its surrounding neighbors (e.g., all neighbors of a US state) to identify principles for computing more qualitative descriptions of directions that are intuitive to people and to correctly interpret descriptions commonly used by people. This is a step towards bridging the qualitative-quantitative divide between spatial information systems and human conceptualizations of space.},
	address = {New York, NY, USA},
	author = {Kritzman, Gregory and Hahmann, Torsten},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274910},
	isbn = {9781450358897},
	keywords = {cardinal directions, qualitative spatial reasoning, spatial relations},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {412–415},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Using commonsensical cardinal directions to describe bordering objects},
	url = {https://doi.org/10.1145/3274895.3274910},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274911,
	abstract = {With the widespread installation of location-enabled devices on public transportation, public vehicles are generating massive amounts of trajectory data in real time. However, using these trajectory data for meaningful analysis requires careful considerations in storing, managing, processing, and visualizing the data. Using the location data of the Los Angeles Metro bus system, along with publicly available bus schedule data, we conduct a data processing and analyses study to measure the performance of the public transportation system in Los Angeles utilizing a number of metrics including travel-time reliability, on-time performance, bus bunching, and travel-time estimation. We demonstrate the visualization of the data analysis results through an interactive web-based application. The developed algorithms and system provide powerful tools to detect issues and improve the efficiency of public transportation systems.},
	address = {New York, NY, USA},
	author = {Nguyen, Kien and Yang, Jingyun and Lin, Yijun and Lin, Jianfa and Chiang, Yao-Yi and Shahabi, Cyrus},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274911},
	isbn = {9781450358897},
	keywords = {GPS bus, gps trajectory, map matching, public transportation},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {560–563},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Los angeles metro bus data analysis using GPS trajectory and schedule data (demo paper)},
	url = {https://doi.org/10.1145/3274895.3274911},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274913,
	abstract = {There are many real world applications that require to identify movement of users such as identifying movement corridors, most popular paths, and nearest neighbours. If one is not given trajectories mapping to movement of people but rather sporadic location data, such as location based social network data, finding movement related information becomes difficult. Rather than processing all points in a data set given a query, a clever approach is to construct a graph, based on user locations, and query this graph for all queries. One example is the shortest path graph. However the shortest path graph can be inefficient and ineffective analysing movement, as it calculates the graph considering all points in a data set. We propose the stepping stone graph, which calculates graph considering point pairs rather than all points, that focuses on local possible movement, making it both efficient and effective for location based social network related queries. We demonstrate its uses by applying it in the aforementioned domain and comparing with the shortest path graph. We also compare its properties to a range of other graphs.},
	address = {New York, NY, USA},
	author = {Kannangara, Sameera and Tanin, Egemen and Harwood, Aaron and Karunasekera, Shanika},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274913},
	isbn = {9781450358897},
	keywords = {graphs, moving objects, shortest path},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {149–158},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Stepping stone graph for public movement analysis},
	url = {https://doi.org/10.1145/3274895.3274913},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274916,
	abstract = {Understanding link travel times (LTT) has received significant attention in transportation and spatial computing literature but they often remain behind closed doors, primarily because the data used for capturing them is considered confidential. Consequently, free and open maps such as OpenStreetMap (OSM) or TIGER, while being remarkably accurate in capturing geometry and topology of the road network are oblivious to actual travel times. Without LTTs computing the optimal routes or estimated time of arrival is challenging and prone to substantial errors. In this work we set to enrich the underlying map information with LTT by using a most basic data about urban trajectories, which also becomes increasingly available for public use: set of origin/destination location/timestamp pairs. Our system, W-edge utilizes such basic trip information to calculate LTT to each individual road segment, effectively assigning a weight to individual edges of the underlying road network. We demonstrate that using appropriately trained edge weights, the errors in estimating travel times are up to 60\% lower than the errors observed in OSRM or GraphHopper, two prominent OSM-based, traffic-oblivious, routing engines.},
	address = {New York, NY, USA},
	author = {Stanojevic, Rade and Abbar, Sofiane and Mokbel, Mohamed},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274916},
	isbn = {9781450358897},
	keywords = {link travel times, maps, ridge regression},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {424–427},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {W-edge: weighing the edges of the road network},
	url = {https://doi.org/10.1145/3274895.3274916},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274918,
	abstract = {Recently, location service using Bluetooth Low-Energy (BLE) beacon is gaining popularity. There also exist researches that estimate the route of the user from the location estimation results, visualize, and analyze it. In the conventional route estimation method based on the BLE beacon, after estimating the all locations from the radio field strength of the BLE, the route is estimated from the sequence of the estimated locations. Therefore, one of the causes of deterioration in accuracy could be the fact that the estimation was repeated twice. Therefore, in this paper, we propose a novel estimation method of the user's route and stay using BLE beacons. Specifically, we apply the idea of global map matching, used in the field of GPS, to route estimation based on BLE beacons. Thus, the global optimum route in beacon network can be estimated from the radio field intensities of the BLE beacon directly. We associate a beacon network with path network to estimate the route and stay of users. We installed approximately 1,600 BLE beacon transmitters in all the classrooms and corridors of Nagoya Institute of Technology, and confirmed the effectiveness of the proposed method by experiments.},
	address = {New York, NY, USA},
	author = {Yamamoto, Daisuke and Tanaka, Ryosuke and Kajioka, Shinsuke and Matsuo, Hiroshi and Takahashi, Naohisa},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274918},
	isbn = {9781450358897},
	keywords = {BLE beacon, indoor navigation, localization, map-matching, trajectory},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {309–318},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Global map matching using BLE beacons for indoor route and stay estimation},
	url = {https://doi.org/10.1145/3274895.3274918},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274919,
	abstract = {We present a novel algorithm to match GPS trajectories onto maps offline (in batch mode) using techniques borrowed from the field of force-directed graph drawing. We consider a simulated physical system where each GPS trajectory is attracted or repelled by the underlying road network via electrical-like forces. We let the system evolve under the action of these physical forces such that individual trajectories are attracted towards candidate roads to obtain a map matching path. Our approach has several advantages compared to traditional, routing-based, algorithms for map matching, including the ability to account for noise and to avoid large detours due to outliers in the data whilst taking into account the underlying topological restrictions (such as one-way roads). Our empirical evaluation using real GPS traces shows that our method produces better map matching results compared to alternative offline map matching algorithms on average, especially for routes in dense, urban areas.},
	address = {New York, NY, USA},
	author = {Rappos, Efstratios and Robert, Stephan and Cudr\'{e}-Mauroux, Philippe},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274919},
	isbn = {9781450358897},
	keywords = {GPS trajectory, force-directed algorithms, map matching, offline routing, road map},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {319–328},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {A force-directed approach for offline GPS trajectory map matching},
	url = {https://doi.org/10.1145/3274895.3274919},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274920,
	abstract = {Location-based social media (e.g., Twitter, Foursquare) have been generating massive amount of geo-textual data. In this paper, we represent the spatial distribution of a keyword by the group of locations tagged with such keyword. Given a query keyword, our problem is to find k keywords with the most similar distribution of locations. Such query finds applications in targeted marketing and recommendation. The performance of existing solutions degrade when different point groups have significant overlapping, which happens rather frequently in real data. We propose efficient techniques to process similarity search on point groups. Experimental results on Twitter data demonstrate that our solution is faster than the state-of-the-art by up to 6 times.},
	address = {New York, NY, USA},
	author = {Li, Zhe and Li, Yu and Yiu, Man Lung},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274920},
	isbn = {9781450358897},
	keywords = {hausdorff distance, similarity searching, spatio-textual searching},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {109–118},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Fast similarity search on keyword-induced point groups},
	url = {https://doi.org/10.1145/3274895.3274920},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274922,
	abstract = {In many instances of the object tracking problem the topological properties of objects can change over time. Such changes include the splitting of an object into multiple objects or merging of multiple objects into a single object. We propose a novel tracking model which is robust to such changes. This model is formulated terms of homology theory whereby 0-dimensional homology classes, which correspond to path-connected components, are tracked. A generalisation of this model for tracking spatially close objects lying in an ambient metric space is also proposed. This generalisation is particularly suitable for tracking spatial-temporal phenomena such as weather phenomena. The utility of the proposed model is demonstrated with respect to tracking rain clouds in radar imagery.},
	address = {New York, NY, USA},
	author = {Corcoran, Padraig and Jones, Christopher B.},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274922},
	isbn = {9781450358897},
	keywords = {spatial-temporal, topology, tracking},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {428–431},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Robust tracking of objects with dynamic topology},
	url = {https://doi.org/10.1145/3274895.3274922},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274923,
	abstract = {In recent years several extensions of Hadoop system have been proposed for dealing with spatial data and SpatialHadoop belongs to this group. In the MapReduce paradigm a task can be parallelized by partitioning data into chunks and performing the same operation on them, eventually combining the partial results at the end. Thus, the applied partitioning technique can tremendously affect the performance of a parallel execution, since it is the key point for obtaining balanced map tasks. However, when skewed distributed datasets are considered, using a regular grid might not be the right choice and other techniques have to be applied, which in turn are more expensive to build. This paper illustrates an approach for detecting the degree of skewness of a spatial dataset, based on the box counting function. Moreover, given the degree of skewness and some experimental observations, a heuristic is sketched in order to decide which partitioning technique to apply in order to improve as much as possible the performance of subsequent operations.},
	address = {New York, NY, USA},
	author = {Belussi, Alberto and Migliorini, Sara and Eldawy, Ahmed},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274923},
	isbn = {9781450358897},
	keywords = {BigData, MapReduce, SpatialHadoop, partitioning, skewed data},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {432–435},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Detecting skewness of big spatial data in SpatialHadoop},
	url = {https://doi.org/10.1145/3274895.3274923},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274929,
	abstract = {Increased availability of large amounts of address data provides opportunities for data driven studies to improve decision making in business applications and support precision public health with high resolution geolocations. Geocoding large number of addresses is challenging due to high cost and often disclosure of sensitive data to vendors over the Web. Most geocoders take advantage of Web APIs which require sending private addresses over the Internet, which may not be an option for many applications with sensitive data including public health and geo-medicine. Meanwhile, the cost for geocoding massive number of addresses could be high and becomes a major hurdle for many users. To overcome these challenges, we developed an open source on-premise geocoding software EaserGeocoder, which uses a novel integrative geocoding model to achieve high accuracy through integrating multiple open data sources. EaserGeocoder takes advantage of machine learning based approaches to determine best answers from multiple data sources. EaserGeocoder can also be easily parallelized to achieve high scalability through parallelized search and distributed computing. EaserGeocoder is on a par with commercial geocoding systems, outperforms open source systems, and is available for free.},
	address = {New York, NY, USA},
	author = {Rashidian, Sina and Dong, Xinyu and Jain, Shubham Kumar and Wang, Fusheng},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274929},
	isbn = {9781450358897},
	keywords = {geocoding, geographic information system, text searching},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {572–575},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {EaserGeocoder: integrative geocoding with machine learning (demo paper)},
	url = {https://doi.org/10.1145/3274895.3274929},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274933,
	abstract = {Given a spatial road network, an origin, a destination, and trajectory data of vehicles on the network, the Energy-efficient Path Selection (EPS) problem aims to find the most energy-efficient path (i.e., with least energy consumption) between the origin and the destination. With world energy consumption growing rapidly, estimating and reducing the energy consumption of road transportation is becoming critical. The main challenge of this problem is to adopt energy consumption as the cost metric of paths, which is neglected by the related work in shortest path selection problem whose typical metrics are distance and time. Additionally, negative energy consumption caused by the use of regenerative braking on electrified vehicles prevents classical algorithms like Dijkstra's algorithm from functioning correctly. We introduce a Physics-guided Energy Consumption (PEC) model based on a low-order physics model, which estimates energy consumption as a function of the vehicle parameters (e.g., mass and powertrain system efficiency) and use the estimation in the proposed adaptive dynamic programming algorithm for path selection. Our PEC model treats energy consumption as a unique metric that is determined not only by the path and vehicle's motion along the path, but also on properties of the vehicle itself. Experiments show that the PEC model estimates are more similar to real trajectory data than the estimates represented by the mean or histogram of historical data. Also, the path found by the proposed method is more energy-efficient than both the currently used path and the fastest path found by a commercial routing package. As far as we know, this is the first paper to use a physics-guided method to estimate the vehicle energy consumption and perform path selection.},
	address = {New York, NY, USA},
	author = {Li, Yan and Shekhar, Shashi and Wang, Pengyue and Northrop, William},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274933},
	isbn = {9781450358897},
	keywords = {energy efficiency, physics-aware, routing, shortest path},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {99–108},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Physics-guided energy-efficient path selection: a summary of results},
	url = {https://doi.org/10.1145/3274895.3274933},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274934,
	abstract = {We propose a neural network component, the regional aggregation layer, that makes it possible to train a pixel-level density estimator using only coarse-grained density aggregates, which reflect the number of objects in an image region. Our approach is simple to use and does not require domain-specific assumptions about the nature of the density function. We evaluate our approach on several synthetic datasets. In addition, we use this approach to learn to estimate high-resolution population and housing density from satellite imagery. In all cases, we find that our approach results in better density estimates than a commonly used baseline. We also show how our housing density estimator can be used to classify buildings as residential or non-residential.},
	address = {New York, NY, USA},
	author = {Jacobs, Nathan and Kraft, Adam and Rafique, Muhammad Usman and Sharma, Ranti Dev},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274934},
	isbn = {9781450358897},
	keywords = {dasymetric mapping, population density, remote sensing},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {33–42},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {A weakly supervised approach for estimating spatial density functions from high-resolution satellite imagery},
	url = {https://doi.org/10.1145/3274895.3274934},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274937,
	abstract = {Linear features on terrains model the boundaries of ground cover regions, delineate glaciers, or form the boundary of rivers and lakes. When computing the similarity between such linear features, it is important to also take their context into account: the terrain. We hence explore the possibilities of volume-based distance measures for linear features on a terrain. Our measures construct suitable base surfaces between the linear features, which can slice through the input terrain and also hover above. The similarity between two linear features is then captured by the volume of "earth" above the base surface and below the terrain, and possibly also by the volume of "air" below the base surface and above the terrain. We suggest six ways of choosing a suitable base surface. These choices give rise to different measured volumes and can be useful in different application scenarios.},
	address = {New York, NY, USA},
	author = {Sonke, Willem and van Kreveld, Marc and Ophelders, Tim and Speckmann, Bettina and Verbeek, Kevin},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274937},
	isbn = {9781450358897},
	keywords = {context, similarity measure, terrain model, volume},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {444–447},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Volume-based similarity of linear features on terrains},
	url = {https://doi.org/10.1145/3274895.3274937},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274945,
	abstract = {Accurate spatio-temporal information is crucial for smart city applications such as modern routing algorithms. Often, this information describes the state of stationary resources, e.g. the availability of parking bays, charging stations or the amount of people waiting for a vehicle to pick them up near a given location. Predicting future states of the monitored resources is often mandatory because a resource might change its state within the time until it is needed. It is often not possible to obtain complete history of a resource's state. For example, the information might be collected from traveling agents visiting the resource with an irregular frequency. Thus, it is necessary to develop methods which work on sparse observations for training and prediction. In this paper, we propose time-inhomogeneous discrete Markov models to allow accurate prediction even when the frequency of observation is very rare. Our new model is able to blend recent observations with historic data and also provide useful probabilistic estimates for future states. Since resource availability in a city is typically time-dependent, our Markov model is time-inhomogeneous and cyclic within a predefined time interval. We propose a modified Baum-Welch algorithm capable of training our model with sparse data. Evaluations on real-world datasets of parking bay availability show that our new method indeed yields good results compared to methods designed for training on complete data and non-cyclic variants.},
	address = {New York, NY, USA},
	author = {Rottkamp, Lukas and Schubert, Matthias},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274945},
	isbn = {9781450358897},
	keywords = {predictive models, smart city data, spatial resources},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {460–463},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {A time-inhomogeneous Markov model for resource availability under sparse observations},
	url = {https://doi.org/10.1145/3274895.3274945},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274952,
	abstract = {Analytic methods can be difficult to build and costly to train for mobility data. We show that information about the topology of the space and how mobile objects navigate the obstacles can be used to extract insights about mobility at larger distance scales. The main contribution of this paper is a topological signature that maps each trajectory to a relatively low dimensional Euclidean space, so that now they are amenable to standard analytic techniques. Data mining tasks: nearest neighbor search with locality sensitive hashing, clustering, regression, etc., work more efficiently in this signature space. We define the problem of mobility prediction at different distance scales, and show that with the signatures simple k nearest neighbor based regression perform accurate prediction. Experiments on multiple real datasets show that the framework using topological signatures is accurate on all tasks, and substantially more efficient than machine learning applied to raw data. Theoretical results show that the signatures contain enough topological information to reconstruct non-self-intersecting trajectories upto homotopy type. The construction of signatures is based on a differential form that can be generated in a distributed setting using local communication, and a signature can be locally and inexpensively updated and communicated by a mobile agent.},
	address = {New York, NY, USA},
	author = {Ghosh, Abhirup and Rozemberczki, Benedek and Ramamoorthy, Subramanian and Sarkar, Rik},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274952},
	isbn = {9781450358897},
	keywords = {clustering, computational topology, differential forms, mobility analysis, near neighbor search, sketches, streaming, trajectory analysis},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {159–168},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Topological signatures for fast mobility analysis},
	url = {https://doi.org/10.1145/3274895.3274952},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274957,
	abstract = {We investigate the following map-matching problem: given a sequence of stations taken by a public transit vehicle and given the underlying network, find the most likely geographical course taken by that vehicle. We provide a new algorithm and tool, which is based on a hidden Markov model and takes characteristics of transit networks into account. Our tool can be useful for the visualization of transit lines in map services, for transit data providers, and for an on-line matching of live passenger GPS data to a public transit vehicle. We evaluate our tool on real-world data, and compare it against two baselines. The shapes produced by our tool are very close to the true shapes. We have made our software publicly available, enabling full reproducibility of our results.},
	address = {New York, NY, USA},
	author = {Bast, Hannah and Brosi, Patrick},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274957},
	isbn = {9781450358897},
	keywords = {GTFS, map-matching, public transit, schedule data},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {480–483},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Sparse map-matching in public transit networks with turn restrictions},
	url = {https://doi.org/10.1145/3274895.3274957},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274959,
	abstract = {We introduce an efficient algorithm for drawing spatially-informative linear cartograms: transforming a geometric network such that given edge lengths are realised, while distorting edge directions as little as possible. Our algorithm is based on carefully linearised least squares optimisation, forgoing the need for an iterative solver. This is fast and ensures a well-defined result. The classic application of linear cartograms is drawing travel-time maps; we also discuss drawing schematised metro maps.},
	address = {New York, NY, USA},
	author = {van Dijk, Thomas C. and Lutz, Dieter},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274959},
	isbn = {9781450358897},
	keywords = {cartograms, least squares optimization, metro maps},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {488–491},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Realtime linear cartograms and metro maps},
	url = {https://doi.org/10.1145/3274895.3274959},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274960,
	abstract = {Given two geometric spaces, a set of matched points between two geometric spaces, the Coverage Constrained Spatial Co-clustering (CCSCO) problem produces k co-clusters that honor the coverage constraint and minimize the total distances of the spatial points to their cluster center. The CCSCO problem is important for many societal applications, such as design of evacuation routes and resource allocation. The problem is NP-hard; it is computationally challenging because of the large size of spatial points and the coverage constraint. This paper proposes a novel approach, called Bipartite Space Shrinking (BSS), for finding k clusters that minimize the total distances of the points to their cluster center under the coverage constraint. To improve the performance, we introduce the Distance Map data structure to efficiently construct a CCSCO. Experiments using real-world New York City Taxi Trip datasets demonstrate that the proposed algorithm significantly reduces the computational cost to create a CCSCO.},
	address = {New York, NY, USA},
	author = {Ohriniuc, Roxana and Reich, Aaron and Yang, KwangSoo},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274960},
	isbn = {9781450358897},
	keywords = {constrained optimization, spatial Co-clustering, spatio-temporal data analysis},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {492–495},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Coverage constrained spatial Co-clustering},
	url = {https://doi.org/10.1145/3274895.3274960},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274962,
	abstract = {Given a geometric space and a set of weighted spatial points, the Size Constrained k Simple Polygons (SCSP) problem identifies k simple polygons that maximize the total weights of the spatial points covered by the polygons and honor the polygon size constraint. The SCSP problem is important for many societal applications, such as hotspot area detection and resource allocation. The problem is NP-hard; it is computationally challenging because of the large number of spatial points and the polygon size constraint. This paper proposes a novel approach for finding k simple polygons that maximize the total weights under the size constraint. Experiments using Chicago crime datasets demonstrate that the proposed algorithm outperforms baseline approaches and reduces the computational cost to create a SCSP.},
	address = {New York, NY, USA},
	author = {Reich, Aaron and Ohriniuc, Roxana and Yang, KwangSoo},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274962},
	isbn = {9781450358897},
	keywords = {constrained optimization, polygonalization, spatial covering},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {500–503},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Size constrained k simple polygons},
	url = {https://doi.org/10.1145/3274895.3274962},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274964,
	abstract = {Predicting behaviors of a population from location-oriented log data from smartphones, i.e., urban population dynamics, has become more common in mobile and pervasive computing. A bilinear representation approach has been proposed to improve the prediction accuracy of urban population dynamics by adding contexts such as geographical information and day of the week. However, this approach has a strong limitation in that additional contexts can not be directly utilized in this representation with a unified manner. To resolve this issue, we propose a new predictive model for urban population dynamics based on multilinear Poisson regression so as to handle multiple contexts in a systematic manner. The model is parameterized using a tensor and can be optimized by using an efficient convex optimization with a sequence of matrix parameter optimizations. An empirical evaluation with large-scale smartphone location data showed that our model outperforms conventional approaches.},
	address = {New York, NY, USA},
	author = {Shimosaka, Masamichi and Tsukiji, Takeshi and Wada, Hideyuki and Tsubouchi, Kota},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274964},
	isbn = {9781450358897},
	keywords = {GPS logs, poisson regression, tensor-based model},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {504–507},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Predictive population behavior analysis from multiple contexts with multilinear poisson regression},
	url = {https://doi.org/10.1145/3274895.3274964},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274965,
	abstract = {Given a set of geospatial objects, the Best Region Search problem finds the optimal placement of a fixed-size rectangle so that the value of a user-defined utility function over the enclosed objects is maximized. The existing algorithm for this problem computes only the top result. However, this is often quite restrictive in practice and falls short in providing sufficient insight about the dataset. In this paper, we introduce the k-BRS problem, and we present a method for efficiently and progressively computing the next best result for any number k of results requested by the user. We show that our approach can accommodate additional constraints. In particular, we consider the requirement of computing the next best rectangle that has no or little overlap with the already retrieved ones, which reduces the repetition and redundancy in the results presented to the user. Our experimental evaluation demonstrates that our algorithms are efficient and scalable to large real-world datasets.},
	address = {New York, NY, USA},
	author = {Skoutas, Dimitrios and Sacharidis, Dimitris and Patroumpas, Kostas},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274965},
	isbn = {9781450358897},
	keywords = {areas of interest, best region search, spatial analytics},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {299–308},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Efficient progressive and diversified top-k best region search},
	url = {https://doi.org/10.1145/3274895.3274965},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274966,
	abstract = {The paper proposes Riso-Tree, a generic indexing framework for geographic knowledge graphs. Riso-Tree enables fast execution of graph queries that involve spatial predicates (aka. GraSp). The proposed framework augments the classic R-Tree structure with pre-materialized sub-graph entries. Riso-Tree first partitions the graph into sub-graphs based on their connectivity to the spatial sub-regions. The proposed index allows for fast execution of GraSp queries by efficiently pruning the traversed vertexes/edges based upon the materialized sub-graph information. The experiments show that the proposed Riso-Tree achieves up to two orders magnitude faster execution time than its counterparts when executing GraSp queries on real knowledge graphs (e.g., WikiData).},
	address = {New York, NY, USA},
	author = {Sun, Yuhan and Sarwat, Mohamed},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274966},
	isbn = {9781450358897},
	keywords = {geospatial knowledge graph, range query, spatial index},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {289–298},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {A generic database indexing framework for large-scale geographic knowledge graphs},
	url = {https://doi.org/10.1145/3274895.3274966},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274968,
	abstract = {In the near future, more and more machines will perform tasks in the vicinity of human spaces or support them directly in their spatially bound activities. In order to simplify the verbal communication and the interaction between robotic units and/or humans, reliable and robust systems w.r.t. noise and processing results are needed. This work builds a foundation to address this task. By using a continuous representation of spatial perception in interiors learned from trajectory data, our approach clusters movement in dependency to its spatial context. We propose an unsupervised learning approach based on a neural autoencoding that learns semantically meaningful continuous encodings of spatio-temporal trajectory data. This learned encoding can be used to form prototypical representations. We present promising results that clear the path for future applications.},
	address = {New York, NY, USA},
	author = {Feld, Sebastian and Illium, Steffen and Sedlmeier, Andreas and Belzner, Lenz},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274968},
	isbn = {9781450358897},
	keywords = {artificial neural networks, auto-encoder, geospatial trajectories, indoor navigation, isovist analysis, spatial syntax},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {329–338},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Trajectory annotation using sequences of spatial perception},
	url = {https://doi.org/10.1145/3274895.3274968},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274969,
	abstract = {This paper investigates conditional generative adversarial networks (cGANs) to overcome a fundamental limitation of using geotagged media for geographic discovery, namely its sparse and uneven spatial distribution. We train a cGAN to generate ground-level views of a location given overhead imagery. We show the "fake" ground-level images are natural looking and are structurally similar to the real images. More significantly, we show the generated images are representative of the locations and that the representations learned by the cGANs are informative. In particular, we show that dense feature maps generated using our framework are more effective for land-cover classification than approaches which spatially interpolate features extracted from sparse ground-level images. To our knowledge, ours is the first work to use cGANs to generate ground-level views given overhead imagery in order to explore the benefits of the learned representations.},
	address = {New York, NY, USA},
	author = {Deng, Xueqing and Zhu, Yi and Newsam, Shawn},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274969},
	isbn = {9781450358897},
	keywords = {computer vision, generative adversarial networks, geotagged social media, land-cover classification},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {43–52},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {What is it like down there? generating dense ground-level views and image features from overhead imagery using conditional generative adversarial networks},
	url = {https://doi.org/10.1145/3274895.3274969},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274971,
	abstract = {Locating spatial data sources for a specific area of interest (AOI) is a difficult task, because of constantly updating server locations (URLs), specific layer location changes within that server location, and the relative unpopularity of a specific spatial data source. The proposed system, named Pine attempts to remedy the issue using automatic, yet user-led discovery of data sources by utilizing a web plug-in that discovers spatial data sources as users browse the web. The web plug-in has a two-fold function, (1) immediately presenting the spatial data source and all of the layers contained within that server discovered by the plug-in, (2) sending those layers and servers into a centralized, search-able repository that is accessible via the web. By utilizing the users browsing habits, and their machines as the discoverers of spatial data, Pine avoids the very high computational overhead of discovery of spatial data sources from a central server using a web crawling approach. As with any user-contributed data set, Pine must get users to actually contribute to the central repository, and it achieves this by including functionality that is useful to the user on its own by displaying a search-able list in the web plug-in of spatial data sources that the user has discovered themselves, a previously difficult task. The system implements a push based approach for discovering data sources, as the plug-in sends the data directly to the central repository, it is always up to date with the newest data sources discovered by any of the many instances of the web plug-in running on users computers},
	address = {New York, NY, USA},
	author = {Haynes, Myles and Hendawi, Abdeltawab and Ali, Mohamed},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274971},
	isbn = {9781450358897},
	keywords = {Google Chrome, crowdsourcing, environmental systems research institute, geographic information systems},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {592–595},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Pine: a system for crowdsourced spatial data source discovery while map browsing},
	url = {https://doi.org/10.1145/3274895.3274971},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274974,
	abstract = {Current approaches to construct road network maps from GPS trajectories suffer from low precision, especially in dense urban areas and in regions with complex topologies such as overpasses and underpasses, parallel roads, and stacked roads. This paper proposes a two-stage method to improve precision without sacrificing recall (coverage). The first stage, RoadRunner, is a method that can generate high-precision maps even in challenging scenarios by incrementally following the flow of trajectories, using the connectivity between observations in each trajectory to decide whether overlapping trajectories are traversing the same road or distinct parallel roads, and to correctly infer road segment connectivity. By itself, RoadRunner is not designed to achieve high recall, but we show how to combine it with a wide range of prior schemes, some that use GPS trajectories and some that use aerial imagery, to achieve recall similar to prior schemes but at substantially higher precision. We evaluated RoadRunner in four U.S. cities using 60,000 GPS trajectories, and found that precision improves by 5.2 points (a 33.6\% error rate reduction) and 24.3 points (a 60.7\% error rate reduction) over two existing schemes, with a slight increase in recall.},
	address = {New York, NY, USA},
	author = {He, Songtao and Bastani, Favyen and Abbar, Sofiane and Alizadeh, Mohammad and Balakrishnan, Hari and Chawla, Sanjay and Madden, Sam},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274974},
	isbn = {9781450358897},
	keywords = {GPS, map inference, road network, spatial data, trajectory},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {3–12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {RoadRunner: improving the precision of road network inference from GPS trajectories},
	url = {https://doi.org/10.1145/3274895.3274974},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274975,
	abstract = {One can infer from the broken window theory that the perception of a city street's safety level relies significantly on the visual appearance of the street. Previous works have addressed the feasibility of using computer vision algorithms to classify urban scenes. Most of the existing urban perception predictions focus on binary outcomes such as safe or dangerous, wealthy or poor. However, binary predictions are not representative and cannot provide informative inferences such as the potential crime types in certain areas. In this paper, we explore the connection between urban perception and crime inferences. We propose a convolutional neural network (CNN) - StreetNet to learn crime rankings from street view images. The learning process is formulated on the basis of preference learning and label ranking settings. We design a street view images retrieval algorithm to improve the representation of urban perception. A data-driven, spatiotemporal algorithm is proposed to find unbiased label mappings between the street view images and the crime ranking records. Extensive evaluations conducted on images from different cities and comparisons with baselines demonstrate the effectiveness of our proposed method.},
	address = {New York, NY, USA},
	author = {Fu, Kaiqun and Chen, Zhiqian and Lu, Chang-Tien},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274975},
	isbn = {9781450358897},
	keywords = {convolutional neural networks, preference learning, spatial analysis, street view},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {269–278},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {StreetNet: preference learning with convolutional neural network on urban crime perception},
	url = {https://doi.org/10.1145/3274895.3274975},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274980,
	abstract = {An important problem in terrain analysis is modeling how water flows across a terrain and creates floods by filling up depressions. In this paper we study a number of flood-risk related problems: Given a terrain Σ, represented as a triangulated xy-monotone surface with n vertices, a rain distribution R and a volume of rain ψ, determine which portions of Σ are flooded. We develop efficient algorithms for flood-risk analysis under the multiflow-directions (MFD) model, in which water at a point can flow along multiple downslope edges to more accurately represent flooding events.We present three main results: First, we present an O(nm)-time algorithm to answer a terrain-flood query: if it rains a volume ψ according to a rain distribution R, determine what regions of Σ will be flooded; here m is the number of sinks in Σ. Second, we present a O(n log n)-time algorithm for preprocessing Σ into a linear-size data structure for answering point-flood queries: given a rain distribution R, a volume of rain ψ falling according to R, and point q ∈ Σ, determine whether q will be flooded. A point-flood query can be answered in O(nk) time, where k is the number of maximal depressions in Σ containing the query point q. Alternately, we can preprocess Σ in O(n log n + nm) time into an O(nm)-size data structure so that a point-flood query can be answered in O(|R|k+k2) time, where |R| is the number of vertices in R with positive rain fall. Finally, we present algorithms for answering a flood-time query: given a rain distribution R and a point q ∈ Σ, determine the volume of rain that must fall before q is flooded. Assuming that the product of two k \texttimes{} k matrices can be computed in O(kω) time, we show that a flood-time query can be answered in O(nk + kω) time. We also give an α-approximation algorithm, for α &gt; 1, that runs in O(nk + k2(log(n + logα)ρ))-time, where ρ is a variable on the terrain which depends on the ratio between depression volumes.We implemented our terrain-flooding algorithm and tested its efficacy and efficiency on real terrains},
	address = {New York, NY, USA},
	author = {Lowe, Aaron and Agarwal, Pankaj K.},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274980},
	isbn = {9781450358897},
	keywords = {flood-risk analysis, merge trees, terrains},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {53–62},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Flood-risk analysis on terrains under the multiflow-direction model},
	url = {https://doi.org/10.1145/3274895.3274980},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274981,
	abstract = {We consider a spatial crowdsourcing scenario where (1) a worker is traveling on a preferred/typical path within a road network where (2) there is a set of tasks, each associated with a positive reward, available to be performed and (3) that the worker is willing to possibly deviate from his/her preferred path to perform tasks as long as (4) he/she travels at most a total given distance/time. We name this the In-Route Task Selection (IRTS) problem and investigate it using the skyline paradigm in order to obtain a set of diverse solutions yielding good combinations of detour and reward. Given the NP-hardness of the IRTS problem we present a heuristic approach that produces solutions with good values of precision and recall for problems of realistic sizes within practical query processing time.},
	address = {New York, NY, USA},
	author = {Costa, Camila F. and Nascimento, Mario A.},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274981},
	isbn = {9781450358897},
	keywords = {in-route queries, road networks, skyline, spatial crowdsourcing},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {524–527},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {In-route task selection in crowdsourcing},
	url = {https://doi.org/10.1145/3274895.3274981},
	year = {2018},
}

@inproceedings{10.1145/3274895.3274988,
	abstract = {We demonstrate the rasdaman ("raster data manager") scalable datacube engine in a series of multi-dimensional live scenarios of spatio-temporal datacube analytics, distributed processing in federations, as well as simple, rapid construction of datacubes.},
	address = {New York, NY, USA},
	author = {Baumann, Peter and Misev, Dimitar and Merticariu, Vlad and Huu, Bang Pham and Bell, Brennan},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3274988},
	isbn = {9781450358897},
	keywords = {OGC, WCPS, WCS, coverage, datacube, rasdaman},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {604–607},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {rasdaman: Spatio-temporal datacubes on steroids},
	url = {https://doi.org/10.1145/3274895.3274988},
	year = {2018},
}

@inproceedings{10.1145/3274895.3276475,
	abstract = {This short paper describes W\"{u}pstream, an efficient code for enumerating upstream features in undirected graphs. It uses a linear-time algorithm based on block-cut trees. We describe this algorithm and discuss some performance considerations in the C++ implementation. Code is available at: https://github.com/tcvdijk/wupstream.},
	address = {New York, NY, USA},
	author = {van Dijk, Thomas C. and Greiner, Tobias and den Heijer, Bas and Henning, Nadja and Klesen, Felix and L\"{o}ffler, Andre},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3276475},
	isbn = {9781450358897},
	keywords = {graph algorithms, network analysis, software engineering},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {626–629},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {W\"{u}pstream: efficient enumeration of upstream features (GIS cup)},
	url = {https://doi.org/10.1145/3274895.3276475},
	year = {2018},
}

@inproceedings{10.1145/3274895.3276476,
	abstract = {This paper presents a solution to the problem posed in the ACM SIGSPATIAL GIS Cup 2018, namely to identify all upstream features---nodes and edges existing on simple paths between starting points and controllers---in a given spatial network. Our approach is based primarily on a two-sweep depth-first search which decomposes a graph into its biconnected components prior to collecting the upstream features. Our algorithm runs in linear time in the size of the graph and, in practice, is able to solve large instances with millions of features within seconds on an ordinary personal laptop, identifying features if and only if they are upstream.},
	address = {New York, NY, USA},
	author = {Goldthorpe, Zach and Cannon, Jason and Farebrother, Jesse and Friggstad, Zachary and Nascimento, Mario A.},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3276476},
	isbn = {9781450358897},
	keywords = {GIS, biconnected components, block-cut tree, graph algorithms, upstream features, utility networks},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {630–633},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Using biconnected components for efficient identification of upstream features in large spatial networks (GIS cup)},
	url = {https://doi.org/10.1145/3274895.3276476},
	year = {2018},
}

@inproceedings{10.1145/3274895.3282802,
	abstract = {The last 15 years have seen astonishing progress in the performance of shortest path algorithms for transportation networks. In particular, for road networks, modern algorithms can be up to seven orders of magnitude faster than standard solutions. Since these algorithms enable several new applications, many of them have found their way into navigation services of major technology companies serving hundreds of millions of users every day. This talk highlights key techniques, discusses their impact on the industry, and provides an outlook on upcoming challenges.},
	address = {New York, NY, USA},
	author = {Delling, Daniel},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3282802},
	isbn = {9781450358897},
	keywords = {route planning, transportation},
	location = {Seattle, Washington},
	numpages = {1},
	pages = {2},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {Route planning in transportation networks: from research to practice},
	url = {https://doi.org/10.1145/3274895.3282802},
	year = {2018},
}

@inproceedings{10.1145/3274895.3284782,
	abstract = {HD Maps, one of the key components of automated driving and a life-saving safety feature, serve as the hub for sensing, perception and decision. Making and maintaining a near-real time HD map on a global scale is an extremely challenging task. I will present how we apply AI technologies to automate the creation of HD Live Maps using both industrial capture and crowd-sourced based data collection. Quality Index is introduced to provide automated driving customers with the confidence of HD map accuracy and reliability in a dynamic world. We implement low power and high throughput edge perception as a reference implementation to enable crowd-sourced based HD map maintenance. Finally I will share best practices to democratize AI in our engineering organization and transition research into production.},
	address = {New York, NY, USA},
	author = {Chen, Xin},
	booktitle = {Proceedings of the 26th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3274895.3284782},
	isbn = {9781450358897},
	keywords = {automated driving, mapping, transportation},
	location = {Seattle, Washington},
	numpages = {1},
	pages = {1},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '18},
	title = {HD live maps for automated driving: an AI approach},
	url = {https://doi.org/10.1145/3274895.3284782},
	year = {2018},
}

@inproceedings{10.1145/3276774.3281018,
	abstract = {Energy footprinting has the potential to raise awareness of energy consumption and lead to energy saving behavior. However, current methods for estimating energy consumption cannot provide fine enough temporal or spatial granularity for a reasonable personal energy footprint estimate. In this work, we present a data-driven system design for estimating personal energy footprint in the city in real-time, even in built environments that do not have existing or accessible energy data or population data.},
	address = {New York, NY, USA},
	author = {Wei, Peter and Jiang, Xiaofan},
	booktitle = {Proceedings of the 5th Conference on Systems for Built Environments},
	doi = {10.1145/3276774.3281018},
	isbn = {9781450359511},
	keywords = {geographic information systems, energy footprint, data mining},
	location = {Shenzen, China},
	numpages = {2},
	pages = {194–195},
	publisher = {Association for Computing Machinery},
	series = {BuildSys '18},
	title = {A data-driven system for city-scale personal energy footprint estimations: poster abstract},
	url = {https://doi.org/10.1145/3276774.3281018},
	year = {2018},
}

@inproceedings{10.1145/3277104.3277114,
	abstract = {Currently, vehicle traffic is one of the major problems in Mexico City. Two factors which contribute to this problem are the government transport policies and the disproportionate growth in the number of automobiles. In this paper, we propose a system called Trafico CDMX which analyzes historical data from three geosocial networks Waze, TomTom and Nokia Here to define proper government transport policies to improve the mobility in Mexico City. This system can also identify the major conflict zones where traffic and accidents occur frequently. Trafico CDMX defines a set of web services classified in one of three kinds, visualization, prediction (of speed or number of incidents reports), and chronological animation of accidents or traffic in one or two days (for comparisons). The services provided by this system can be used for other systems, and it can be easily extended to other cities.},
	address = {New York, NY, USA},
	author = {P\'{e}rez-Espinosa, Adriana and Reyes-Cabello, Araceli L. and Quiroz-Fabi\'{a}n, Jos\'{e} Luis and Bravo-Grajales, Emilio},
	booktitle = {Proceedings of the 2018 International Conference on Computing and Big Data},
	doi = {10.1145/3277104.3277114},
	isbn = {9781450365406},
	keywords = {Geo-social networks, Mexico City, Traffic},
	location = {Charleston, SC, USA},
	numpages = {5},
	pages = {13–17},
	publisher = {Association for Computing Machinery},
	series = {ICCBD '18},
	title = {Trafico CDMX System: Using Big Data to improve the Mobility in Mexico City},
	url = {https://doi.org/10.1145/3277104.3277114},
	year = {2018},
}

@inproceedings{10.1145/3281354.3281355,
	abstract = {Geographic information retrieval (GIR) has largely been synonymous with spatial information retrieval. However, geographic information in text is not always explicitly, nor even implicitly, spatial, and when people are seeking geographic information, it is not exlusively for the purpose of spatial analysis or understanding. Here we argue that GIR research is artificially limited by a focus on static spatial representation and we could expand GIR's domain of interest to include organization of information that is non-spatial. Specifically, we highlight three areas worth further study: 1) information about geographic processes and change in text, 2) geographic information where relational context is more important than spatial context, and 3) geographic entities referenced in text that take on thematic (non-spatial) roles.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Adams, Benjamin},
	booktitle = {Proceedings of the 12th Workshop on Geographic Information Retrieval},
	doi = {10.1145/3281354.3281355},
	isbn = {9781450360340},
	keywords = {Geographic information retrieval, Geographic processes, Narrative, Place, Relational networks, Semantic roles},
	location = {Seattle, WA, USA},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {GIR'18},
	title = {From spatial representation to processes, relational networks, and thematic roles in geographic information retrieval},
	url = {https://doi.org/10.1145/3281354.3281355},
	year = {2018},
}

@inproceedings{10.1145/3281354.3281362,
	abstract = {Large amounts of geospatial data have been made available recently on the linked open data cloud and on the portals of many national cartographic agencies (e.g., OpenStreetMap data, administrative geographies of various countries, or land cover/land use data sets). These datasets use various geospatial vocabularies and can be queried using SPARQL or its OGC-standardized extension GeoSPARQL. In this paper we go beyond these approaches to offer a question answering service on top of linked geospatial data sources. Our system has been implemented as re-usable components of the Qanary question answering architecture to provide benefits for future research tasks. We give a detailed description of the architecture of the system, its underlying algorithms and its evaluation using a set of 201 natural language questions.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Punjani, D. and Singh, K. and Both, A. and Koubarakis, M. and Angelidis, I. and Bereta, K. and Beris, T. and Bilidas, D. and Ioannidis, T. and Karalis, N. and Lange, C. and Pantazi, D. and Papaloukas, C. and Stamoulis, G.},
	booktitle = {Proceedings of the 12th Workshop on Geographic Information Retrieval},
	doi = {10.1145/3281354.3281362},
	isbn = {9781450360340},
	keywords = {DBpedia, General Administrative Divisions dataset (GADM), OpenStreetMap, linked geospatial data},
	location = {Seattle, WA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {GIR'18},
	title = {Template-Based Question Answering over Linked Geospatial Data},
	url = {https://doi.org/10.1145/3281354.3281362},
	year = {2018},
}

@proceedings{10.1145/3281548,
	abstract = {In today's era of big data, advanced algorithms, and immense computational power, artificial intelligence (AI) is bringing tremendous opportunities and challenges to geospatial research. Big data enable computers to observe and learn the world from many different perspectives, while high performance machines support the developing, training, and applying of AI models within reasonable amount of time. Recent years have witnessed significant advances in the integration of geography and AI in both academia and industry. There have already been many successful studies. Focusing on modeling the physical nature, research has shown that deep learning can improve the representation of clouds that are smaller than the grid resolutions of climate models. Examining the human society, AI and natural language processing methods, such as word embeddings, are helping quantify changes in stereotypes and attitudes toward women and ethnic minorities over 100 years in the United States. There are also many other applications that effectively integrate AI with problems in geospatial studies, such as vehicle trajectory prediction, indoor navigation, historical map digitizing, gazetteer conflation, geographic feature extraction, geo-ontologies, and place understanding.},
	address = {New York, NY, USA},
	isbn = {9781450360364},
	location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
	publisher = {Association for Computing Machinery},
	title = {GeoAI '18: Proceedings of the 2nd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	year = {2018},
}

@inproceedings{10.1145/3281548.3281550,
	abstract = {Road extraction is a fundamental problem in remote sensing and mapping. Recent advances in Convolution Neural Network (CNN) have contributed significant improvements in automatic road extraction from satellite imagery, albeit prediction gaps challenge post-processing. Some of the gaps are hard to bridge by satellite imagery alone due to dense vegetation, road construction, and building shadows. In this paper, we combine satellite imagery with GPS data to improve road extraction quality. Our dataset includes 100cm pixel resolution satellite imagery and 192-hour taxi GPS traces from the urban area of Beijing. Experimenting with various layers to combine GPS data, our CNN model outperforms the RGB-only model by nearly 13\% on mean IoU.},
	address = {New York, NY, USA},
	author = {Sun, Tao and Di, Zonglin and Wang, Yin},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3281548.3281550},
	isbn = {9781450360364},
	keywords = {GPS, Road extraction, Satellite image, U-Net},
	location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {29–32},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '18},
	title = {Combining Satellite Imagery and GPS Data for Road Extraction},
	url = {https://doi.org/10.1145/3281548.3281550},
	year = {2018},
}

@inproceedings{10.1145/3281548.3281551,
	abstract = {Estimating a moving crowd, such as the head count of a presidential inauguration or a football game, presents a practical and intellectual challenge that is often politically and emotionally charged. The objectives of this paper are to discuss the integration of artificial intelligence and agent-based model (ABM) to simulate and estimate a moving crowd and outline some key issues and research agenda.To simulate individual movements of a moving crowd, Genetic Algorithm (GA) can be employed to fine-tune agent parameters in wayfinding (e.g. direction, speed, etc.) through mutation, crossover, elitism and extinction. Besides individual-based wayfinding parameters, GA can also be employed to optimize population-wide model parameters as well, such as the maximum walking speed, maximum crowd capacity, early departure and late arrival rates. These individual and global model parameters present different bottom-up and top-down forces in shaping and precipitating diverse crowd behaviors and movements to match empirical pattern. Besides spatial optimization, convolutional NN can also be trained to derive snapshots of crowd count and crowd density from still-frame pictures and videos to better provide feedbacks to the fitness function of GA. However, more researches are needed to better understand and overcome various technical issues in crowd simulation, including but not limited to overtraining in optimization, feature extraction of objects moving in multi- and random directions, ontological separation of protesters from pedestrians and spectators, reconciliation of a single/multiple crowds over time and space.},
	address = {New York, NY, USA},
	author = {Chow, T. Edwin},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3281548.3281551},
	isbn = {9781450360364},
	keywords = {Mobile crowd simulation, agent-based modeling, convolutional neural network, genetic algorithm},
	location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
	numpages = {2},
	pages = {52–53},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '18},
	title = {When GeoAI Meets the Crowd},
	url = {https://doi.org/10.1145/3281548.3281551},
	year = {2018},
}

@inproceedings{10.1145/3281548.3281552,
	abstract = {In this paper, we introduce Aconcagua, a novel spatio-temporal emotion change analysis framework. Our current research uses Twitter tweets as the knowledge source for emotion analysis. The inputs for the emotion mapping and change analysis system, we are currently developing, are the location and time of the tweets and their corresponding emotion assessment score falling in the range [-1, +1], with +1 representing a very positive emotion and -1 representing a very negative emotion. We start by identifying spatial clusters that capture positive and negative emotion regions for batches of the dataset with each batch corresponding to a specific time interval, e.g. a single day. These obtained spatial clusters and their statistical summaries are then used as the input for Aconcagua which monitors change of emotions with respect to a set of unary and binary change predicates that are evaluated with respect to the set of spatial clusters; as the result of this process an emotion change graph is obtained whose nodes are spatial clusters and whose edges capture different types of temporal relationships between spatial clusters. An implementation of the change monitoring process is discussed which operates on top of a relational database and uses SQL queries to specify change predicates. To obtain more aggregated change summaries and ultimately change stories, the change graph further must be mined and summarized based on what aspects of change the analyst is interested in. To support such capabilities, our approach supports several types of change analysis templates called story types. We demo our approach using tweets collected in the state of New York in June 2014.},
	address = {New York, NY, USA},
	author = {Elgarroussi, Karima and Wang, Sujing and Banerjee, Romita and Eick, Christoph F.},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3281548.3281552},
	isbn = {9781450360364},
	keywords = {Emotion Change Analysis, Sentiment Analysis, Spatial Clustering, Spatio Temporal Data Storytelling, Spatiotemporal Data Analysis, Tweet Emotion Mapping},
	location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
	numpages = {8},
	pages = {54–61},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '18},
	title = {Aconcagua: A Novel Spatiotemporal Emotion Change Analysis Framework},
	url = {https://doi.org/10.1145/3281548.3281552},
	year = {2018},
}

@inproceedings{10.1145/3281548.3281558,
	abstract = {Influenza is one of the most common causes of human illness and death; thus, accurate and timely predictions for influenza trends are critical tasks for public health. Many studies have attempted to conduct influenza prediction at or beyond the city scale; however, larger spatial scales are too coarse to help analyze influenza epidemics or allow offering precise interventions inside a city. Moreover, the existing prediction models often ignore the spatial correlations of influenza activity between neighbouring regions although such correlations are potentially helpful in influenza prediction. To address the above issues, this study proposes an influenza prediction model based on a deep residual network that predicts influenza trends by integrating the spatial-temporal properties of influenza at an intra-urban scale. Using a real dataset of influenza in Shenzhen City, China, we tested our prediction model on 10 districts within the city. Our results show that our proposed deep residual model outperforms four baseline models, including linear regression (LR), artificial neural network (ANN), long short-term memory (LSTM) and spatiotemporal LSTM (ST-LSTM) models, thus demonstrating the effectiveness of the proposed prediction model. To our best knowledge, although deep-learning-based approaches have been shown to be useful in many fields in recent years, there has been no attempt to apply such approaches to influenza prediction. Therefore, this study is an initial attempt to introduce a deep learning model into influenza prediction. The proposed deep residual network is able to incorporate the spatial correlations of influenza, and it has obvious potential for making influenza predictions at finer spatial scales within a city, which can offer critical support for preciser public health interventions.},
	address = {New York, NY, USA},
	author = {Xi, Guikai and Yin, Ling and Li, Ye and Mei, Shujiang},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3281548.3281558},
	isbn = {9781450360364},
	keywords = {Influenza prediction, convolutional neural network, deep learning, spatial-temporal properties},
	location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
	numpages = {10},
	pages = {19–28},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '18},
	title = {A Deep Residual Network Integrating Spatial-temporal Properties to Predict Influenza Trends at an Intra-urban Scale},
	url = {https://doi.org/10.1145/3281548.3281558},
	year = {2018},
}

@inproceedings{10.1145/3282825.3282828,
	abstract = {When users browse beautiful scenery photos uploaded on a social media website, they may have a passion to know about where those photos are taken so that they could view the similar sceneries when they go to the same spot. Advancement in computer vision technology enables the extraction of visual features from those images and the widespread of location-awareness devices makes image positioning possible with GPS coordinates or geo-tags (e.g., landmarks, place names). In this paper, we propose a novel method for image positioning by utilizing spatial analysis and computer vision techniques. A prototype system is implemented based on large-scale Flickr photos and a case-study of the Eiffel Tower is demonstrated. Both global and local visual features as well as the spatial context are utilized aiming at building a more accurate and efficient framework. The result illustrates that our approach can achieve a better accuracy compared with the baseline approach. To our knowledge, it is among the first researches that combine not only the visual features of photos, but also take the spatial context into consideration for the image geo-localization using high-density social media photos at the spatial scale of a landmark.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Kang, Yuhao and Gao, Song and Liang, Yunlei},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Recommendations for Location-Based Services and Social Networks},
	doi = {10.1145/3282825.3282828},
	isbn = {9781450360401},
	keywords = {Geo-localization, image matching, image retrieval, spatial heterogeneity, viewshed analysis},
	location = {Seattle, WA, USA},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {LocalRec'18},
	title = {Utilizing Reverse Viewshed Analysis in Image Geo-Localization},
	url = {https://doi.org/10.1145/3282825.3282828},
	year = {2018},
}

@inproceedings{10.1145/3282834.3282840,
	abstract = {Goal of the BigPicture project is to enhance satellite-based information with empirical in-situ ground-truthing. In this approach, the symptoms detected from remote sensing data are a mere starting point where additional data -- like long-term time series, location of unusually growing areas, and weather conditions -- get mixed in to ultimately obtain recommendations on soil treatments and applications of seeds, fertilizers and plant protection products.In this case study we present BigPicture as a practical, non-trivial application of the emerging datacube paradigm It shows a service-centric approach using a high-level standardized query language, the OGC Web Coverage Processing Service (WCPS), rather than low-level procedural programming approaches.},
	address = {New York, NY, USA},
	author = {Baumann, Peter and Kohler, Katrin and Merticariu, Vlad and Isroilov, Ismoil},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3282834.3282840},
	isbn = {9781450360418},
	keywords = {OGC WCPS, Precision farming, analytics, datacube, ground truthing, rasdaman},
	location = {Seattle, WA, USA},
	numpages = {6},
	pages = {10–15},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '18},
	title = {Using Datacube Technology for In-Situ-Enhanced Precision Farming},
	url = {https://doi.org/10.1145/3282834.3282840},
	year = {2018},
}

@inproceedings{10.1145/3282834.3282842,
	abstract = {With a significant amount of spatial data archives online, data conflation is becoming more and more critical in the domain of Geographical Information Science (GIScience) because of its broad applications such as detecting the development of road networks and the change of river course. Existing conflation approaches usually rely on the vector data of corresponding features in multiple sources to have an approximate location. However, they commonly overlook the uncertainty produced during the vector data generation process in the data sources. In previous work, we presented a Convolutional Neural Networks (CNN) recognition system that automatically recognizes areas of geographic features from maps and then generates a centerline representation of the area feature (e.g., from pixels of road areas to a road network). In this paper, we propose a method to systematically quantify the uncertainty generated by an image recognition model and the centerline extraction process. We provide an end-to-end evaluation method that exploits the distance map to calculate the uncertainty value for centerline extraction. Compared with methods that do not consider uncertainty value, our algorithm avoids using a fixed buffer size to identify corresponding features from multiple sources and generate accurate conflation results.},
	address = {New York, NY, USA},
	author = {Lin, Haowen and Chiang, Yao-Yi},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3282834.3282842},
	isbn = {9781450360418},
	keywords = {Historical maps, Uncertainty, Vector data conflation},
	location = {Seattle, WA, USA},
	numpages = {8},
	pages = {20–27},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '18},
	title = {An Uncertainty Aware Method for Geographic Data Conflation},
	url = {https://doi.org/10.1145/3282834.3282842},
	year = {2018},
}

@inproceedings{10.1145/3282866.3282867,
	abstract = {Increasing popularity of social networks made them a viable data source for many data mining applications and event detection is no exception. Researchers aim not only to find events that happen in networks but more importantly to identify and locate events occurring in the real world. In this paper, we propose an enhanced version of quadtree - convolutional quadtree (ConvTree) - and demonstrate its advantage compared to the standard quadtree. We also introduce the algorithm for searching events of different scales using geospatial data obtained from social networks. The algorithm is based on statistical analysis of historical data, generation of ConvTrees representing the normal state of the city and anomalies evaluation for events detection. Experimental study conducted on the dataset of 60 million geotagged Instagram posts in the New York City area demonstrates that the proposed approach is able to find a wide range of events from very local (indie band concert or wedding party) to city (baseball game or holiday march) and even country scale (political protest or Christmas) events. This opens up a perspective of building a simple and fast yet powerful system for real-time multiscale events monitoring.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Visheratin, Alexander A. and Mukhina, Ksenia D. and Visheratina, Anastasia K. and Nasonov, Denis and Boukhanovsky, Alexander V.},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Analytics for Local Events and News},
	doi = {10.1145/3282866.3282867},
	isbn = {9781450360357},
	keywords = {spatial data, social media, monitoring, geogrids, geographic information systems, event detection, convolution, Quadtree},
	location = {Seattle, WA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {LENS'18},
	title = {Multiscale event detection using convolutional quadtrees and adaptive geogrids},
	url = {https://doi.org/10.1145/3282866.3282867},
	year = {2018},
}

@inproceedings{10.1145/3282933.3282935,
	abstract = {With recent increases in incidences of political violence globally, the world has now become more uncertain and less predictable. Of particular concern is the case of violence against civilians, who are often caught in the crossfire between armed state or non-state actors. Classical methods of studying political violence and international relations need to be updated. Adopting the use of data analytic tools and techniques of studying big data would enable academics and policy makers to make sense of a rapidly changing world.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Mack, Vincent Z. W. and Kam, Tin Seong},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities},
	doi = {10.1145/3282933.3282935},
	isbn = {9781450360326},
	keywords = {political violence, knowledge discovery, hotspot detection, geospatial autocorrelation, Africa},
	location = {Seattle, WA, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '18},
	title = {Is There Space for Violence? A Data-driven Approach to the Exploration of Spatial-Temporal Dimensions of Conflict},
	url = {https://doi.org/10.1145/3282933.3282935},
	year = {2018},
}

@inproceedings{10.1145/3282933.3282936,
	abstract = {This paper presents a series of projects where one of the main sources for toponomastic research in Finland, the corpora of 2.7 million place names in the Names Archive database of the Institute for the Languages of Finland, was digitized, enriched, and published as Linked Open Data using a data processing pipeline. Utilizing the Linked Data infrastructure and various external data sources, a modern full-stack web application, NameSampo, was created in collaboration between toponomastic researchers and computer scientists for searching, analyzing, and visualizing digital toponomastic data sources.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Ikkala, Esko and Tuominen, Jouni and Raunamaa, Jaakko and Aalto, Tiina and Ainiala, Terhi and Uusitalo, Helin\"{a} and Hyv\"{o}nen, Eero},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities},
	doi = {10.1145/3282933.3282936},
	isbn = {9781450360326},
	keywords = {Toponyms, Toponomastics, RDF, Onomastics, Linked Data, Geographic Information Retrieval},
	location = {Seattle, WA, USA},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '18},
	title = {NameSampo: A Linked Open Data Infrastructure and Workbench for Toponomastic Research},
	url = {https://doi.org/10.1145/3282933.3282936},
	year = {2018},
}

@inproceedings{10.1145/3282933.3282937,
	abstract = {In this paper we propose a novel method for quality assessment of crowdsourced data. It computes user reputation scores without requiring ground truth; instead, it is based on the consistency among users. In this pilot study, we perform some explorative data analysis on two real crowdsourcing projects by the New York Public Library: extracting building footprints as polygons from historical insurance atlases, and geolocating historical photographs. We show that the computed reputation scores are plausible and furthermore provide insight into user behavior.},
	address = {New York, NY, USA},
	articleno = {3},
	author = {Barz, Bj\"{o}rn and van Dijk, Thomas C. and Spaan, Bert and Denzler, Joachim},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Geospatial Humanities},
	doi = {10.1145/3282933.3282937},
	isbn = {9781450360326},
	keywords = {user reputation, quality control, historical data, crowdsourcing},
	location = {Seattle, WA, USA},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '18},
	title = {Putting User Reputation on the Map: Unsupervised Quality Control for Crowdsourced Historical Data},
	url = {https://doi.org/10.1145/3282933.3282937},
	year = {2018},
}

@inproceedings{10.1145/3283207.3283208,
	abstract = {We propose the group diagram (GD) as a representation for multiple trajectories representing one or several moving groups. Given a distance threshold, a similarity measure and a minimality criterion, a minimal GD is a minimal representation of the groups maintaining the spatio-temporal structure of the groups' movement. We state hardness results and approximation algorithms for computing several variants of the GD and experimentally evaluate our algorithms on GPS data of a family group of migrating geese.},
	address = {New York, NY, USA},
	author = {Buchin, Maike and Kilgus, Bernhard and K\"{o}lzsch, Andrea},
	booktitle = {Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3283207.3283208},
	isbn = {9781450360371},
	keywords = {Trajectory Analysis, Movement Analysis, Computational Geometry},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {1–10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'18},
	title = {Group Diagrams for Representing Trajectories},
	url = {https://doi.org/10.1145/3283207.3283208},
	year = {2018},
}

@inproceedings{10.1145/3283207.3283212,
	abstract = {Public transportation systems, in particular, bus systems, play an essential role in the process of urbanization. Typically more bus stops enable more people to access the bus whereas lower the efficiency of bus system. This study uses a Spatial Interaction Coverage (SIC) model to identify and remove redundant bus stops while maintain the overall success of the whole bus system. The SIC model aims to model the relationship between demand points and bus stops. It takes factors such as the distance and the attractiveness of each bus stop into consideration. The simulated annealing algorithm is then applied with the SIC model to find the optimal combination of bus stops. By applying the SIC model to the iXpress 202 route in Kitchener-Waterloo region, it can effectively identify the number of stops to maintain and remove redundant stops. The bus operation efficiency can be increased by 7.28\% after optimization. The SIC model provides a reliable method of modelling the interaction between facilities and demands. The ability of considering the attractiveness of stops and including distance decay into the model can help the transportation agency better plan bus routes. The relationships between bus ridership and the socioeconomic variables (population, income, and age) in the study area are also analyzed.},
	address = {New York, NY, USA},
	author = {Liang, Yunlei and Gao, Song and Wu, Tianyu and Wang, Sujing and Wu, Yuhao},
	booktitle = {Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3283207.3283212},
	isbn = {9781450360371},
	keywords = {Transportation, Optimization, Interaction, GIS},
	location = {Seattle, WA, USA},
	numpages = {7},
	pages = {53–59},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'18},
	title = {Optimizing Bus Stop Spacing Using the Simulated Annealing Algorithm with Spatial Interaction Coverage Model},
	url = {https://doi.org/10.1145/3283207.3283212},
	year = {2018},
}

@inproceedings{10.1145/3283207.3283213,
	abstract = {A traffic congestion in a road network may propagate to upstream road segments. Such a congestion propagation may make a series of connected road segments congested in the near future. Given a spatial-temporal network and congested road segments in current time, the aim of predicting traffic congestion propagation pattern is to predict where those congestion will propagate to. This can provide users (e.g. city officials) with valuable information on how congestion will propagate in the near future to help mitigating emerging congestions. However, it is challenging to predict in real-time due to complex propagation process between roads and high computational intensity caused by large dataset. Recent studies have been focusing on finding frequent or most likely congestion propagation patterns in historical data. In contrast, this research will address the problem of predicting congestion propagation patterns in the near future. We predict the footprint of congestion propagation as Propagation Graphs (Pro-Graphs) where the root of each Pro-Graph is a set of congested roads propagating congestion to nearby roads. We propose an efficient algorithm called PPI_Fast to achieve this prediction. Our experiments on real-word dataset from Shenzhen, China shows that the PPI_Fast is able to predict near future propagations with AUC of 0.75 and improves the running time of the baseline algorithm. Two case studies have been done to show our work can find meaningful patterns.},
	address = {New York, NY, USA},
	author = {Xiong, Haoyi and Vahedian, Amin and Zhou, Xun and Li, Yanhua and Luo, Jun},
	booktitle = {Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3283207.3283213},
	isbn = {9781450360371},
	keywords = {Traffic Congestion, Spatio-Temporal data Mining, Congestion Propagation},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {60–69},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'18},
	title = {Predicting Traffic Congestion Propagation Patterns: A Propagation Graph Approach},
	url = {https://doi.org/10.1145/3283207.3283213},
	year = {2018},
}

@inproceedings{10.1145/3283207.3283214,
	abstract = {These days we live in a digital era where most societies rely on applications that depend on geospatial data. In addition, most of the recent road network maps are represented in vector format and they have accurate road points coordinates that form the road segments representing the roads. However, there are data discrepancies between maps for various reasons. Therefore, if there is a matching process between vector-vector road network datasets, the data discrepancy will cause some incorrect matching pairs. This paper presents a novel solution inspired by Hausdorff distance to confirm if the candidate similar roads from different maps are really similar to each other or not. We use local divergence measurements that make sure these candidate roads have approximately the same length and also run in parallel to each other, which preserves the shape between them. Confirming the similarity requires also a global divergence measurement to be met that ensures the candidate roads are for the same road, not different roads that happen to be beside each other having similar length and shape. Moreover, this approach has the capability to identify the similar roads when one of the roads has either missing road segments or extra incorrect road segments. The experiments for our method shows promising results.},
	address = {New York, NY, USA},
	author = {Almotairi, Mousa and Alsahfi, Tariq and Elmasri, Ramez},
	booktitle = {Proceedings of the 11th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3283207.3283214},
	isbn = {9781450360371},
	keywords = {Spatial Databases, Road Similarity, Road Networks, Road Divergence, Hausdorff Distance, Data Matching},
	location = {Seattle, WA, USA},
	numpages = {8},
	pages = {21–28},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'18},
	title = {Using Local and Global Divergence Measures to Identify Road Similarity in Different Road Network Datasets},
	url = {https://doi.org/10.1145/3283207.3283214},
	year = {2018},
}

@inproceedings{10.1145/3283590.3283593,
	abstract = {Trajectory prediction has a significant impact on many location-based services such as local search, traffic management, and routing services. Existing trajectory prediction techniques utilize the object's motion history to predict the future path(s). However, these techniques fail when the history is unavailable which realistically happens for multiple reasons such as; history might be difficult to obtain, newly registered user has no past history, or previously recorded data is protected for privacy reasons. This paper introduces a novel system named SimilarMove to predict the future paths of moving objects on road networks without relying on their past trajectories. SimilarMove analyzes the motion pattern of the moving object under investigation and identifies other moving objects that show similar motion patterns. Then, a Markov Model is adopted to digest this set of similar motion patterns and produce the next potential movements of the object under investigation along with their likelihoods. A key aspect of SimilarMove lies in achieving a high quality prediction while being efficient in terms of performance.},
	address = {New York, NY, USA},
	author = {Abdalla, Mohammed and Hendawi, Abdeltawab and ElGamal, Neveen and Mokhtar, Hoda M. O. and Ali, Mohamed and Krumm, John},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Prediction of Human Mobility},
	doi = {10.1145/3283590.3283593},
	isbn = {9781450360425},
	keywords = {Trajectories, Similarity Measurements, Route Prediction, Moving Objects, Location Prediction},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {15–24},
	publisher = {Association for Computing Machinery},
	series = {PredictGIS 2018},
	title = {SimilarMove: Similarity-based Prediction for Moving Object Future Path},
	url = {https://doi.org/10.1145/3283590.3283593},
	year = {2018},
}

@inproceedings{10.1145/3283590.3283594,
	abstract = {City connectivity is an important measurement in characterizing human dynamics from regional to international scales. World City Network has been built based on companies' communication. The interactions between spatial and social dimensions of cities have both conceptual and practical significance. To further expand the studies of inter-city network in the big social data context, this research builds a network at the county level using digital footprints from Twitter users. Retrieving geotags from Twitter users, we identify the connection strength of each pair of counties based on the amounts of shared users who leave digital footprints on both counties. Using the shared user amount as the weighted link and each county as the node, we build a county-to-county user flow network. Various network structures have been detected at the state level. In addition, by creating a direct flow chain, we can identify influential counties and its hinterland. This network demonstrates how human mobility operate across various spatial settings and distances. Results of this study can be used in transportation planning, regional planning and metropolitan management.},
	address = {New York, NY, USA},
	author = {Jiang, Yuqin and Li, Zhenlong and Ye, Xinyue},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL Workshop on Prediction of Human Mobility},
	doi = {10.1145/3283590.3283594},
	isbn = {9781450360425},
	keywords = {social media, human mobility, big data, Inter-city network},
	location = {Seattle, WA, USA},
	numpages = {7},
	pages = {25–31},
	publisher = {Association for Computing Machinery},
	series = {PredictGIS 2018},
	title = {Measuring Inter-city Network Using Digital Footprints from Twitter Users},
	url = {https://doi.org/10.1145/3283590.3283594},
	year = {2018},
}

@inproceedings{10.1145/3284103.3284113,
	abstract = {In recent years, frequent public emergencies have resulted in heavy casualties and economic losses. In the early stage of emergency rescue, the demand for emergency resources is strictly greater than the supply. As an important part of emergency management, emergency resource scheduling is an important manifestation of its rescue value. At present, Many studies take emergency resource scheduling as a multi-objective optimization problem. They focus on the total emergency cost and does not evaluate the emergency resource allocation according to the actual disaster severity of each emergency demand point. What's more, they do not consider road information. However, in actual, the condition of roads directly affects the emergency cost. To address this problem, we proposed an effective emergency logistics scheduling model based on multi-objective optimization algorithms. We get road information from spatial-temporal trajectory data. The research on emergency resource scheduling in this paper is helpful to achieve greater utility under the condition of limited emergency resources and reduce the loss of public emergencies.},
	address = {New York, NY, USA},
	articleno = {10},
	author = {Ding, Zhiming and Cao, Yang and Chi, Yuanying and Li, Xuyang and Yao, Rui and Guo, Limin},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Safety and Resilience},
	doi = {10.1145/3284103.3284113},
	isbn = {9781450360449},
	keywords = {trajectory vector feature, road network condition, multi-objective optimization algorithms, emergency logistics scheduling},
	location = {Seattle, WA, USA},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {Safety and Resilience'18},
	title = {An effective emergency logistics scheduling model based on multi-objective optimization algorithms},
	url = {https://doi.org/10.1145/3284103.3284113},
	year = {2018},
}

@inproceedings{10.1145/3284557.3284709,
	abstract = {In the field of flow visualization, the optimization of algorithms and the improvement of visual effects have always been the focus of researchers, in recent years many research and improvement algorithms for evaluating visualization quality have been proposed. However, in the process of algorithm implementation and quality evaluation, ambiguity of flow field direction and unclear description of vector field direction and size are often encountered, in order to solve this problem, an attempt is made to apply the theory of human visual perception to the evaluation of visualization results and the improvement of visualization effects, and on the basis of research by simulation of cyclone data, the two-dimensional line integral convolution algorithm for vector field visualization is studied.},
	address = {New York, NY, USA},
	articleno = {58},
	author = {Ma, Ying-Yi and Guo, Yi-Feng},
	booktitle = {Proceedings of the 2nd International Symposium on Computer Science and Intelligent Control},
	doi = {10.1145/3284557.3284709},
	isbn = {9781450366281},
	keywords = {Flow Visualization, LIC, OLIC, Visual Perception, wind field},
	location = {Stockholm, Sweden},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {ISCSIC '18},
	title = {Visualization of Vector Field using Line Integral Convolution based on Visual Perception},
	url = {https://doi.org/10.1145/3284557.3284709},
	year = {2018},
}

@inproceedings{10.1145/3284566.3284569,
	abstract = {The rapid progress of urbanization creates great challenges for urban planners. These challenges include the environment, energy consumption, transport, and other areas. The massive amount of information generated everyday in urban environments allows more precise diagnostics of the problems and helps in the design of solutions. Since such sources of information are heterogeneous and involve a large amount of data, major computational challenges arise. The objective of this work is to propose techniques and methods that allow the integration and visualization of urban data from multiple heterogeneous sources, aiming to create tools for urban data analysis, focusing mainly on transportation and transit. We propose the Urban Transit Fingerprint visualization, in which the geometry, length and duration of travels within a city can be compared, and indicators of the relative efficiency between private (individual) and public (mass) transport can be assessed. Data from Belo Horizonte and S\~{a}o Paulo, Brazil, are analyzed using the proposed technique as case studies. Results show wide discrepancies in the effort a public transit user has to make for her mobility in various regions of each city, as compared with individual travel. These results help explaining the sharp drop in the number of transit users over the last years, thus calling on urban planners to devise public policies in the search for better solutions.},
	address = {New York, NY, USA},
	author = {Fortini, Pedro Magalh\~{a}es and Davis, Clodoveu A.},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL Workshop on Advances on Resilient and Intelligent Cities},
	doi = {10.1145/3284566.3284569},
	isbn = {9781450360395},
	keywords = {Visual Analytics, Urban Computing, Geographic Information Systems, Data Visualization},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {17–26},
	publisher = {Association for Computing Machinery},
	series = {ARIC'18},
	title = {Analysis, Integration and Visualization of Urban Data From Multiple Heterogeneous Sources},
	url = {https://doi.org/10.1145/3284566.3284569},
	year = {2018},
}

@inproceedings{10.1145/3286606.3286796,
	abstract = {An address system is an essential infrastructure which paves the way for social and economic development. It allows people to connect, improves emergency response, increases access to utilities and facilitates postal services and the delivery of goods. On the other hand a digital address infrastructure is undeniably essential for many other processes such as opening bank accounts, the creation of national ID Cards, voting and representing sets of information in data collection and analysis, as well as in marketing and in population census.Designing an addressing system suitable for all countries is challenging. This paper presents a solution by providing a comprehensive universal addressing system - the MappGuru - designed to function as a standardized addressing system with the ability to provide postcode and addresses to any properties.The MappGuru addressing system suggests a unique process to assign addresses based on an existing postcode. The MappGuru addresses are easy for human to understand and can inherently be integrated with information technology systems.To illustrate the MappGuru addressing system, a dedicated section consisting of a case study to assign addresses to an unstructured area in Kigali/Rwanda is described.},
	address = {New York, NY, USA},
	articleno = {19},
	author = {Rwerekane, Valentin and Ndashimye, Maurice},
	booktitle = {Proceedings of the 3rd International Conference on Smart City Applications},
	doi = {10.1145/3286606.3286796},
	isbn = {9781450365628},
	keywords = {Address, postal code, postcode},
	location = {Tetouan, Morocco},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {SCA '18},
	title = {The MappGuru, a universal addressing system},
	url = {https://doi.org/10.1145/3286606.3286796},
	year = {2018},
}

@inproceedings{10.1145/3287921.3287956,
	abstract = {Jakarta has become a megacity with elaborate service network activities. Fast food restaurants as a type of food service provider have a role in supporting urban lifestyles. Despite the growth of value and transaction volume, there are some fast food categories in Indonesia which have a negative percentage of outlets growth. In general, the location of fast food restaurants divides into two categories. The first one is stand-alone restaurants, and the second is restaurants which located in other public facilities, such as malls, supermarket, and market area. According to the first law of Tobler, closer public facilities will have activity relatedness. This study aims to examine whether proximity between fast food restaurant locations and other public facilities affect categories of fast food restaurants, using spatial decision tree analysis approach. The public facilities examined for proximity to fast food restaurants consist of 11 criteria, which are considered to have a co-location pattern from previous research results. The results will be spatial characteristics of public facilities which expected to be indicators of consumer movement behavior, especially from and to fast food restaurant.},
	address = {New York, NY, USA},
	author = {Widaningrum, Dyah Lestari and Surjandari, Isti and Sudiana, Dodi},
	booktitle = {Proceedings of the 9th International Symposium on Information and Communication Technology},
	doi = {10.1145/3287921.3287956},
	isbn = {9781450365390},
	keywords = {Spatial, Location Pattern, Fast Food Restaurant, Decision Tree},
	location = {Danang City, Viet Nam},
	numpages = {8},
	pages = {422–429},
	publisher = {Association for Computing Machinery},
	series = {SoICT '18},
	title = {Spatial Decision Tree Analysis to Identify Location Pattern},
	url = {https://doi.org/10.1145/3287921.3287956},
	year = {2018},
}

@inproceedings{10.1145/3288599.3288621,
	abstract = {In this paper, we propose a novel climate-smart Agriculture Cyber-Physical System (ACPS) for precision farming. The primary motive of the ACPS is to perform real-time fault location tracking in the agricultural field using multivariate sensor data. The computing model in the ACPS uses a novel hybrid classification approach which combines two classifiers for the location estimation of the sensor node. The novelty of the proposed method lies in predicting the locations that need more irrigation, soil nutrients or immediate human intervention using the sensor data. We also derive the computational complexity of the proposed method. The location accuracy improves reasonably as compared to the current-state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Pandey, Ankur and Tiwary, Piyush and Kumar, Sudhir and Das, Sajal K},
	booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
	doi = {10.1145/3288599.3288621},
	isbn = {9781450360944},
	keywords = {wireless sensor networks, smart agriculture, machine learning, location estimation, cyber-physical systems},
	location = {Bangalore, India},
	numpages = {5},
	pages = {337–341},
	publisher = {Association for Computing Machinery},
	series = {ICDCN '19},
	title = {A hybrid classifier approach to multivariate sensor data for climate smart agriculture cyber-physical systems},
	url = {https://doi.org/10.1145/3288599.3288621},
	year = {2019},
}

@inproceedings{10.1145/3288599.3295588,
	abstract = {Dissemination of information after a large-scale disaster is always a difficult task and hinders the possibilities of proper decision making. Relief work and rescuing survivors take a major hit without a proper map showcasing the current scenario of the disaster-hit terrain. All due to the lack of traditional communication channels after a disaster. However, we want to change this with the proposition of Android-based mobile application SURAKSHIT that prepares 'localized' crisis map through 'offline' crowdsourcing of circumstantial data and distribution of the same in a smooth manner using a delay-tolerant network. The information gap between the first responders and local survivors can thus be greatly reduced with proper knowledge about the affected area collected by the volunteers. Multimedia data accumulated with the application can further assist the rescue forces in planning rescue-relief operations better.},
	address = {New York, NY, USA},
	author = {Paul, Partha Sarathi and Mehta, Naman and Das, Aman Kumar and Agarwal, Sourav and Saha, Sujoy and Nandi, Subrata},
	booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
	doi = {10.1145/3288599.3295588},
	isbn = {9781450360944},
	keywords = {volunteered geographic information, geographical information system, disaster management system, delay-tolerant network},
	location = {Bangalore, India},
	numpages = {4},
	pages = {393–396},
	publisher = {Association for Computing Machinery},
	series = {ICDCN '19},
	title = {SURAKSHIT: a smartphone-based application for 'localized' GIS data aggregation in absence of internet},
	url = {https://doi.org/10.1145/3288599.3295588},
	year = {2019},
}

@inproceedings{10.1145/3288599.3295592,
	abstract = {Modeling the demand and supply during an absolute catastrophe is a major challenge for any disaster manager. This paper demonstrates implementation of an easily deployable end-to-end system, by which, various situational data can be collected by the stakeholders during any post-disaster circumstances, and a general summarized view can be plotted on crisis map. The proposed Four Tier Hybrid Ad hoc Network Architecture does not require an Internet backbone but can conduct the complete process in 'offline' mode. Periodic Merging of GIS data enriches the quality of the summarized view. Here we have tried to evaluate the system based on some initial results on a dummy challenged network scenario.},
	address = {New York, NY, USA},
	author = {Paul, Partha Sarathi and Mukherjee, Chandrika and Ghosh, Bishakh Chandra and Pandit, Sudipta and Saha, Sujoy and Nandi, Subrata},
	booktitle = {Proceedings of the 20th International Conference on Distributed Computing and Networking},
	doi = {10.1145/3288599.3295592},
	isbn = {9781450360944},
	keywords = {hybrid network architecture, geographic information system, delay-tolerant network, crisis mapping},
	location = {Bangalore, India},
	numpages = {4},
	pages = {409–412},
	publisher = {Association for Computing Machinery},
	series = {ICDCN '19},
	title = {On designing a fast-deployable 'localized' GIS platform for using 'offline' during post-disaster situation},
	url = {https://doi.org/10.1145/3288599.3295592},
	year = {2019},
}

@inproceedings{10.1145/3289600.3290995,
	abstract = {Identifying and recommending potential new customers for local businesses are crucial to the survival and success of local businesses. A key component to identifying the right customers is to understand the decision-making process of choosing a business over the others. However, modeling this process is an extremely challenging task as a decision is influenced by multiple factors. These factors include but are not limited to an individual's taste or preference, the location accessibility of a business, and the reputation of a business from social media. Most of the recommender systems lack the power to integrate multiple factors together and are hardly extensible to accommodate new incoming factors. In this paper, we introduce a unified framework, CORALS, which considers the personal preferences of different customers, the geographical influence, and the reputation of local businesses in the customer recommendation task. To evaluate the proposed model, we conduct a series of experiments to extensively compare with 12 state-of-the-art methods using two real-world datasets. The results demonstrate that CORALS outperforms all these baselines by a significant margin in most scenarios. In addition to identifying potential new customers, we also break down the analysis for different types of businesses to evaluate the impact of various factors that may affect customers' decisions. This information, in return, provides a great resource for local businesses to adjust their advertising strategies and business services to attract more prospective customers.},
	address = {New York, NY, USA},
	author = {Li, Ruirui and Jiang, Jyun-Yu and Ju, Chelsea J.-T. and Wang, Wei},
	booktitle = {Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining},
	doi = {10.1145/3289600.3290995},
	isbn = {9781450359405},
	keywords = {reputation reliance, pairwise ranking, geographical preference, customer prediction},
	location = {Melbourne VIC, Australia},
	numpages = {9},
	pages = {69–77},
	publisher = {Association for Computing Machinery},
	series = {WSDM '19},
	title = {CORALS: Who Are My Potential New Customers? Tapping into the Wisdom of Customers' Decisions},
	url = {https://doi.org/10.1145/3289600.3290995},
	year = {2019},
}

@article{10.1145/3291059,
	abstract = {In fine-grained tweet geolocation, tweets are linked to the specific venues (e.g., restaurants, shops) from which they were posted. This explicitly recovers the venue context that is essential for applications such as location-based advertising or user profiling. For this geolocation task, we focus on geolocating tweets that are contained in tweet sequences. In a tweet sequence, tweets are posted from some latent venue(s) by the same user and within a short time interval. This scenario arises from two observations: (1) It is quite common that users post multiple tweets in a short time and (2) most tweets are not geocoded. To more accurately geolocate a tweet, we propose a model that performs query expansion on the tweet (query) using two novel approaches. The first approach temporal query expansion considers users’ staying behavior around venues. The second approach visitation query expansion leverages on user revisiting the same or similar venues in the past. We combine both query expansion approaches via a novel fusion framework and overlay them on a Hidden Markov Model to account for sequential information. In our comprehensive experiments across multiple datasets and metrics, we show our proposed model to be more robust and accurate than other baselines.},
	address = {New York, NY, USA},
	articleno = {17},
	author = {Chong, Wen-Haw and Lim, Ee-Peng},
	doi = {10.1145/3291059},
	issn = {1046-8188},
	issue_date = {April 2019},
	journal = {ACM Trans. Inf. Syst.},
	keywords = {temporal proximity, staying behavior, Tweet geolocation},
	month = {jan},
	number = {2},
	numpages = {33},
	publisher = {Association for Computing Machinery},
	title = {Fine-grained Geolocation of Tweets in Temporal Proximity},
	url = {https://doi.org/10.1145/3291059},
	volume = {37},
	year = {2019},
}

@article{10.1145/3291933,
	abstract = {Widespread use of advanced mobile devices has led to the emergence of a new class of crowdsourcing called spatial crowdsourcing. Spatial crowdsourcing advances the potential of a crowd to perform tasks related to real-world scenarios involving physical locations, which were not feasible with conventional crowdsourcing methods. The main feature of spatial crowdsourcing is the presence of spatial tasks that require workers to be physically present at a particular location for task fulfillment. Research related to this new paradigm has gained momentum in recent years, necessitating a comprehensive survey to offer a bird’s-eye view of the current state of spatial crowdsourcing literature. In this article, we discuss the spatial crowdsourcing infrastructure and identify the fundamental differences between spatial and conventional crowdsourcing. Furthermore, we provide a comprehensive view of the existing literature by introducing a taxonomy, elucidate the issues/challenges faced by different components of spatial crowdsourcing, and suggest potential research directions for the future.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Gummidi, Srinivasa Raghavendra Bhuvan and Xie, Xike and Pedersen, Torben Bach},
	doi = {10.1145/3291933},
	issn = {0362-5915},
	issue_date = {June 2019},
	journal = {ACM Trans. Database Syst.},
	keywords = {task scheduling, task matching, task assignment, spatial databases, spatial crowdsourcing, rewards, quality assurance, location privacy, incentive mechanism, Algorithms},
	month = {mar},
	number = {2},
	numpages = {46},
	publisher = {Association for Computing Machinery},
	title = {A Survey of Spatial Crowdsourcing},
	url = {https://doi.org/10.1145/3291933},
	volume = {44},
	year = {2019},
}

@inproceedings{10.1145/3292425.3292429,
	abstract = {NOTICE OF RETRACTION: While investigating potential publication-related misconduct in connection with the IHIP 2018 Conference Proceedings, serious concerns were raised that cast doubt on the integrity of the peer-review process and all papers published in the Proceedings of this Conference. The integrity of the entire Conference has been called into question. As a result, of its investigation, ACM has decided to retract the Entire Conference Proceedings and all related papers from the ACM Digital Library.None of the published papers from this Proceeding should be cited in the literature because of the questionable integrity of the peer review process for this Conference.},
	address = {New York, NY, USA},
	author = {Liu, Feng and Huang, Zhifeng and Wang, Zhihao and Ma, Shiji},
	booktitle = {Proceedings of the 2018 International Conference on Information Hiding and Image Processing},
	doi = {10.1145/3292425.3292429},
	isbn = {9781450365468},
	keywords = {Multi-field bus, custom communication protocol, multi-strategy configuration, universal send and receive},
	location = {Manchester, United Kingdom},
	numpages = {4},
	pages = {68–71},
	publisher = {Association for Computing Machinery},
	series = {IHIP 2018},
	title = {A Data Communication Method Based on Multi - field Bus},
	url = {https://doi.org/10.1145/3292425.3292429},
	year = {2018},
}

@inproceedings{10.1145/3292500.3330767,
	abstract = {In this paper we consider the problem of estimating the difficulty of parking at a particular time and place; this problem is a critical sub-component for any system providing parking assistance to users. We describe an approach to this problem that is currently in production in Google Maps, providing inferences in cities across the world. We present a wide range of features intended to capture different aspects of parking difficulty and study their effectiveness both alone and in combination. We also evaluate various model architectures for the prediction problem. Finally, we present challenges faced in estimating parking difficulty in different regions of the world, and the approaches we have taken to address them.},
	address = {New York, NY, USA},
	author = {Arora, Neha and Cook, James and Kumar, Ravi and Kuznetsov, Ivan and Li, Yechen and Liang, Huai-Jen and Miller, Andrew and Tomkins, Andrew and Tsogsuren, Iveel and Wang, Yi},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3292500.3330767},
	isbn = {9781450362016},
	keywords = {trajectories, parking difficulty, location history},
	location = {Anchorage, AK, USA},
	numpages = {9},
	pages = {2296–2304},
	publisher = {Association for Computing Machinery},
	series = {KDD '19},
	title = {Hard to Park? Estimating Parking Difficulty at Scale},
	url = {https://doi.org/10.1145/3292500.3330767},
	year = {2019},
}

@inproceedings{10.1145/3292500.3330878,
	abstract = {Spatial structured models are predictive models that capture dependency structure between samples based on their locations in the space. Learning such models plays an important role in many geoscience applications such as water surface mapping, but it also poses significant challenges due to implicit dependency structure in continuous space and high computational costs. Existing models often assume that the dependency structure is based on either spatial proximity or network topology, and thus cannot incorporate complex dependency structure such as contour and flow direction on a 3D potential surface. To fill the gap, this paper proposes a novel spatial structured model called hidden Markov contour tree (HMCT), which generalizes the traditional hidden Markov model from a total order sequence to a partial order polytree. HMCT also advances existing work on hidden Markov trees through capturing complex contour structures on a 3D surface. We propose efficient model construction and learning algorithms. Evaluations on real world hydrological datasets show that our HMCT outperforms multiple baseline methods in classification performance and that HMCT is scalable to large data sizes (e.g., classifying millions of samples in seconds).},
	address = {New York, NY, USA},
	author = {Jiang, Zhe and Sainju, Arpan Man},
	booktitle = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3292500.3330878},
	isbn = {9781450362016},
	keywords = {structured prediction, spatial structured model, hidden Markov contour tree, flood mapping, 3D surface},
	location = {Anchorage, AK, USA},
	numpages = {10},
	pages = {804–813},
	publisher = {Association for Computing Machinery},
	series = {KDD '19},
	title = {Hidden Markov Contour Tree: A Spatial Structured Model for Hydrological Applications},
	url = {https://doi.org/10.1145/3292500.3330878},
	year = {2019},
}

@article{10.1145/3295459,
	abstract = {An important problem in terrain analysis is modeling how water flows across a terrain and creates floods by filling up depressions. In this article, we study the flooding query problem: Preprocess a given terrain Σ, represented as a triangulated xy-monotone surface with n vertices, into a data structure so that for a query rain region R and a query point q on Σ, one can quickly determine how much rain has to fall in R so that q is flooded. Available terrain data is often subject to uncertainty, which must be incorporated into the terrain analysis. For instance, the digital elevation models of terrains have to be refined to incorporate underground pipes, tunnels, and waterways under bridges, but there is often uncertainty in their existence. By representing the uncertainty in the terrain data explicitly, we can develop methods for flood risk analysis that properly incorporate terrain uncertainty when reporting what areas are at risk of flooding.We present two results. First, we present an O(n log n)-time algorithm for preprocessing Σ with a linear-size data structure that can answer a flooding query in O(|R| + m log n) time, where |R| is the number of vertices in R, m is the number of so-called tributaries of q at which rain is falling, and n is the number of vertices of the terrain. Next, we extend this data structure to handle “uncertain” terrains using a standard Monte Carlo method. Given a probability distribution on terrain data, our data structure returns the probability of a query point being flooded if a specified amount of rain falls on a query region. We implement our data structure and test it on real terrains, showing that a small number of samples suffice to accurately estimate the flood risk.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Rav, Mathias and Lowe, Aaron and Agarwal, Pankaj K.},
	doi = {10.1145/3295459},
	issn = {2374-0353},
	issue_date = {March 2019},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {stochastic process, geographical information systems, data uncertainty, contour trees, Terrains, Monte Carlo method},
	month = {jun},
	number = {1},
	numpages = {31},
	publisher = {Association for Computing Machinery},
	title = {Flood Risk Analysis on Terrains},
	url = {https://doi.org/10.1145/3295459},
	volume = {5},
	year = {2019},
}

@inproceedings{10.1145/3297001.3297017,
	abstract = {Prediction of spatial raster time series, especially those obtained from satellite remote sensing imagery, plays a crucial role in monitoring various complex spatio-temporal processes, such as urban growth, deforestation, flooding, and so on. With the recent advancement of remote sensing technology, the availability of such raster data with high spatial/temporal density has increased exponentially. However, efficiently extracting complex spatio-temporal relationships by proper utilization of these enormous volumes of data is a critical issue which imposes significant challenge in timely processing and prediction of such spatio-temporal data. This paper proposes spatio-temporal Bayesian network (STBN) which is able to efficiently model/capture the temporal dynamics of spatial dependency among variables and eventually helps accelerating the spatial time series prediction process. The performance of the proposed STBN has been evaluated in comparison with several state-of-the-art prediction models and baseline techniques. Overall, the proposed STBN-based prediction model is found to show improved accuracy with considerably reduced computational cost.},
	address = {New York, NY, USA},
	author = {Das, Monidipa and Ghosh, Soumya K.},
	booktitle = {Proceedings of the ACM India Joint International Conference on Data Science and Management of Data},
	doi = {10.1145/3297001.3297017},
	isbn = {9781450362078},
	keywords = {Time series prediction, Spatial raster, Remote sensing, Composite node, Bayesian network},
	location = {Kolkata, India},
	numpages = {7},
	pages = {129–135},
	publisher = {Association for Computing Machinery},
	series = {CODS-COMAD '19},
	title = {Space-time Prediction of High Resolution Raster Data: An Approach based on Spatio-temporal Bayesian Network (STBN)},
	url = {https://doi.org/10.1145/3297001.3297017},
	year = {2019},
}

@inproceedings{10.1145/3297156.3297193,
	abstract = {With the development of science and technology, the efficient storage and management of massive GIS(Geographic Information System) spatio-temporal data has become more important. The traditional spatio-temporal data model is completely based on relational database, with limited service capabilities, data patterns, and indexing capabilities. In response to the shortcomings of traditional storage methods, this paper proposes a ST-OpenGIS spatio-temporal data model suitable for cloud environment storage and maintaining spatio-temporal data structures and correlations, it describes spatial dimension, time dimension and attribute dimension information as well as temporal relationship and spatial topological relationship. It proposes a general-purpose spatio-temporal data read-write architecture for massive GIS based on MapReduce framework, it proposes a global distributed DBZ(Distributed BZ) index of improved BZ tree, the node information is stored in NoSQL(Not Only SQL) database, the offline processing, online maintenance and data query strategy of index are designed to realize high-performance spatio-temporal index. Based on the above, the prototype system is implemented, and the function verification and performance test are carried out. The results show that the prototype system is superior to the traditional relational storage scheme and can achieve the purpose of efficient storage and query of spatio-temporal data.},
	address = {New York, NY, USA},
	author = {Yu, Bin and Zhang, Chen and Sun, Jiangyan and Zhang, Yu},
	booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
	doi = {10.1145/3297156.3297193},
	isbn = {9781450366069},
	keywords = {Spatio-Temporal Index, Spatio-Temporal Data, NoSQL, MapReduce},
	location = {Shenzhen, China},
	numpages = {5},
	pages = {105–109},
	publisher = {Association for Computing Machinery},
	series = {CSAI '18},
	title = {Massive GIS Spatio-temporal Data Storage Method in Cloud Environment},
	url = {https://doi.org/10.1145/3297156.3297193},
	year = {2018},
}

@inproceedings{10.1145/3297280.3297341,
	abstract = {In an increasingly urbanized world, where cities are changing continuously, it is essential for policy makers to have access to regularly updated decision-making tools for an effective management of urban areas. An example of these tools is the delineation of cities into functional areas which provides knowledge on high spatial interaction zones and their socioeconomic composition. In this paper, we presented a method for the structural analysis of a city, specifically for the determination of its functional areas, based on communities detection in graph. The nodes of the graph correspond to geographical units resulting from a cartographic division of the city according to the road network. The edges are weighted using a Gaussian distance-decay function and the amount of spatial interactions between nodes. Our approach optimize the modularity to ensure that the functional areas detected have strong interactions within their borders but lower interactions outside. Moreover, it leverages on POIs' entropy to maintain a good socioeconomic heterogeneity in the detected areas. We conducted experiments using taxi trips and POIs datasets from the city of Porto, as a study case. Trough those experiments, we demonstrate the ability of our method to portray functional areas while including spatial and socioeconomic dynamics.},
	address = {New York, NY, USA},
	author = {Houssou, Noud\'{e}hou\'{e}nou L. J. and Guillaume, Jean-loup and Prigent, Armelle},
	booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
	doi = {10.1145/3297280.3297341},
	isbn = {9781450359337},
	keywords = {trajectories analysis, functional areas, complex networks, community detection},
	location = {Limassol, Cyprus},
	numpages = {7},
	pages = {652–658},
	publisher = {Association for Computing Machinery},
	series = {SAC '19},
	title = {A graph based approach for functional urban areas delineation},
	url = {https://doi.org/10.1145/3297280.3297341},
	year = {2019},
}

@inproceedings{10.1145/3297280.3297556,
	abstract = {Big data visualization is a main task for data analysis. Due to its complexity in terms of volume and variety, very large datasets are unable to be queried for similarities among entries in traditional Database Management Systems. In this paper, we propose an effective approach for indexing millions of elements with the purpose of performing single and multiple visual similarity queries on multidimensional data associated with geographical locations. Our approach makes use of Z-Curve algorithm to map into 1D space considering similarities between data. Additionally, we present a set of results using real data of different sources and we analyze the insights obtained from the interactive exploration.},
	address = {New York, NY, USA},
	author = {Peralta-Aranibar, Roger and Pahins, Cicero A. L. and Comba, Joao L. D. and Gomez-Nieto, Erick},
	booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
	doi = {10.1145/3297280.3297556},
	isbn = {9781450359337},
	keywords = {similarity, multidimensional data, interactive visualization, geographic information},
	location = {Limassol, Cyprus},
	numpages = {4},
	pages = {683–686},
	publisher = {Association for Computing Machinery},
	series = {SAC '19},
	title = {Similarity-based visual exploration of very large georeferenced multidimensional datasets},
	url = {https://doi.org/10.1145/3297280.3297556},
	year = {2019},
}

@inproceedings{10.1145/3297280.3299802,
	abstract = {Network theory has established itself as an appropriate tool for complex systems analysis and pattern recognition. In the context of spatiotemporal data analysis, correlation networks are used in the vast majority of works. However, the Pearson correlation coefficient captures only linear relationships and does not correctly capture recurrent events. This missed information is essential for temporal pattern recognition. In this work, we propose a chronological network construction process that is capable of capturing various events. Similar to the previous methods, we divide the area of study into grid cells and represent them by nodes. In our approach, links are established if two consecutive events occur in two different nodes. Our method is computationally efficient, adaptable to different time windows and can be applied to any spatiotemporal data set. As a proof-of-concept, we evaluated the proposed approach by constructing chronological networks from the MODIS dataset for fire events in the Amazon basin. We explore two data analytic approaches: one static and another temporal. The results show some activity patterns on the fire events and a displacement phenomenon over the year. The validity of the analyses in this application indicates that our data modeling approach is very promising for spatio-temporal data mining.},
	address = {New York, NY, USA},
	author = {Vega-Oliveros, Didier A. and Cotacallapa, Mosh\'{e} and Ferreira, Leonardo N. and Quiles, Marcos G. and Zhao, Liang and Macau, Elbert E. N. and Cardoso, Manoel F.},
	booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
	doi = {10.1145/3297280.3299802},
	isbn = {9781450359337},
	keywords = {temporal networks, geographical data modeling and analytics, fire activity, complex networks, Amazon basin},
	location = {Limassol, Cyprus},
	numpages = {8},
	pages = {675–682},
	publisher = {Association for Computing Machinery},
	series = {SAC '19},
	title = {From spatio-temporal data to chronological networks: an application to wildfire analysis},
	url = {https://doi.org/10.1145/3297280.3299802},
	year = {2019},
}

@inproceedings{10.1145/3297662.3365826,
	abstract = {Owing to the increasing popularity of mobile devices embedded with a Global Positioning System (GPS) sensor, large amounts of user-generated content containing spatial information have been uploaded to social media websites, such as Flickr or Twitter. That content posted from tourist spots can be used to search for and recommend other tourism spots and routes. Recent research papers in the field of Natural Language Processing (NLP) have proposed learning a distributed representation of words using embedding algorithms. In this paper, we use a Skip-gram model to analyze user movements obtained from social media websites. We propose a new Skip-gramgemodel to learn movements between a pair of locations quantified by the latitude and the longitude. The embedding vectors by our model represents user movements between the locations. We successfully demonstrated that the embedded vectors generated with our proposed method can extract detour spots for tourism spots and routes.},
	address = {New York, NY, USA},
	author = {Hirota, Masaharu and Oda, Tetsuya and Endo, Masaki and Ishikawa, Hiroshi},
	booktitle = {Proceedings of the 11th International Conference on Management of Digital EcoSystems},
	doi = {10.1145/3297662.3365826},
	isbn = {9781450362382},
	keywords = {Flickr, Skip-gram model, User location, User trajectory, Word2ec},
	location = {Limassol, Cyprus},
	numpages = {6},
	pages = {250–255},
	publisher = {Association for Computing Machinery},
	series = {MEDES '19},
	title = {Generating Distributed Representation of User Movement for Extracting Detour Spots},
	url = {https://doi.org/10.1145/3297662.3365826},
	year = {2020},
}

@inproceedings{10.1145/3299869.3320242,
	abstract = {Immense volumes of geospatial arrays are generated daily. Examples of such include satellite imagery, numerical simulation, and derivative data avalanche. Array DBMS are one of the prominent tools for working with large geospatial arrays. Usually the arrays natively come as raster files. ChronosDB is a novel distributed, file based, geospatial array DBMS: http://chronosdb.gis.land/ ChronosDB operates directly on raster files, delegates array processing to existing elaborate command line tools, and outperforms SciDB by up to 75x on average. This demonstration will showcase three new components of ChronosDB enabling users to interact with the system and appreciate its benefits: (i) a Web GUI (edit, submit queries and get the output), (ii) an execution plan explainer (investigate the generated DAG), and (iii) a dataset visualizer (display ChronosDB arrays on an interactive web map).},
	address = {New York, NY, USA},
	author = {Rodriges Zalipynis, Ramon Antonio},
	booktitle = {Proceedings of the 2019 International Conference on Management of Data},
	doi = {10.1145/3299869.3320242},
	isbn = {9781450356435},
	keywords = {wmts (web map tile service), web gui, gml (graph modeling language), geospatial data, gdal, execution plan, earth remote sensing, command line tools, array dbms},
	location = {Amsterdam, Netherlands},
	numpages = {4},
	pages = {1985–1988},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '19},
	title = {ChronosDB in Action: Manage, Process, and Visualize Big Geospatial Arrays in the Cloud},
	url = {https://doi.org/10.1145/3299869.3320242},
	year = {2019},
}

@inproceedings{10.1145/3301506.3301512,
	abstract = {Availability of various geospatial data has increased with the recent growth of geospatial information on the web. Integrating or conflating multiple geospatial datasets have become an important aspect in modern geographic information processing as it can provide insights that are not capable to obtain from an individual dataset. However, positional inconsistencies or misalignments can occur during the conflation process as different geospatial datasets could have different accuracy levels, different geospatial data types (vector or raster), and different projections when they originate from different sources. Thus, making the conflation process a challenging task. In this paper, we present a novel alignment algorithm which uses the concept of B\'{e}zier curves to refine the positional inconsistencies that occur during the vector to imagery conflation process of road vector data. A novel evaluation model which considers the area of the polygon bordering the actual road and the vector layer is introduced. Proposed approach is evaluated against the existing Piecewise Linear Rubber Sheeting method on the Sri Lankan road vector data on test scenarios consisting of road segments with varied misalignments. Results show that the proposed b\'{e}zier approach has an average misalignment reduction percentage of 75.8\% compared to 68.6\% from the existing piecewise linear rubber-sheeting method.},
	address = {New York, NY, USA},
	author = {Perera, Maneesha and Karunaratne, Damith and Hettiarachchi, Enosha},
	booktitle = {Proceedings of the 2018 2nd International Conference on Video and Image Processing},
	doi = {10.1145/3301506.3301512},
	isbn = {9781450366137},
	keywords = {Vector, Satellite Images, Raster, GIS, Conflation, B\'{e}zier},
	location = {Hong Kong, Hong Kong},
	numpages = {6},
	pages = {192–197},
	publisher = {Association for Computing Machinery},
	series = {ICVIP '18},
	title = {Using Bezier Curves to Refine Road Vector Data through Satellite Images},
	url = {https://doi.org/10.1145/3301506.3301512},
	year = {2018},
}

@inproceedings{10.1145/3301551.3301581,
	abstract = {The ability to analyze and turn the data into useful information is becoming increasingly important to businesses as they need to quickly react to changing demands and needs. Data visualization has become progressively important to enable data to be visually explored and analyzed. A known challenge lies in how to move beyond exploration and focus attention in helping decision makers to capture further insights and reason about what they are seeing. In this paper, we show how data visualization can play a critical role not only for patterns visualization but also towards developing new insights through exploratory data analysis. To highlight the effectiveness of finding insights, we use a motivating example drawing from an airline industry for predicting flight delays.},
	address = {New York, NY, USA},
	author = {Teong, Kai Sheng and En Tan, Yong and Shabbir, Mehlam and Chua, Sook-Ling and Foo, Lee Kien},
	booktitle = {Proceedings of the 6th International Conference on Information Technology: IoT and Smart City},
	doi = {10.1145/3301551.3301581},
	isbn = {9781450366298},
	keywords = {flight delays prediction, exploratory data analysis, decision tree, Interactive visualization},
	location = {Hong Kong, Hong Kong},
	numpages = {5},
	pages = {49–53},
	publisher = {Association for Computing Machinery},
	series = {ICIT '18},
	title = {Finding Insights through Interactive Visualization},
	url = {https://doi.org/10.1145/3301551.3301581},
	year = {2018},
}

@inproceedings{10.1145/3307334.3328619,
	abstract = {The VWorld Data Center provides high quality spatial information data with 3D terrains and buildings of major cities in Korea. In this paper, we proposed the visualization method for the city model using VWorld data. Anyone can use through the VWorld 3D map service site. We expect that our platform can be used to visualize 3D spatial information for various applications.},
	address = {New York, NY, USA},
	author = {Lee, Ahyun and Jang, Insung},
	booktitle = {Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services},
	doi = {10.1145/3307334.3328619},
	isbn = {9781450366618},
	keywords = {vworld, gis, 3d map},
	location = {Seoul, Republic of Korea},
	numpages = {1},
	pages = {549},
	publisher = {Association for Computing Machinery},
	series = {MobiSys '19},
	title = {Visualization of City model with VWorld (poster)},
	url = {https://doi.org/10.1145/3307334.3328619},
	year = {2019},
}

@inproceedings{10.1145/3308561.3354625,
	abstract = {Traditional turn-by-turn navigation approaches often do not provide sufficiently detailed information to help people with a visual impairment (PVI) to successfully navigate through an urban environment. To provide PVI with clear and supportive navigation information we created Sidewalk, a new wayfinding message syntax for mobile applications. Sidewalk proposes a consistent structure for detailed wayfinding instructions, short instructions and alerts. We tested Sidewalk with six PVI in the urban center of Amsterdam, the Netherlands. Results show that our approach to wayfinding was positively valued by the participants.},
	address = {New York, NY, USA},
	author = {van der Bie, Joey and Jaschinski, Christina and Ben Allouch, Somaya},
	booktitle = {Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility},
	doi = {10.1145/3308561.3354625},
	isbn = {9781450366762},
	keywords = {wayfinding, visually impaired, spoken messages, smartphone, navigation, natural language, assistive technology},
	location = {Pittsburgh, PA, USA},
	numpages = {3},
	pages = {609–611},
	publisher = {Association for Computing Machinery},
	series = {ASSETS '19},
	title = {Sidewalk, A Wayfinding Message Syntax for People with a Visual Impairment},
	url = {https://doi.org/10.1145/3308561.3354625},
	year = {2019},
}

@article{10.1145/3313799,
	abstract = {Inside a logistic system, actors of the logistics have to interact to manage a coherent flow of goods. They also must deal with the constraints of their environment. The article’s first goal is to study how macro properties (such as global performance) emerge from the dynamic and local behaviors of actors and the structure of the territory. The second goal is to understand which local parameters affect these macro properties. A multi-scale approach made of an agent-based model coupled with dynamic graphs describes the system’s components, including actors and the transportation network. Adaptive behaviors are implemented in this model (with data about the Seine axis) to highlight the system’s dynamics. Agent strategies are evolving according to traffic dynamics and disruptions. This logistic system simulator has the capacity to exhibit large-scale evolution of territorial behavior and efficiency face to various scenarios of local agent behaviors.},
	address = {New York, NY, USA},
	articleno = {15},
	author = {D\'{e}mare, Thibaut and Bertelle, Cyrille and Dutot, Antoine and Fournier, Dominique},
	doi = {10.1145/3313799},
	issn = {1556-4665},
	issue_date = {September 2018},
	journal = {ACM Trans. Auton. Adapt. Syst.},
	keywords = {logistic system, geographical information system, dynamic graph, complex system, adaptive behavior, Agent-based model},
	month = {mar},
	number = {3},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {Adaptive Behavior Modeling in Logistic Systems with Agents and Dynamic Graphs},
	url = {https://doi.org/10.1145/3313799},
	volume = {13},
	year = {2019},
}

@inproceedings{10.1145/3313950.3314187,
	abstract = {The paper presents a new design of a three-axis gravimeter of aviation gravimetric system, which provides compensation for errors caused by influence of mobile base vertical accelerations and in the result of measurement of full gravity acceleration vector. Angle of inclination of a mark that is applied to a gravimeter body and coincides with vertical sensitive axis direction is determined by linear approximation in digital video images. These data are used to point sensitive axes and improve gravimeter accuracy.},
	address = {New York, NY, USA},
	author = {Korobiichuk, Igor and Podchashinskiy, Yuriy and Bezvesilna, Olena and Nechay, Sergiy and Shavurskiy, Yuriy},
	booktitle = {Proceedings of the 2nd International Conference on Image and Graphics Processing},
	doi = {10.1145/3313950.3314187},
	isbn = {9781450360920},
	keywords = {three-axis gravimeter, pointing of sensitive axis, linear approximation, digital video image},
	location = {Singapore, Singapore},
	numpages = {5},
	pages = {89–93},
	publisher = {Association for Computing Machinery},
	series = {ICIGP '19},
	title = {Three-coordinate gravimeter with exhibition of axis sensitivity based on digital videoimages},
	url = {https://doi.org/10.1145/3313950.3314187},
	year = {2019},
}

@inproceedings{10.1145/3314183.3323862,
	abstract = {Thematic maps, traditionally developed to present specific themes within defined geographical areas, are an interesting information presentation model for Cultural Heritage exploration because of the abstract view on the territory they provide. However, in order to cope with possibly heterogeneous user interests, they should be adapted to the individual user by including the relevant types of information, given her/his specific interests. In a previous paper, we proposed an approach to the integration of thematic maps in the OnToMap Participatory GIS (Geographic Information System), in order to support query expansion during an exploratory search task. The proposed maps were built on the basis of a survey in which we asked people to rate the relevance of a set of concepts to five main themes around which we developed the maps. In this paper we go one step forward and we propose a more general approach to information search support in order to automatically create thematic maps, based on the analysis of frequently co-occurring search interests in a search engine query log. This type of analysis supports the identification of clusters of concepts that people frequently search within the same sessions and helps the identification of co-occurring topics that can be proposed to users when exploring an information space. In this way, when the user browses a catalog of Cultural Heritage information, (s)he can both visualize the thematic maps relevant to the search context, and be guided in the navigation within types of information, looking for possibly complementary types of data to satisfy her/his needs.},
	address = {New York, NY, USA},
	author = {Mauro, Noemi},
	booktitle = {Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization},
	doi = {10.1145/3314183.3323862},
	isbn = {9781450367110},
	keywords = {thematic maps, session-based concept suggestion, personalization, information search},
	location = {Larnaca, Cyprus},
	numpages = {6},
	pages = {371–376},
	publisher = {Association for Computing Machinery},
	series = {UMAP'19 Adjunct},
	title = {Supporting the Exploration of Cultural Heritage Information via Search Behavior Analysis},
	url = {https://doi.org/10.1145/3314183.3323862},
	year = {2019},
}

@article{10.1145/3314402,
	abstract = {Protecting citizens' lives from emergent accidents (e.g. traffic accidents) and diseases (e.g. heart attack) is of vital importance in urban computing. Every day many people are caught in emergent accidents or diseases and thus need ambulances to transport them to hospitals. In this paper, we propose a dynamic ambulance redeployment system to reduce the time needed for ambulances to pick up patients and to increase the probability of patients being saved in time. For patients in danger, every second counts. Specifically, whenever there is an ambulance becoming available (e.g. finishing transporting a patient to a hospital), our dynamic ambulance redeployment system will redeploy it to a proper ambulance station such that it can better pick up future patients. However, the dynamic ambulance redeployment is challenging, as when we redeploy an available ambulance we need to simultaneously consider each station's multiple dynamic factors. To trade off these multiple factors using handcrafted rules are almost impossible. To deal with this issue, we propose using a deep neural network, called deep score network, to balance each station's dynamic factors into one score, leveraging the excellent representation ability of deep neural networks. And then we propose a deep reinforcement learning framework to learn the deep score network. Finally, based on the learned deep score network, we provide an effective dynamic ambulance redeployment algorithm. Experiment results using data collected in real world show clear advantages of our method over baselines, e.g. comparing with baselines, our method can save ~100 seconds (~20\%) of average pickup time of patients and improve the ratio of patients being picked up within 10 minutes from 0.786 to 0.838. With our method, people in danger can be better saved.},
	address = {New York, NY, USA},
	articleno = {15},
	author = {Ji, Shenggong and Zheng, Yu and Wang, Zhaoyuan and Li, Tianrui},
	doi = {10.1145/3314402},
	issue_date = {March 2019},
	journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
	keywords = {urban computing, dynamic redeployment system for mobile ambulances, deep score network, deep reinforcement learning, Mobile computing},
	month = {mar},
	number = {1},
	numpages = {20},
	publisher = {Association for Computing Machinery},
	title = {A Deep Reinforcement Learning-Enabled Dynamic Redeployment System for Mobile Ambulances},
	url = {https://doi.org/10.1145/3314402},
	volume = {3},
	year = {2019},
}

@inproceedings{10.1145/3316287.3316293,
	abstract = {Mapathons and hackathons are short-lived events with different purposes. A mapathon is a collaborative effort for collecting geographic data in unmapped areas, while hackathons are focused on application development. Mapathon outputs need to be high quality to be reusable, but often, when applications are later built on top of map data, there is a mismatch between data collected and application requirements. We conducted an international collaboration project aiming to address this situation by creating a circular process where geographic information is collected in a mapathon and later used in a hackathon. Based on user feedback this cycle can be repeated so the collected data and developed applications can be improved. In this event report, we describe the two mapathon-hackathon cycles that were part of our pilot to validate that process. We present their outcomes and some lessons learned. We focused on the so-called "blue economy" (i.e. the sustainable use of marine and ocean resources for economic growth and improved livelihoods in coastal areas.) as the target domain for this pilot. Data for carefully selected areas of the South African coast was collected through in mapathons and later use in hackathons. The mapathons were held in South Africa, and the hackathons took place in Brazil.},
	address = {New York, NY, USA},
	articleno = {9},
	author = {Gama, Kiev and Rautenbach, Victoria and Green, Cameron and Gon\c{c}alves, Breno Alencar and Coetzee, Serena and Fourie, Nicolene and Sastry, Nishanth},
	booktitle = {Proceedings of the International Conference on Game Jams, Hackathons and Game Creation Events 2019},
	doi = {10.1145/3316287.3316293},
	isbn = {9781450362054},
	keywords = {Volunteer Geographic Information, Mapathon, Hackathon},
	location = {San Francisco, CA, USA},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {ICGJ '19},
	title = {Mapathons and Hackathons to Crowdsource the Generation and Usage of Geographic Data},
	url = {https://doi.org/10.1145/3316287.3316293},
	year = {2019},
}

@inproceedings{10.1145/3316782.3316787,
	abstract = {Road network map is one of the datasets that are used in many different applications. Many smart cities have more than one Road Network map from different sources (government authorities, private enterprise, or volunteered). Be that as it may, there is a high chance of mismatches between road maps that represent the same area for different reasons. These reasons include: one of the datasets is not updated; datasets have different names for the same road; and so on. As a result, matching the roads in such datasets with each other is challenging. This paper introduces a framework that demonstrates methods of how two datasets for the same area can be matched to each other even though there are some data discrepancies. In addition, it gives an overview of each component of the framework and focuses mainly on the similarity measurements. These measurements are local divergence measurements and global divergence measurement. Local divergence measurements compare two roads from different datasets to each other to see if they are similar or not by deciding if these two roads have a similar shape as well as the same length. On the other hand, global divergence measurement is used in order to ensure that these two roads are similar in the real world, not different roads that happen to be beside each other having similar length and shape. This paper discusses several types of applications that could utilize this framework not only for matching different road maps and unify the information for smart cities usages but also data enrichment and being up-to-date.},
	address = {New York, NY, USA},
	author = {Almotairi, Mousa and Alsahfi, Tariq and Elmasri, Ramez},
	booktitle = {Proceedings of the 12th ACM International Conference on PErvasive Technologies Related to Assistive Environments},
	doi = {10.1145/3316782.3316787},
	isbn = {9781450362320},
	keywords = {spatial databases, smart cities, road similarity, road networks, road divergence, hausdorff distance, data matching},
	location = {Rhodes, Greece},
	numpages = {8},
	pages = {164–171},
	publisher = {Association for Computing Machinery},
	series = {PETRA '19},
	title = {Challenges of comparing and matching roads from different spatial datasets},
	url = {https://doi.org/10.1145/3316782.3316787},
	year = {2019},
}

@inproceedings{10.1145/3318236.3318238,
	abstract = {National land cover data of China at scale of 1 to 250,000, Digital Elevation Model (DEM) at resolution of 1km by 1km and county-level administrative division data were used as basic data to explore the relationship between spatial distribution patterns of land cover and terrain factors. Firstly, county-level administrative units were used as samples for statistical analysis and multiple linear regression analysis as method to establish quantitative models between area ratio of land cover and main topographical factors at national and provincial scales respectively. Secondly, methodology that the models were applied to estimate area proportion of land cover on grid cell scale was explored with forest as an example. Regression analysis results showed that area proportion of five of the six categories of land cover was significantly correlated with main terrain factors. The quantitative models between area proportion of land cover and terrain factors based on county-level statistical unit (macro-scale) might absolutely be used to estimate area proportion of land cover at grid cell (micro-scale). Although there existed some differences between estimated results and actual situation, the overall trend between the two remained the same.},
	address = {New York, NY, USA},
	author = {Liao, Shunbao and Wang, Keli and Jiang, Xiao},
	booktitle = {Proceedings of the 2019 2nd International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3318236.3318238},
	isbn = {9781450362450},
	keywords = {terrain factors, spatial pattern, land cover, classification of land cover, DEM},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {71–75},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '19},
	title = {Establishment and Preliminary Application of Relationship between Land Cover and Terrain Factors},
	url = {https://doi.org/10.1145/3318236.3318238},
	year = {2019},
}

@inproceedings{10.1145/3318236.3318240,
	abstract = {A spatio-temporal analysis system is becoming critical in many disciplinaries to acquire, analyze, and visualize data. However, conducting spatio-temporal analysis often requires certain levels of domain knowledge and experience: on one hand, sophisticated and domain-specific software design makes the analysis difficult for public users; on the other hand, conveying findings from the analysis could result in ineffectiveness and inefficiencies. In this work, we present a Natural Language Processing (NLP)-enabled Question Answering (QA) framework for spatio-temporal analysis and visualization. It allows users to conduct spatio-temporal analysis by speaking or typing questions. Interactive visualization component in the framework creates better communication between insights and users. We use a dataset from the domain of climate science as a case study to demonstrate the framework. The case study is evaluated through a mid-size software company, and great feedback was received. With the microservice architecture in it, the framework is general enough to be applied in a variety of applications and domains.},
	address = {New York, NY, USA},
	author = {Yin, Zhengcong and Zhang, Chong and Goldberg, Daniel W. and Prasad, Sathya},
	booktitle = {Proceedings of the 2019 2nd International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3318236.3318240},
	isbn = {9781450362450},
	keywords = {Visualization, Spatio-temporal Analysis, Question Answering, Microservices, GIS},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {61–65},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '19},
	title = {An NLP-based Question Answering Framework for Spatio-Temporal Analysis and Visualization},
	url = {https://doi.org/10.1145/3318236.3318240},
	year = {2019},
}

@inproceedings{10.1145/3318236.3318264,
	abstract = {Currently, there are rapid changes in the supermarket sector, as more and more companies provide home delivery services of most of their products. Recently, perishable groceries are also directly shipped to people's home, which leads to new logistical challenges. By only using statistical data known as location factors, problems arose for metropolises like Berlin. These cities have several millions of inhabitants, but are only modeled as one geographic entity with a single attributes like average net income. In order to provide insights into cities down to street level, this geodata model was enriched with information of OpenStreetMap (OSM). Additionally, the road network of OSM was used to dynamically calculate catchment areas and combine them with location factors of the geodata model. This combination enables dynamic and data-driven customer, competitor, and supplier analysis. The evaluation was performed on the application scenario of online food delivery models of Edeka, Rewe and Amazon-Fresh. The results indicate, that Edeka's delivery model-currently evaluated by Edeka in Berlin-only reaches 0,28 \% of the inhabitants, whereas Amazon fresh achieves nearly 100 \% coverage. As a conclusion, the presented data-driven business analysis enables new possibilities for site selection and potential for future applications.},
	address = {New York, NY, USA},
	author = {Baumbach, Sebastian and Rubel, Christoph and Ahmed, Sheraz and Dengel, Andreas},
	booktitle = {Proceedings of the 2019 2nd International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3318236.3318264},
	isbn = {9781450362450},
	keywords = {Spatial Decision Support Systems, Spatial Data Modeling, OpenStreetMap, Geospatial Data, GIS},
	location = {Prague, Czech Republic},
	numpages = {5},
	pages = {110–114},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '19},
	title = {Geospatial Customer, Competitor and Supplier Analysis for Site Selection of Supermarkets},
	url = {https://doi.org/10.1145/3318236.3318264},
	year = {2019},
}

@inproceedings{10.1145/3318396.3318436,
	abstract = {In today's digital era, most businesses occur in geographical locations and their use of geographical information systems (GIS) and advanced technology become crucial for their business growth. Consequently, creation and use of geospatial data has been rapidly growing with increasing number of satellites that enable communication, earth monitoring, and web-enabled sensor networks. Many industries including oil, minerals, agriculture commodities, and banking are using substantially geospatial data in their businesses.It is becoming essential for entrepreneurs and business executives to have an ability to represent, understand and interpret geospatial data in map-like projections, and to make decisions based on the geographic information. As a result, demand for business graduates who understand geographical information systems has gradually increased over the time. To address this need, business schools have started incorporating GIS into their academic programs and curricula. These programs offer geospatial technology that aims to help students in preparing for job markets and support them in gaining knowledge and spatial perspective of business.The research highlights the role of GIS in business development and a strong need for business graduates with GIS specialization. This study investigates how business schools can integrate GIS into their academic programs in order to ensure that business graduates have opportunity to develop GIS knowledge and meet the demand from companies. It also suggests to introduce GIS in the business programs at the higher educational institutions in Mongolia and provides potential integration strategies for business schools in the country. In addition, opportunities and challenges are listed for potential research and studies in this area.},
	address = {New York, NY, USA},
	author = {Chimgee, D. and Bolor, A. and Enerel, A. and Erdenechimeg, J. and Oyunsuren, S.},
	booktitle = {Proceedings of the 2019 8th International Conference on Educational and Information Technology},
	doi = {10.1145/3318396.3318436},
	isbn = {9781450362672},
	keywords = {geospatial data, geographical information systems, business school, business curricula, GIS},
	location = {Cambridge, United Kingdom},
	numpages = {5},
	pages = {217–221},
	publisher = {Association for Computing Machinery},
	series = {ICEIT 2019},
	title = {Integrating GIS into Business School Curricula},
	url = {https://doi.org/10.1145/3318396.3318436},
	year = {2019},
}

@inproceedings{10.1145/3318464.3389718,
	abstract = {Given two vertices of interest (POIs) s and t on a spatial network, a distance (path) query returns the shortest network distance (shortest path) from s to t. This query has a variety of applications in practice and is a fundamental operation for many database and data mining algorithms. In this paper, we propose an efficient distance and path oracle on dynamic road networks using the randomization technique. Our oracle has a good performance in practice and remarkably, and at the same time, it has a favorable theoretical bound. Specifically, it has O(n log2 n) (resp. O(n log2n)) preprocessing time (resp. space) and O(log4n log log n) (resp. O(log4n log log n+l)) distance query time (resp. shortest path query time) as well as O(log3n) update time with high probability (w.h.p.), where n is the number of vertices in the spatial network and l is the number of edges on the shortest path. Our experiments show that the existing oracles suffer from a huge updating time that renders them impractical and our oracle enjoys a negligible updating time and meanwhile has comparable query time and indexing cost with the best existing oracle.},
	address = {New York, NY, USA},
	author = {Wei, Victor Junqiu and Wong, Raymond Chi-Wing and Long, Cheng},
	booktitle = {Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
	doi = {10.1145/3318464.3389718},
	isbn = {9781450367356},
	keywords = {shortest distance, dynamic road networks},
	location = {Portland, OR, USA},
	numpages = {16},
	pages = {1841–1856},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '20},
	title = {Architecture-Intact Oracle for Fastest Path and Time Queries on Dynamic Spatial Networks},
	url = {https://doi.org/10.1145/3318464.3389718},
	year = {2020},
}

@inproceedings{10.1145/3321619.3321651,
	abstract = {A rapid growth of the older population all around the world is being a significant number now a day. Public transport is an essential component of most people's lives. Good mobility and decent transportation alternatives are important in enabling the older population to participate in social interaction and daily activities. The most important concern inaccessibility is the relatively short distance and high-frequency movements from a permanent home. The most common complaint from those aged 65 and over was that public transport is not convenient and does not go where they want. The accessibility is a measure to identify transport service quality. A number of studies have been published to analyze accessibility indices, This study focuses on public transport accessibility for the elderly. This research uses datasets available from different sources to analyses the elderly travel pattern, travel time \&amp; travel duration on the Melbourne statistical area (SA1). This study analyses data from the Victorian Integrated Survey of Travel and Activity (VISTA), 2012. This study focuses to discuss various indices for public transport accessibility. This paper identifies index for elderly public transport accessibility.},
	address = {New York, NY, USA},
	author = {Fatima, Kaniz and Moridpour, Sara and Saghapour, Tayebeh and De Gruyter, Chris},
	booktitle = {Proceedings of the Asia-Pacific Conference on Intelligent Medical 2018 \&amp; International Conference on Transportation and Traffic Engineering 2018},
	doi = {10.1145/3321619.3321651},
	isbn = {9781450366045},
	keywords = {Mobility, GIS, Elderly travel, Ageing in place, Active Transport, Accessibility Index},
	location = {Beijing, China},
	numpages = {5},
	pages = {253–257},
	publisher = {Association for Computing Machinery},
	series = {APCIM \&amp; ICTTE 2018},
	title = {A Case Study of Elderly Public Transport Accessibility},
	url = {https://doi.org/10.1145/3321619.3321651},
	year = {2018},
}

@inproceedings{10.1145/3322905.3322921,
	abstract = {Historic itinerary research investigates the traveling paths of historic entities, to determine their influence and reach. A potential source of such information are the Regesta Imperii (RI), a large-scale resource for European medieval history research. However, two important intermediate problems must be addressed: 1. place names may be stated as unknown or are left empty; 2., place name queries return large candidate sets of points scattered all across Europe and the correct point must be selected. For 1., we perform a place name completion step to predict place names for regests referencing charters of unknown origin. To address 2., we formulate a graph framework which allows efficient reconstruction of the emperors' itineraries by means of shortest path finding algorithms. Our experiments show that our method predicts coordinates of places with significant correlation to human gold coordinates and significantly outperforms a baseline which selects points randomly from the candidate sets. We further show that the method can be leveraged to detect errors in human coordinate labels of place names.},
	address = {New York, NY, USA},
	author = {Opitz, Juri and Born, Leo and Nastase, Vivi and Pultar, Yannick},
	booktitle = {Proceedings of the 3rd International Conference on Digital Access to Textual Cultural Heritage},
	doi = {10.1145/3322905.3322921},
	isbn = {9781450371940},
	keywords = {place name prediction, coordinate prediction, Historic Itineraries},
	location = {Brussels, Belgium},
	numpages = {6},
	pages = {39–44},
	publisher = {Association for Computing Machinery},
	series = {DATeCH2019},
	title = {Automatic Reconstruction of Emperor Itineraries from the Regesta Imperii},
	url = {https://doi.org/10.1145/3322905.3322921},
	year = {2019},
}

@inproceedings{10.1145/3323503.3349542,
	abstract = {Location-Based Services (LBS) are part of our day-to-day life, assisting in a variety of tasks such as tracing better routes to a destination, traffic verification and even for safety applications such as car or people tracking. However, it may not possible visually to identify some activities like attempted theft or kidnapping in the same way that a surveillance camera with visual information. It was identified that the integration of these two information (Global Position and Video) could improve surveillance systems from a security perspective. IP cameras are common examples of multimedia sensors that have become an increasingly popular and ubiquitously installed item in urban centers applied to security. This paper proposes a location-based architecture for video stream selection that can be used to monitor entities (people or not) in a context of Internet of Multimedia Things (IoMT) through a representational model of coverage areas of IP cameras. A prototype was implemented as a proof of concept of the proposed architecture. This prototype includes a low latency video transmission service to quickly start a video transmission only when an entity is inside a visual field of view. Tests were performed on the Internet to check for video delivery latency, bandwidth consumption, and the results suggest the effectiveness of the proposal.},
	address = {New York, NY, USA},
	author = {de Castro Perdomo, Diogo and Viterbo, Jos\'{e} and Saade, D\'{e}bora Christina Muchaluat},
	booktitle = {Proceedings of the 25th Brazillian Symposium on Multimedia and the Web},
	doi = {10.1145/3323503.3349542},
	isbn = {9781450367639},
	keywords = {tracking, coverage area, LBS, IoMT, IP cameras, GPS},
	location = {Rio de Janeiro, Brazil},
	numpages = {8},
	pages = {461–468},
	publisher = {Association for Computing Machinery},
	series = {WebMedia '19},
	title = {A location-based architecture for video stream selection in the context of IoMT},
	url = {https://doi.org/10.1145/3323503.3349542},
	year = {2019},
}

@inproceedings{10.1145/3323716.3323743,
	abstract = {Agriculture is the main activity of Thailand. Samut Songkhram province has been one of the major agricultural province that producing coconut, lychee and orchard. Water use or water demand of people depends on daily life activities. There are the extension of agriculture, industry, residence, and community which increase water demand. It can cause a problem of water use among urban, industrial and agricultural. The quality and quantity of water is an indicator for measure of the quality and yield of agricultural products. The aim of research is to estimate water use through geographic information system, and remote sensing from Landsat8 satellite. This research considers the main parameters which were; estimation of spatial rainfall runoff and quantity of evaporation, infiltration rate of the soil. To evaluate the amount of soil moisture in order to create soil moisture map for planning the utilization and management of water for agriculture. The classification of land use were to investigate from Landsat8 which shown currently agricultural area. The result showed the soil moisture which distributes in each area especially the agricultural area. This information can be used by agriculturists and related organizations to plan and make the decision of growing appropriate plants according to the soil moisture for the sustainability of the agriculture.},
	address = {New York, NY, USA},
	author = {Phonphan, Walaiporn and Thanakunwutthirot, Manatsanan},
	booktitle = {Proceedings of the 8th International Conference on Informatics, Environment, Energy and Applications},
	doi = {10.1145/3323716.3323743},
	isbn = {9781450361040},
	keywords = {remote sensing, geographic information systems, estimate water use, agricultural},
	location = {Osaka, Japan},
	numpages = {4},
	pages = {59–62},
	publisher = {Association for Computing Machinery},
	series = {IEEA '19},
	title = {Application of remote sensing and GIS for assessing crop water demand},
	url = {https://doi.org/10.1145/3323716.3323743},
	year = {2019},
}

@article{10.1145/3324883,
	abstract = {There are many real world applications that require identifying public movements such as identifying movement corridors in cities and most popular paths. If one is not given user trajectories but rather sporadic location data, such as location-based social network data, finding movement related information becomes difficult. Rather than processing all points in a dataset given a query, a clever approach is to construct a graph, based on user locations, and query this graph to answer questions such as shortest paths, most popular paths, and movement corridors. Shortest path graph is one of the popular graphs. However, the shortest path graph can be inefficient and ineffective for analysing movement data, as it calculates the graph edges considering the shortest paths over all the points in a dataset. Therefore, edge sets resulting from shortest path graphs are usually very restrictive and not suitable for movement analysis because of its global view of the dataset. We propose the stepping stone graph, which calculates the graph considering point pairs rather than all points; the stepping stone graph focuses on possible local movements, making it both efficient and effective for location-based social network related data. We demonstrate its capabilities by applying it in the Location-Based Social Network domain and comparing with the shortest path graph. We also compare its properties to a range of other graphs and demonstrate how stepping stone graph relates to Gabriel graph, relative neighbourhood graph, and Delaunay triangulation.},
	address = {New York, NY, USA},
	articleno = {23},
	author = {Kannangara, Sameera and Tanin, Egemen and Harwood, Aaron and Karunasekera, Shanika},
	doi = {10.1145/3324883},
	issn = {2374-0353},
	issue_date = {December 2019},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Moving objects, graphs, shortest path},
	month = {dec},
	number = {4},
	numpages = {24},
	publisher = {Association for Computing Machinery},
	title = {Stepping Stone Graph: A Graph for Finding Movement Corridors using Sparse Trajectories},
	url = {https://doi.org/10.1145/3324883},
	volume = {5},
	year = {2019},
}

@article{10.1145/3325135,
	abstract = {Effective processing of extremely large volumes of spatial data has led to many organizations employing distributed processing frameworks. Apache Spark is one such open source framework that is enjoying widespread adoption. Within this data space, it is important to note that most of the observational data (i.e., data collected by sensors, either moving or stationary) has a temporal component or timestamp. To perform advanced analytics and gain insights, the temporal component becomes equally important as the spatial and attribute components. In this article, we detail several variants of a spatial join operation that addresses both spatial, temporal, and attribute-based joins. Our spatial join technique differs from other approaches in that it combines spatial, temporal, and attribute predicates in the join operator. In addition, our spatio-temporal join algorithm and implementation differs from others in that it runs in commercial off-the-shelf (COTS) application. The users of this functionality are assumed to be GIS analysts with little if any knowledge of the implementation details of spatio-temporal joins or distributed processing. They are comfortable using simple tools that do not provide the ability to tweak the configuration of the algorithm or processing environment. The spatio-temporal join algorithm behind the tool must always succeed, regardless of input data parameters (e.g., it can be highly irregularly distributed, contain large numbers of coincident points, it can be extremely large, etc.). These factors combine to place additional requirements on the algorithm that are uncommonly found in the traditional research environment. Our spatio-temporal join algorithm was shipped as part of the GeoAnalytics Server [12], part of the ArcGIS Enterprise platform from version 10.5 onward.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Whitman, Randall T. and Marsh, Bryan G. and Park, Michael B. and Hoel, Erik G.},
	doi = {10.1145/3325135},
	issn = {2374-0353},
	issue_date = {March 2019},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {spatio-temporal join, geospatial and spatiotemporal databases, distributed processing, Spatial join, Spark, Hadoop, HDFS},
	month = {jun},
	number = {1},
	numpages = {28},
	publisher = {Association for Computing Machinery},
	title = {Distributed Spatial and Spatio-Temporal Join on Apache Spark},
	url = {https://doi.org/10.1145/3325135},
	volume = {5},
	year = {2019},
}

@article{10.1145/3325913,
	abstract = {There are thousands of road closures and changed traffic rules that impact vehicle routing every day. Detecting the road closures and traffic rule changes is essential for dynamic route planning and navigation serving. In this article, we propose a driving-behavior modeling-based method for accurately and effectively detecting the road anomalies. In the first step, we detect the areas of anomalies by using the deviation between drivers’ actual and expected behaviors. To discover the cause of anomalies, we explore the drivers’ short-term destination and find the crucial link pairs in anomalous areas through a novel optimized link entanglement search algorithm, namely, the Select Link Entanglements (SELES) algorithm. Finally, we analyze the crowd's driving patterns to explain the road network anomalies further. Experiments on a very large GPS dataset demonstrate that the proposed approach outperforms the existing methods in terms of both accuracy and effectiveness.},
	address = {New York, NY, USA},
	articleno = {11},
	author = {Wang, Haiquan and Li, Yilin and Liu, Guoping and Wen, Xiang and Qie, Xiaohu},
	doi = {10.1145/3325913},
	issn = {2374-0353},
	issue_date = {June 2019},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {trajectory mining, human mobility, anomaly detection, Intelligent transportation},
	month = {aug},
	number = {2},
	numpages = {17},
	publisher = {Association for Computing Machinery},
	title = {Accurate Detection of Road Network Anomaly by Understanding Crowd's Driving Strategies from Human Mobility},
	url = {https://doi.org/10.1145/3325913},
	volume = {5},
	year = {2019},
}

@article{10.1145/3325915,
	abstract = {In this article, we introduce generalized group trip scheduling (GGTS) queries that enable friends and families to perform activities at different points of interest (POIs), such as a shopping center, a restaurant and a pharmacy with the minimum total travel distance. Trip planning and scheduling for groups, an important class of location-based services (LBSs), have recently received attention from researchers. However, both group trip planning (GTP) and group trip scheduling (GTS) queries have restrictions: a GTP query assumes that all group members visit all required POIs together, whereas a GTS query requires that each POI is visited by a single group member. A GGTS query is more general and allows any number of group members to visit a POI together. We propose an efficient algorithm to evaluate the exact answers for GGTS queries in road networks. Since finding the answer for a GGTS query is an NP-hard problem, to reduce the processing overhead for a large group size or a large number of required POI types or a large POI dataset, we propose two heuristic solutions—trip-scheduling heuristic (TSH) and search region refinement heuristic (SRH)—for processing GGTS queries. Extensive experiments with real datasets show that our optimal algorithm is preferable for small parameter settings, and the heuristic solutions reduce the processing overhead significantly in return for sacrificing the accuracy slightly.},
	address = {New York, NY, USA},
	articleno = {10},
	author = {Rayhan, Yeasir and Hashem, Tanzima and Jahan, Roksana and Cheema, Muhammad Aamir},
	doi = {10.1145/3325915},
	issn = {2374-0353},
	issue_date = {June 2019},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {road networks, points of interests, group trip scheduling, generalized group trip scheduling, Group trip planning},
	month = {jul},
	number = {2},
	numpages = {24},
	publisher = {Association for Computing Machinery},
	title = {Efficient Scheduling of Generalized Group Trips in Road Networks},
	url = {https://doi.org/10.1145/3325915},
	volume = {5},
	year = {2019},
}

@article{10.1145/3326060,
	abstract = {A road network is a critical aspect of both urban planning and route recommendation. This article proposes an efficient approach to build a fine-grained road network based on sparsely sampled private car trajectory data under complex urban environment. In order to resolve difficulties introduced by low sampling rate trajectory data, we concentrate sample points around intersections by utilizing the turning characteristics from the large-scale trajectory data to ensure the accuracy of the detection of intersections and road segments. In front of complex road networks including many complex intersections, such as the overpasses and underpasses, we first layer intersections into major and minor one, and then propose a simplified representation of intersections and corresponding computable model based on the features of roads, which can significantly improve the accuracy of detected road networks, especially for the complex intersections. In order to construct fine-grained road networks, we distinguish various types of intersections using direction information and detected turning limit. To the best of our knowledge, our road network building method is the first time to give fine-grained road networks based on low-sampling rate private car trajectory data, especially able to infer the location of complex intersections and its connections to other intersections. Last but not the least, we propose an effective parameter selection process for the Density-Based Spatial Clustering of Applications with Noise based clustering algorithm, which is used to implement the reliable intersection detection. Extensive evaluations are conducted based on a real-world trajectory dataset from 1,345 private cars in Futian district, Shenzhen city of China. The results demonstrate the effectiveness of the proposed method. The constructed road network matches close to the one from a public editing map OpenStreetMap, especially the location of the road intersections and road segments, which achieves 92.2\% intersections within 20m and 91.6\% road segments within 8m.},
	address = {New York, NY, USA},
	articleno = {35},
	author = {Huang, Yourong and Xiao, Zhu and Yu, Xiaoyou and Wang, Dong and Havyarimana, Vincent and Bai, Jing},
	doi = {10.1145/3326060},
	issn = {1556-4681},
	issue_date = {June 2019},
	journal = {ACM Trans. Knowl. Discov. Data},
	keywords = {road networks, private cars, Trajectory data},
	month = {jun},
	number = {3},
	numpages = {28},
	publisher = {Association for Computing Machinery},
	title = {Road Network Construction with Complex Intersections Based on Sparsely Sampled Private Car Trajectory Data},
	url = {https://doi.org/10.1145/3326060},
	volume = {13},
	year = {2019},
}

@inproceedings{10.1145/3328905.3332507,
	abstract = {Complex Event Processing (CEP) is the state-of-the-art technology for continuously monitoring and analyzing streams of events. One of the key features of CEP is to support pattern matching queries to detect user-defined sequences of predicates on event streams. However, due to the vast amount of parameters, tweaking the queries to deliver the desired results is challenging. For this demonstration, we connected a database-backed CEP system (Jepc, ChronicleDB) with a scientific toolbox for interactive data exploration and geo visualization (Vat System), and thus allow users to interactively explore event data via CEP queries. Furthermore, the pairing of these systems allows to combine the results of event queries with additional non-relational data-sets such as raster data, leading to insights beyond pure event-based analytics. In this demonstration, we showcase the first promising results of this combination by evaluating three use cases involving high-volume event data collected from aircrafts in-flight.},
	address = {New York, NY, USA},
	author = {Beilschmidt, Christian and Dr\"{o}nner, Johannes and Glombiewski, Nikolaus and Heigele, Christian and Holznigenkemper, Jana and Isenberg, Anna and K\"{o}rber, Michael and Mattig, Michael and Morgen, Andreas and Seeger, Bernhard},
	booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-Based Systems},
	doi = {10.1145/3328905.3332507},
	isbn = {9781450367943},
	keywords = {spatio-temporal data, geovisualization, complex event processing},
	location = {Darmstadt, Germany},
	numpages = {4},
	pages = {224–227},
	publisher = {Association for Computing Machinery},
	series = {DEBS '19},
	title = {Pretty Fly for a VAT GUI: Visualizing Event Patterns for Flight Data},
	url = {https://doi.org/10.1145/3328905.3332507},
	year = {2019},
}

@inproceedings{10.1145/3330089.3330128,
	abstract = {GIS is becoming a necessity in a wide variety of application domains and the extraction of such geographic information has taken an important part in the computer science field.This thesis has the objective of extracting geographic data from Wikipedia to make it easier for users to obtain the information they want.One problematic aspect is the large volume XML file processing, we try to use text mining and machine learning techniques to solve this problem.In this work, we present and evaluate an approach to extract geographic data from Wikipedia from a very large XML file and create a geographic databae. Our technique is to extract infoboxes from geographic articles using the supervised machine learning (SVM) technique. We create after that tables containing geographic data (name, longitude, latitude ... etc) and we make the joins between different tables that will help us to structure our result.},
	address = {New York, NY, USA},
	articleno = {28},
	author = {Benhaddouche, Djamila and Tekkouk, Mohamed and Youcef, Abdelghani Chernnouf},
	booktitle = {Proceedings of the 7th International Conference on Software Engineering and New Technologies},
	doi = {10.1145/3330089.3330128},
	isbn = {9781450361019},
	keywords = {Wikipedia, Text Mining, Machine Learning},
	location = {Hammamet, Tunisia},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {ICSENT 2018},
	title = {Extracting Geographic Knowledge from Wikipedia},
	url = {https://doi.org/10.1145/3330089.3330128},
	year = {2018},
}

@inproceedings{10.1145/3330089.3330469,
	abstract = {Since 1917, the population of the city of El Jadida (formerly Mazagan) has continued to increase remarkably especially in specific periods. This could be explained by deterministic factors including the rural exodus, the port, the industrial center of Jorf Lasfar, the university complex Chouaib Doukkali, the train and bus station and the provincial hospital. These factors are referred to in our work as centers of attraction.This increase has generated an important urban sprawl. The question that arises: Is this expansion due to arbitrary effects or based on scientific methods?After reviewed literature, it has been noted out that there are several methods and mathematical models governing territorial development, like the cell model applied to the Mexican town TIJUANA by Michel Le Page.In this research, we propose a simulation with ArcGIS to determine the evolution of the urban occupation of the city of El Jadida.To this end, we have used satellite images from the period 1982 to 2018. These images have undergone subsequent treatments to identify urban areas.Analysis of the results showed that evolution is made to the South (To the industrial area Jorf Lasfar) and to the South West (To the university complex).},
	address = {New York, NY, USA},
	articleno = {34},
	author = {Saadani, Said and Aaroud, Abdessadek and Naim, Ayman},
	booktitle = {Proceedings of the 7th International Conference on Software Engineering and New Technologies},
	doi = {10.1145/3330089.3330469},
	isbn = {9781450361019},
	keywords = {Urban expansion, Urban Sprawl, Spreading, Land Planning, Deep Learning, Attraction Centers, ArcGIS},
	location = {Hammamet, Tunisia},
	numpages = {3},
	publisher = {Association for Computing Machinery},
	series = {ICSENT 2018},
	title = {Simulation of the Urban Expansion of the El Jadida City},
	url = {https://doi.org/10.1145/3330089.3330469},
	year = {2018},
}

@inproceedings{10.1145/3330530.3332300,
	abstract = {Landform elements (e.g. ridge, valley) are the basic morphologic types of earth's surface. All types of geomorphy are formed by different spatial combination of landform elements, so landform elements are the foundation of studying geomorphy. Using a pre-assigned set of topographic attributes derived from the digital elevation model (DEM), most of existing classification methods of landform elements are likely to encounter a problem that local topographic attributes can not catch the terrain context well in the process of landform elements mapping. To solve the problem, we improve the Geomorphons classification method and propose a new classification method of landform elements based on Douglas-Peucker algorithm for the multi-scale morphology in this paper. The proposed method effectively overcomes the problem of the Geomorphons classification method which can not show the multi-scale characteristics of landform elements and thus might result in misclassification in rugged area. The experimental results show that the proposed method can get more reasonable results than that by the Geomorphons classification method.},
	address = {New York, NY, USA},
	author = {Wang, Qisheng and Xin, Kang and Yang, Afeng and Li, Lin and Cheng, Guo},
	booktitle = {Proceedings of the 2019 5th International Conference on Computing and Data Engineering},
	doi = {10.1145/3330530.3332300},
	isbn = {9781450361248},
	keywords = {multi-scale morphology, landform elements, Douglas-Peucker algorithm, DEM},
	location = {Shanghai, China},
	numpages = {5},
	pages = {65–69},
	publisher = {Association for Computing Machinery},
	series = {ICCDE '19},
	title = {A Classification Method of Landform Elements Based on Douglas-Peucker Algorithm for Multi-scale Morphology},
	url = {https://doi.org/10.1145/3330530.3332300},
	year = {2019},
}

@inproceedings{10.1145/3331054.3331552,
	abstract = {Cooperative positioning is considered a key strategy for the improvement of localization and navigation performance in harsh contexts such as urban areas. Modern communication paradigms can support the exchange of inter-vehicle ranges measured from on-board sensors or obtained through Global Satellite Navigation System (GNSS) measurements. The paper presents an overview of the GNSS-only collaborative localization in the context of cooperative connected cars. It provides an experimental example along with new results about the tight integration of collaboratively-generated inter-vehicle relative measurements collected by a target vehicle by means of a double differentiation w.r.t. to a set of five aiding vehicles. An average improvement of the positioning accuracy of about 11\% motivates the research effort towards multi-agent connected positioning systems.},
	address = {New York, NY, USA},
	author = {Minetto, Alex and Nardin, Andrea and Dovis, Fabio},
	booktitle = {Proceedings of the 1st ACM MobiHoc Workshop on Technologies, MOdels, and Protocols for Cooperative Connected Cars},
	doi = {10.1145/3331054.3331552},
	isbn = {9781450368070},
	location = {Catania, Italy},
	numpages = {6},
	pages = {37–42},
	publisher = {Association for Computing Machinery},
	series = {TOP-Cars '19},
	title = {GNSS-only Collaborative Positioning Among Connected Vehicles},
	url = {https://doi.org/10.1145/3331054.3331552},
	year = {2019},
}

@inproceedings{10.1145/3331076.3331101,
	abstract = {The amount of sources and sheer volumes of spatiotemporal data have met an unprecedented growth during the last decade. As a consequence, a rapidly increasing number of applications are seeking to generate value by crunching those data. The development of a system that will tap into the potential value of the spatiotemporal big data analysis for a multitude of applications remains one of the biggest challenges in computer engineering. This paper delves into the key-characteristics of the most prominent suchlike systems. In particular, it provides a thorough analysis of NoSQL datastores as well as a traditional relational database system in terms of their geospatial querying capabilities.},
	address = {New York, NY, USA},
	articleno = {21},
	author = {Makris, Antonios and Tserpes, Konstantinos and Anagnostopoulos, Dimosthenis and Nikolaidou, Mara and de Macedo, Jose Ant\^{o}nio Fernandes},
	booktitle = {Proceedings of the 23rd International Database Applications \&amp; Engineering Symposium},
	doi = {10.1145/3331076.3331101},
	isbn = {9781450362498},
	keywords = {spatio-temporal databases, spatio-temporal characteristics, geospatial functionality, data stores},
	location = {Athens, Greece},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {IDEAS '19},
	title = {Database system comparison based on spatiotemporal functionality},
	url = {https://doi.org/10.1145/3331076.3331101},
	year = {2019},
}

@article{10.1145/3331450,
	abstract = {Pricing in mobility-on-demand (MOD) networks, such as Uber, Lyft, and connected taxicabs, is done adaptively by leveraging the price responsiveness of drivers (supplies) and passengers (demands) to achieve such goals as maximizing drivers’ incomes, improving riders’ experience, and sustaining platform operation. Existing pricing policies only respond to short-term demand fluctuations without accurate trip forecast and spatial demand-supply balancing, thus mismatching drivers to riders and resulting in loss of profit.We propose CAPrice, a novel adaptive pricing scheme for urban MOD networks. It uses a new spatio-temporal deep capsule network (STCapsNet) that accurately predicts ride demands and driver supplies with vectorized neuron capsules while accounting for comprehensive spatio-temporal and external factors. Given accurate perception of zone-to-zone traffic flows in a city, CAPrice formulates a joint optimization problem by considering spatial equilibrium to balance the platform, providing drivers and riders/passengers with proactive pricing “signals.” We have conducted an extensive experimental evaluation upon over 4.0\texttimes{} 108 MOD trips (Uber, Didi Chuxing, and connected taxicabs) in New York City, Beijing, and Chengdu, validating the accuracy, effectiveness, and profitability (often 20\% ride prediction accuracy and 30\% profit improvements over the state-of-the-arts) of CAPrice in managing urban MOD networks.},
	address = {New York, NY, USA},
	articleno = {39},
	author = {He, Suining and Shin, Kang G.},
	doi = {10.1145/3331450},
	issn = {2157-6904},
	issue_date = {July 2019},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {traffic prediction, smart transportation, sharing economy, ride sharing, flow balancing, deep learning, adaptive pricing, Mobility-on-demand},
	month = {jul},
	number = {4},
	numpages = {28},
	publisher = {Association for Computing Machinery},
	title = {Spatio-temporal Adaptive Pricing for Balancing Mobility-on-Demand Networks},
	url = {https://doi.org/10.1145/3331450},
	volume = {10},
	year = {2019},
}

@inproceedings{10.1145/3331453.3360961,
	abstract = {The traditional flood inundation algorithms generally adopt seed filling algorithm along with several improved serial methods to evaluate flooding region. When dealing with huge amount of data, some of these algorithms fail due to large recursive depth, others end up spending overmuch time in obtaining large flooding zone. Due to the dependency of digital terrain data in flooding analysis, few parallel flooding algorithms has been developed to improve the computational efficiency, which fails to make full use of the performance advantages of computer clusters. To solve above problems, this paper proposes a parallel flood inundation algorithm for massive DEM data. The algorithm presents a boundary component method using strip data partition and run length code to generate potential flooding area in parallel, after which it traverses all strips in sequence to gain whole flooding region. The computational efficiency of the proposed algorithm is verified by comparing it with the strip seed filling algorithm over practical DEM data. Meanwhile, the capability to process massive data of the proposed algorithm is proved by experimental results.},
	address = {New York, NY, USA},
	articleno = {53},
	author = {Wu, Xuqiao and Wu, Ye and Chen, Luo and Jing, Ning},
	booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
	doi = {10.1145/3331453.3360961},
	isbn = {9781450362948},
	keywords = {Strip data partition, Run length code, Parallel computing, Massive DEM data, Flood inundation, Boundary component algorithm},
	location = {Sanya, China},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {CSAE '19},
	title = {A Parallel Flood Inundation Algorithm for Massive DEM Data},
	url = {https://doi.org/10.1145/3331453.3360961},
	year = {2019},
}

@inproceedings{10.1145/3331453.3360970,
	abstract = {As an essential part of geographic information retrieval, gazetteer services are closely related to people's life quality. It is important to construct intelligent gazetteer service and improve its retrieval quality. Considering that the accuracy of document matching in large-scale data scenarios is relatively low and it is difficult to obtain the true intention of user queries, we propose a gazetteer retrieval method which takes into account the spatial relationship semantics. Firstly, the multivariable geo-location retrieval pattern of gazetteer service is defined as the basis for constructing the spatial relationship semantic model. Secondly, a measurement formula is given for calculating attribute similarity and spatial relationship similarity. In particular, we propose a similarity measurement approach that reconstructs geographical name entity so as to adapt the characteristics of non-point objects' spatial relationship. Finally, our proposal is evaluated on real-world datasets against traditional methods. The results reveal that the proposed method can not only enrich user's geo-location retrieval pattern, but also improve the effectiveness and quality of gazetteer services under the background of big data.},
	address = {New York, NY, USA},
	articleno = {22},
	author = {Zhu, Xiangdian and Wu, Ye and Chen, Luo and Jing, Ning},
	booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
	doi = {10.1145/3331453.3360970},
	isbn = {9781450362948},
	keywords = {Spatial relationship semantic model, Geo-location retrieval pattern, Gazetteer services},
	location = {Sanya, China},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {CSAE '19},
	title = {Research on Gazetteer Information Retrieval Involving Spatial Relationship Semantics},
	url = {https://doi.org/10.1145/3331453.3360970},
	year = {2019},
}

@inproceedings{10.1145/3331453.3361327,
	abstract = {Viewshed analysis is a fundamental operation of most geographic information system software. The last decade has witnessed an explosion in the availability of terrain data with high resolutions. Hence, it is essential to develop efficient viewshed analysis system to process these data. This paper demonstrates the design and engineering of an interactive online viewshed analysis system named HiViewshed on massive grid terrains. HiViewshed takes advantage of hybrid parallel computing architectures with message-based and shared-memory parallelism, and it has overcome problems and deficiencies of the existing systems. The main features of the system include (a) MPI/OpenMP based parallel computing of total viewshed analysis for multiple observation points in a large region, (b) parallel DEM data access with multi-resolution strategy ensuring online analysis quality with limited bandwidth and computing resources, and (c) favorable parallel acceleration and scalability in cluster architecture. Experimental results have shown that, it can respond in an acceptable time with several hundreds of observation points simultaneously.},
	address = {New York, NY, USA},
	articleno = {62},
	author = {Wu, Ye and Ma, Mengyu and Chen, Luo},
	booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
	doi = {10.1145/3331453.3361327},
	isbn = {9781450362948},
	keywords = {Viewshed analysis, Parallel computing, Hybrid parallelism, Grid},
	location = {Sanya, China},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {CSAE '19},
	title = {HiVewshed: An Interactive Online Viewshed Analysis System for Multiple Observers},
	url = {https://doi.org/10.1145/3331453.3361327},
	year = {2019},
}

@inproceedings{10.1145/3331453.3361652,
	abstract = {The relevance of geographic entities has always been the focus of research on geographic information retrieval, geographic knowledge graph and recommendation systems. Traditional research methods, which use spatial or semantic similarity to calculate the correlation between regions, have certain one-sidedness and limitations. The network topology can clearly represent the relationship between entities. However, semantic relationships are difficult to define, so there are few cases where network-related algorithms are used to solve the relevance of geographic entities. With the development of the Internet, web pages provide people with a huge amount of information, and geographical names as a key element are often ignored by researchers, and the rich semantic information contained in it needs further research. This study attempts to explore the geographic entity relevance of integrated semantics and spatial factors based on textual data from a network perspective. Based on the community mining algorithm, the experiment studies the aggregation characteristics of geographic entities and can find areas that are close to each other and tightly related, which is more satisfied with people's common sense.},
	address = {New York, NY, USA},
	articleno = {92},
	author = {Yan, Mengyu and Jing, Ning and Zhong, Zhinong and Wu, Ye},
	booktitle = {Proceedings of the 3rd International Conference on Computer Science and Application Engineering},
	doi = {10.1145/3331453.3361652},
	isbn = {9781450362948},
	keywords = {Community mining, Geographic entities, Louvain Algorithm, Semantic similarity, Spatial similarity},
	location = {Sanya, China},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {CSAE '19},
	title = {Geographical Entity Community Mining Based on Spatial and Semantic Association},
	url = {https://doi.org/10.1145/3331453.3361652},
	year = {2019},
}

@inproceedings{10.1145/3332186.3333163,
	abstract = {As the emergent technology trends are progressing towards extreme-scale HPC coupled with data intensive analytics, the challenge of storing and analyzing data is significantly greater than we are observing currently. Given the vast volume of data generated by various simulation and experiments, the process of successfully analyzing and validating diverse sets of research phenomena mandates execution of various complex queries, the correlation of the disparate data retrieved followed by generating visualizations facilitating meaningful insights of both the simulated processes and the experimental phenomena. This process of data discovery, analysis, and visualization requires execution of several phases by the researchers. Therefore, managing and generating data visualizations is a major bottleneck in the research discovery process that degrades not only the ability to mine the data but also limits the utilization of scientific data sets. An important challenge in exploring geospatial data is visualizing different scales ranging from micrometers to kilometers. The purpose of this study is to develop and implement a framework for storing, analyzing and visualizing geological datasets at different scales, with the goal of improving the scientific discovery process and reducing the time to insight in the geological domain along with hiding the intricacies of the underlying low-level complex hardware platforms from the end-user.},
	address = {New York, NY, USA},
	articleno = {105},
	author = {Sukhija, Nitin and Sevin, Sonny and Walters, Jordan and Zieg, Michael},
	booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
	doi = {10.1145/3332186.3333163},
	isbn = {9781450372275},
	keywords = {iRODS, High Performance Computing, Geospatial, Gateway, Data Grid, Big Data},
	location = {Chicago, IL, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {PEARC '19},
	title = {Geo-Grid: Interactive Visualization of Geological Datasets},
	url = {https://doi.org/10.1145/3332186.3333163},
	year = {2019},
}

@inproceedings{10.1145/3332186.3333266,
	abstract = {We are in the era of Spatial Big Data. Due to the developments of topographic techniques, clear satellite imagery, and various means for collecting information, geospatial datasets are growing in volume, complexity and heterogeneity. For example, OpenStreetMap data for the whole world is about 1 TB and NASA world climate datasets are about 17 TB. Spatial data volume and variety makes spatial computations both data-intensive and compute-intensive. Due to the irregular distribution of spatial data, domain decomposition becomes challenging. In this work, we present spatial data partitioning technique that takes into account spatial join cost. In addition, we present spatial join computation using Asynchronous Dynamic Load Balancing (ADLB) library. ADLB is a software library designed to help rapidly build scalable parallel programs using MPI. We evaluated the performance of ADLB-based MPI-GIS implementation. In our existing work, spatial data movement cost from ADLB server to worker MPI processes limited the scalability of MPI-GIS.},
	address = {New York, NY, USA},
	articleno = {121},
	author = {Yang, Jie and Paudel, Anmol and Puri, Satish},
	booktitle = {Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (Learning)},
	doi = {10.1145/3332186.3333266},
	isbn = {9781450372275},
	keywords = {HPC, Message Passing Interface, Parallel IO, Spatial Data, Spatial Join},
	location = {Chicago, IL, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {PEARC '19},
	title = {Spatial Data Decomposition and Load Balancing on HPC Platforms},
	url = {https://doi.org/10.1145/3332186.3333266},
	year = {2019},
}

@inproceedings{10.1145/3335656.3335696,
	abstract = {The popularity of mobile internet accelerates the dissemination and communication of information and also changes the way tourists obtain information. Tourists no longer rely on the officially published travel brochures and TV programs to obtain tourism information. Through Twitter, Sina Weibo, Facebook and other We-Media channels, tourists can get first-hand information about the tourist destination. A large number of GPS trajectory data, such as taxi trajectory data and mobile signaling data, are generated through the widely existing GPS sensors and have been widely used in traffic and resident travel research. Since tourists are not familiar with the road distribution and traffic rules of the destination city, taxi car is an important travel method for non-local tourists to choose, and its OD(origin-destination) points reflect the travel needs and travel characteristics of tourists. Therefore, this paper applies the taxi data to the tourism research. In our study, CFSDPF clustering algorithm is adopted to cluster Sina Weibo data to form tourism ROI (region of interest), and the tourism ROI is used to cluster taxi OD data. The travel characteristics of tourists can be fully and accurately reflected through multi-source data. From two different scales of citywide and central city, we can comprehensively analyze the relationship between the travel characteristics of tourists in chengdu and the tourism ROI.},
	address = {New York, NY, USA},
	author = {Yuan, Rongzheng},
	booktitle = {Proceedings of the 2019 International Conference on Data Mining and Machine Learning},
	doi = {10.1145/3335656.3335696},
	isbn = {9781450360906},
	keywords = {Travel Mode, Taxi Trajectory Data, Spatio-Temporal Changes, Sina Weibo Data},
	location = {Hong Kong, Hong Kong},
	numpages = {5},
	pages = {120–124},
	publisher = {Association for Computing Machinery},
	series = {ICDMML 2019},
	title = {Spatio-temporal Changes of Tourists Based on Multi-source data in Chengdu},
	url = {https://doi.org/10.1145/3335656.3335696},
	year = {2019},
}

@article{10.1145/3336144,
	abstract = {Maps in video games have grown into complex interactive systems alongside video games themselves. What map systems have done and currently do have not been cataloged or evaluated. We trace the history of game map interfaces from their paper-based inspiration to their current smart phone-like appearance. Read-only map interfaces enable players to consume maps, which is sufficient for wayfinding. Game cartography interfaces enable players to persistently modify maps, expanding the range of activity to support planning and coordination. We employ thematic analysis on game cartography interfaces, contributing a near-exhaustive catalog of games featuring such interfaces, a set of properties to describe and design such interfaces, a collection of play activities that relate to cartography, and a framework to identify what properties promote the activities. We expect that designers will find the contributions enable them to promote desired play experiences through game map interface design.},
	address = {New York, NY, USA},
	articleno = {30},
	author = {Toups Dugas, Phoebe O. and Lalone, Nicolas and Alharthi, Sultan A. and Sharma, Hitesh Nidhi and Webb, Andrew M.},
	doi = {10.1145/3336144},
	issn = {1073-0516},
	issue_date = {October 2019},
	journal = {ACM Trans. Comput.-Hum. Interact.},
	keywords = {maps, cartography, Game design},
	month = {jul},
	number = {5},
	numpages = {43},
	publisher = {Association for Computing Machinery},
	title = {Making Maps Available for Play: Analyzing the Design of Game Cartography Interfaces},
	url = {https://doi.org/10.1145/3336144},
	volume = {26},
	year = {2019},
}

@inproceedings{10.1145/3336191.3371841,
	abstract = {Recommendation systems tend to suffer severely from the sparse training data. A large portion of users and items usually have a very limited number of training instances. The data sparsity issue prevents us from accurately understanding users' preferences and items' characteristics and jeopardize the recommendation performance eventually. In addition, models, trained with sparse data, lack abundant training supports and tend to be vulnerable to adversarial perturbations, which implies possibly large errors in generalization.In this work, we investigate the recommendation task in the context of prospective customer recommendation in location based social networks. To comprehensively utilize the training data, we explicitly learn to compare users' historical check-in businesses utilizing self-attention mechanisms. To enhance the robustness of a recommender system and improve its generalization performance, we perform adversarial training. Adversarial perturbations are dynamically constructed during training and models are trained to be tolerant of such nuisance perturbations. In a nutshell, we introduce a Self-Attentive prospective Customer RecommendAtion framework, SACRA, which learns to recommend by making comparisons among users' historical check-ins with adversarial training. To evaluate the proposed model, we conduct a series of experiments to extensively compare with 12 existing methods using two real-world datasets. The results demonstrate that SACRA significantly outperforms all baselines.},
	address = {New York, NY, USA},
	author = {Li, Ruirui and Wu, Xian and Wang, Wei},
	booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
	doi = {10.1145/3336191.3371841},
	isbn = {9781450368223},
	keywords = {self-attention, pairwise ranking, customer recommendation, adversarial training},
	location = {Houston, TX, USA},
	numpages = {9},
	pages = {349–357},
	publisher = {Association for Computing Machinery},
	series = {WSDM '20},
	title = {Adversarial Learning to Compare: Self-Attentive Prospective Customer Recommendation in Location based Social Networks},
	url = {https://doi.org/10.1145/3336191.3371841},
	year = {2020},
}

@article{10.1145/3337798,
	abstract = {Class ambiguity refers to the phenomenon whereby similar features correspond to different classes at different locations. Given heterogeneous geographic data with class ambiguity, the spatial ensemble learning (SEL) problem aims to find a decomposition of the geographic area into disjoint zones such that class ambiguity is minimized and a local classifier can be learned in each zone. The problem is important for applications such as land cover mapping from heterogeneous earth observation data with spectral confusion. However, the problem is challenging due to its high computational cost. Related work in ensemble learning either assumes an identical sample distribution (e.g., bagging, boosting, random forest) or decomposes multi-modular input data in the feature vector space (e.g., mixture of experts, multimodal ensemble) and thus cannot effectively minimize class ambiguity. In contrast, we propose a spatial ensemble framework that explicitly partitions input data in geographic space. Our approach first preprocesses data into homogeneous spatial patches and uses a greedy heuristic to allocate pairs of patches with high class ambiguity into different zones. We further extend our spatial ensemble learning framework with spatial dependency between nearby zones based on the spatial autocorrelation effect. Both theoretical analysis and experimental evaluations on two real world wetland mapping datasets show the feasibility of the proposed approach.},
	address = {New York, NY, USA},
	articleno = {43},
	author = {Jiang, Zhe and Sainju, Arpan Man and Li, Yan and Shekhar, Shashi and Knight, Joseph},
	doi = {10.1145/3337798},
	issn = {2157-6904},
	issue_date = {July 2019},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {spatial heterogeneity, spatial ensemble, local models, class ambiguity, Spatial classification},
	month = {aug},
	number = {4},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {Spatial Ensemble Learning for Heterogeneous Geographic Data with Class Ambiguity},
	url = {https://doi.org/10.1145/3337798},
	volume = {10},
	year = {2019},
}

@inproceedings{10.1145/3338286.3344419,
	abstract = {People with a visual impairment (PVI) often experience difficulties with wayfinding. Current navigation applications have limited communication channels and do not provide detailed enough information to support PVI. By transmitting wayfinding information via multimodal channels and combining these with wearables, we can provide tailored information for wayfinding and reduce the cognitive load. This study presents a framework for multimodal wayfinding communication via smartwatch. The framework consists of four modalities: audio, voice, tactile and visual. Audio and voice messages are transmitted using a bone conduction headphone, keeping the ears free to focus on the environment. With a smartwatch vibrations are directed to a sensitive part of the body (i.e., the wrist), making it easier to sense the vibrations. Icons and short textual feedback are viewed on the display of the watch, allowing for hands-free navigation.},
	address = {New York, NY, USA},
	articleno = {68},
	author = {van der Bie, Joey and Ben Allouch, Somaya and Jaschinski, Christina},
	booktitle = {Proceedings of the 21st International Conference on Human-Computer Interaction with Mobile Devices and Services},
	doi = {10.1145/3338286.3344419},
	isbn = {9781450368254},
	keywords = {Assistive technology, Bone conduction, Multimodal feedback, Smartphone, Smartwatch, Tactile, Ubiquitous computing, Visually impaired, Wayfinding},
	location = {Taipei, Taiwan},
	numpages = {7},
	publisher = {Association for Computing Machinery},
	series = {MobileHCI '19},
	title = {Communicating Multimodal Wayfinding Messages for Visually Impaired People via Wearables},
	url = {https://doi.org/10.1145/3338286.3344419},
	year = {2019},
}

@inproceedings{10.1145/3338906.3340454,
	abstract = {Data is arguably the most valuable asset of the modern world. In this era, the success of any data-intensive solution relies on the quality of data that drives it. Among vast amount of data that are captured, managed, and analyzed everyday, geospatial data are one of the most interesting class of data that hold geographical information of real-world phenomena and can be visualized as digital maps. Geo-spatial data is the source of many enterprise solutions that provide local information and insights. Companies often aggregate geospacial datasets from various sources in order to increase the quality of such solutions. However, a lack of a global standard model for geospatial datasets makes the task of merging and integrating datasets difficult and error prone. Traditionally, this aggregation was accomplished by domain experts manually validating the data integration process by merging new data sources and/or new versions of previous data against conflicts and other requirement violations. However, this manual approach is not scalable is a hinder toward rapid release when dealing with big datasets which change frequently. Thus more automated approaches with limited interaction with domain experts is required. As a first step to tackle this problem, we have leveraged Information Retrieval (IR) and geospatial search techniques to propose a systematic and automated conflict identification approach. To evaluate our approach, we conduct a case study in which we measure the accuracy of our approach in several real-world scenarios and followed by interviews with Localintel Inc. software developers to get their feedbacks.},
	address = {New York, NY, USA},
	author = {Miryeganeh, Nima and Amoui, Mehdi and Hemmati, Hadi},
	booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	doi = {10.1145/3338906.3340454},
	isbn = {9781450355728},
	keywords = {Data Integration, Data Intensive Software, Geospatial Datasets, Map-based Software, TFIDF, Text Mining},
	location = {Tallinn, Estonia},
	numpages = {9},
	pages = {946–954},
	publisher = {Association for Computing Machinery},
	series = {ESEC/FSE 2019},
	title = {An IR-based approach towards automated integration of geo-spatial datasets in map-based software systems},
	url = {https://doi.org/10.1145/3338906.3340454},
	year = {2019},
}

@inproceedings{10.1145/3340531.3412030,
	abstract = {In the last decade, there has been great progress in the field of machine learning and deep learning. These models have been instrumental in addressing a great number of problems. However, they have struggled when it comes to dealing with high dimensional data. In recent years, representation learning models have proven to be quite efficient in addressing this problem as they are capable of capturing effective lower-dimensional representations of the data. However, most of the existing models are quite ineffective when it comes to dealing with high dimensional spatiotemporal data as they encapsulate complex spatial and temporal relationships that exist among real-world objects. High-dimensional spatiotemporal data of cities represent urban communities. By learning their social structure we can better quantitatively depict them and understand factors influencing rapid growth, expansion, and changes.In this paper, we propose a collective embedding framework that leverages the use of auto-encoders and Laplacian score to learn effective embeddings of spatiotemporal networks of urban communities. In addition, we also develop a weighted degree centrality measure for constructing spatiotemporal heterogeneous networks. To evaluate the performance of our proposed model, we implement it on real-world urban community data. Experimental results demonstrate the effectiveness of our model over state-of-the-art alternatives.},
	address = {New York, NY, USA},
	author = {Keerthi Chandra, Dakshak and Wang, Pengyang and Leopold, Jennifer and Fu, Yanjie},
	booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
	doi = {10.1145/3340531.3412030},
	isbn = {9781450368599},
	keywords = {autoencoder, laplacian score, network embedding, representation learning, spatiotemporal heterogeneous network, weighted degree centrality measure},
	location = {Virtual Event, Ireland},
	numpages = {10},
	pages = {615–624},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {Collective Embedding with Feature Importance: A Unified Approach for Spatiotemporal Network Embedding},
	url = {https://doi.org/10.1145/3340531.3412030},
	year = {2020},
}

@inproceedings{10.1145/3340531.3417459,
	abstract = {We study the problem of deriving geolocations for Wikipedia pages. To this end, we introduce a general four-step process to location derivation, and consider different instantiations of this process, leveraging both textual and categorical data. Extensive experimentation shows that our methods provide good precision-recall trade-offs and improvements over text-only methods. Hence, our system can be used to augment the geographic information of Wikipedia, and to enable more effective geographic information retrieval.},
	address = {New York, NY, USA},
	author = {Krause, Amir and Cohen, Sara},
	booktitle = {Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management},
	doi = {10.1145/3340531.3417459},
	isbn = {9781450368599},
	keywords = {geographic information retrieval, geotagging, wikipedia},
	location = {Virtual Event, Ireland},
	numpages = {4},
	pages = {3293–3296},
	publisher = {Association for Computing Machinery},
	series = {CIKM '20},
	title = {Deriving Geolocations in Wikipedia},
	url = {https://doi.org/10.1145/3340531.3417459},
	year = {2020},
}

@inproceedings{10.1145/3340631.3394845,
	abstract = {The suggestion of Points of Interest to people with Autism Spectrum Disorder (ASD) challenges recommender systems research because these users' perception of places is influenced by idiosyncratic sensory aversions which can mine their experience by causing stress and anxiety. Therefore, managing individual preferences is not enough to provide these people with suitable recommendations. In order to address this issue, we propose a Top-N recommendation model that combines the user's idiosyncratic aversions with her/his preferences in a personalized way to suggest the most compatible and likable Points of Interest for her/him. We are interested in finding a user-specific balance of compatibility and interest within a recommendation model that integrates heterogeneous evaluation criteria to appropriately take these aspects into account. We tested our model on both ASD and "neurotypical" people. The evaluation results show that, on both groups, our model outperforms in accuracy and ranking capability the recommender systems based on item compatibility, on user preferences, or which integrate these two aspects by means of a uniform evaluation model.},
	address = {New York, NY, USA},
	author = {Mauro, Noemi and Ardissono, Liliana and Cena, Federica},
	booktitle = {Proceedings of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
	doi = {10.1145/3340631.3394845},
	isbn = {9781450368612},
	keywords = {recommender systems, autism spectrum disorder, accessibility},
	location = {<conf-loc>, <city>Genoa</city>, <country>Italy</country>, </conf-loc>},
	numpages = {10},
	pages = {163–172},
	publisher = {Association for Computing Machinery},
	series = {UMAP '20},
	title = {Personalized Recommendation of PoIs to People with Autism},
	url = {https://doi.org/10.1145/3340631.3394845},
	year = {2020},
}

@article{10.1145/3340707,
	abstract = {An important problem in terrain analysis is modeling how water flows across a terrain and creates floods by filling up depressions. In this article, we study a number of flood-risk related problems: Given a terrain Σ, represented as a triangulated xy-monotone surface with n vertices, a rain distribution R, and a volume of rain ψ, determine which portions of Σ are flooded. We develop efficient algorithms for flood-risk analysis under the multiflow-directions (MFD) model, in which water at a point can flow along multiple downslope edges and which more accurately represent flooding events.We present three main results: First, we present an O(n log n)-time algorithm to answer a terrain-flood query: if it rains a volume ψ according to a rain distribution R, determine what regions of Σ will be flooded. Second, we present a O(n log n + nm)-time algorithm for preprocessing Σ containing m sinks into a data structure of size O(nm) for answering point-flood queries: Given a rain distribution R, a volume of rain ψ falling according to R, and point q ∈ Σ, determine whether q will be flooded. A point-flood query can be answered in O(|R|k+k2) time, where k is the number of maximal depressions in Σ containing the query point q and |R| is the number of vertices in R with positive rainfall. Finally, we present algorithms for answering a flood-time query: given a rain distribution R and a point q ∈ Σ, determine the volume of rain that must fall before q is flooded. Assuming that the product of two k \texttimes{} k matrices can be computed in O(kω) time, we show that a flood-time query can be answered in O(nk + kω) time. We also give an α-approximation algorithm, for α &gt; 1, which runs in O(n log n log α ρ)-time, where ρ is a variable on the terrain that depends on the ratio between depression volumes. We implemented our algorithms for computing terrain and point-flood queries as well as approximate flood-time queries. We tested the efficacy and efficiency of these algorithms on three real terrains of different types (urban, suburban, and mountainous.)},
	address = {New York, NY, USA},
	articleno = {26},
	author = {Lowe, Aaron and Agarwal, Pankaj K.},
	doi = {10.1145/3340707},
	issn = {2374-0353},
	issue_date = {December 2019},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {merge trees, flood-risk analysis, Terrains},
	month = {sep},
	number = {4},
	numpages = {27},
	publisher = {Association for Computing Machinery},
	title = {Flood-Risk Analysis on Terrains under the Multiflow-Direction Model},
	url = {https://doi.org/10.1145/3340707},
	volume = {5},
	year = {2019},
}

@inproceedings{10.1145/3340964.3340971,
	abstract = {The trajectory-aware lowest-cost path selection problem aims to find the lowest-cost path using trajectory data. Trajectory data is valuable since it carries information about travel cost along paths, and also reflects travelers' routing preference. Path-centric travel cost estimation models using trajectory data grows popular recently, which considers the auto-correlation of the energy consumption on different segments of a path. However, path-centric models are more computationally expensive than edge-centric models. The main challenge of this problem is that the travel cost of every candidate path explored during the process of searching for the lowest-cost path need to be estimated, resulting in high computational cost. The current path selection algorithms that use path-centric cost estimation models still follow the pattern of "path + edge" when exploring candidate paths, which may result in redundant computation. We introduce a trajectory-aware graph model in which each node is a maximal trajectory-aware path. Two nodes in the trajectory-aware graph are linked by an edge if their union forms a trajectory-union path. We then propose a path selection algorithm to find a path in the proposed trajectory-aware graph which corresponds to the lowest-cost path in the input spatial network. We prove theoretically the proposed algorithm is correct and complete. Moreover, we prove theoretically that the proposed path selection algorithm cost much less computational time than the algorithm used in the related work, and validate it through experiments using real-world trajectory data.},
	address = {New York, NY, USA},
	author = {Li, Yan and Kotwal, Pratik and Wang, Pengyue and Shekhar, Shashi and Northrop, William},
	booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
	doi = {10.1145/3340964.3340971},
	isbn = {9781450362801},
	keywords = {trajectory, shortest path, routing, path-centric, path selection},
	location = {Vienna, Austria},
	numpages = {9},
	pages = {61–69},
	publisher = {Association for Computing Machinery},
	series = {SSTD '19},
	title = {Trajectory-aware Lowest-cost Path Selection: A Summary of Results},
	url = {https://doi.org/10.1145/3340964.3340971},
	year = {2019},
}

@inproceedings{10.1145/3340964.3340979,
	abstract = {Besides the traditional cartographic data sources, spatial information can also be derived from location-based sources. Location-based sources offer rich spatial information describing the semantics of locations. However, even though different location-based sources refer to the same physical world, each one has only partial coverage of the spatial entities of interest, describe them with different attributes, and sometimes provide contradicting information. Hence, the problem of finding which pairs of spatial entities belong to the same physical spatial entity demands specific attention. We propose a solution (QuadSky) to the problem of spatial entity linkage across diverse location-based sources. QuadSky starts with a spatial blocking technique (QuadFlex) that inherits the concept and the complexity from the quadtree algorithm but improves the splitting technique not to separate nearby points. After comparing the spatial entities of the same block, we propose a novel algorithm, referred to as SkyEx that separates the pairs considered as a match (positive class) from the rest (negative class) by using Pareto optimality. SkyEx does not require weights on the attributes, scoring function, or a training set. QuadSky achieves 0.85 precision and 0.85 recall for a manually labeled dataset of 1,500 pairs and 0.87 precision and 0.6 recall for a semi-manually labeled dataset of 777,452 pairs. Moreover, QuadSky provides the best trade-off between precision and recall and consequently, the best F-measure compared to the existing baselines.},
	address = {New York, NY, USA},
	author = {Isaj, Suela and Zim\'{a}nyi, Esteban and Pedersen, Torben Bach},
	booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
	doi = {10.1145/3340964.3340979},
	isbn = {9781450362801},
	location = {Vienna, Austria},
	numpages = {10},
	pages = {1–10},
	publisher = {Association for Computing Machinery},
	series = {SSTD '19},
	title = {Multi-Source Spatial Entity Linkage},
	url = {https://doi.org/10.1145/3340964.3340979},
	year = {2019},
}

@inproceedings{10.1145/3340964.3340984,
	abstract = {Road network traffic data has been widely studied by researchers and practitioners in different areas such as urban planning, traffic prediction and spatial-temporal databases. The existing urban traffic simulators suffer from two critical issues (1) scalability: most of them only offer single-machine solutions which are not adequate to produce large-scale data. Some simulators can generate traffic in parallel but do not well balance the load among machines in a cluster. (2) granularity: many simulators do not consider microscopic traffic situations including traffic lights, lane changing, and car following. In the paper, we propose GeoSparkSim, a scalable traffic simulator which extends Apache Spark to generate large-scale road network traffic datasets with microscopic traffic simulation. The proposed system seamlessly integrates with a Spark-based spatial data management system, GeoSpark, to deliver a holistic approach that allows data scientists to simulate, analyze and visualize large-scale urban traffic data. To implement microscopic traffic models, GeoSparkSim employs a simulation-aware vehicle partitioning method to partition vehicles among different machines such that each machine has a balanced workload. A full-fledged prototype of GeoSparkSim is implemented in Apache Spark. In this demonstration, we will show the attendees how to issue GeoSparkSim simulation tasks via the user interface, visualize simulated vehicle movements, and monitor the backend Spark cluster status.},
	address = {New York, NY, USA},
	author = {Fu, Zishan and Yu, Jia and Sarwat, Mohamed},
	booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
	doi = {10.1145/3340964.3340984},
	isbn = {9781450362801},
	keywords = {Traffic simulation, distributed computation, road network},
	location = {Vienna, Austria},
	numpages = {4},
	pages = {186–189},
	publisher = {Association for Computing Machinery},
	series = {SSTD '19},
	title = {Demonstrating GeoSparkSim: A Scalable Microscopic Road Network Traffic Simulator Based on Apache Spark},
	url = {https://doi.org/10.1145/3340964.3340984},
	year = {2019},
}

@inproceedings{10.1145/3340964.3340990,
	abstract = {Large vehicle trajectory data sets can give detailed insight into traffic and congestion that is useful for routing as well as transportation planning. Making information from such data sets available to more users can enable applications that reduce travel time and fuel consumption. However, extracting such information efficiently requires deep knowledge of the underlying schema and indexing methods. To enable more users to extract information from trajectory data, we have developed an API that removes the need to be familiar with the schema. Furthermore, when giving access to trajectory data, privacy concerns often call for the application of anonymization methods before analysis results are made available. In our demonstration, owners of trajectory data are able to experiment with different levels of anonymization to see how this affects the quality of different types of trajectory analysis services implemented on top of a large trajectory data set.},
	address = {New York, NY, USA},
	author = {Waury, Robert and Dolog, Peter and Jensen, Christian S. and Torp, Kristian},
	booktitle = {Proceedings of the 16th International Symposium on Spatial and Temporal Databases},
	doi = {10.1145/3340964.3340990},
	isbn = {9781450362801},
	location = {Vienna, Austria},
	numpages = {4},
	pages = {198–201},
	publisher = {Association for Computing Machinery},
	series = {SSTD '19},
	title = {Analyzing Trajectories Using a Path-based API},
	url = {https://doi.org/10.1145/3340964.3340990},
	year = {2019},
}

@inproceedings{10.1145/3341105.3373933,
	abstract = {E-commerce and logistics operations produce a vast amount of geospatial data labelled with postal addresses. The data has great potential to mine geospatial knowledge, and we demonstrate that regional maps can be automatically built using the same. We propose an algorithm to construct non-overlapping polygons of the localities at a city level. The algorithm involves non-parametric spatial probability modelling of the localities followed by locality classification of the cells in a hexagonal grid. We show that our algorithm is capable of handling noise, which is significantly high in our setting due to the small scale of localities. A property about the noise and the correct information is presented such that our algorithm infers a correct locality polygon. We quantitatively measure the accuracy of our system by comparing its output with the available ground truth. We also discuss multiple applications of the generated maps in the context of e-commerce and logistics operations.},
	address = {New York, NY, USA},
	author = {Dahiya, Manjeet and Samatia, Devendra and Rustogi, Kabir},
	booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3341105.3373933},
	isbn = {9781450368667},
	keywords = {postal addresses, polygons, maps, localities, labelled geospatial data, geospatial mining},
	location = {Brno, Czech Republic},
	numpages = {8},
	pages = {601–608},
	publisher = {Association for Computing Machinery},
	series = {SAC '20},
	title = {Learning locality maps from noisy geospatial labels},
	url = {https://doi.org/10.1145/3341105.3373933},
	year = {2020},
}

@inproceedings{10.1145/3341105.3374045,
	abstract = {In the last few years, several trajectory classification methods have been proposed for mobility data collected from GPS devices. Most of them only use information derived from the physical movement of the object, as speed, acceleration, and direction variation. More recently, trajectory data obtained from location-based social networks, based on user check-ins in Points of Interest (POIs), have been used to analyze user mobility patterns, giving rise to the development of methods for this specific type of data. While GPS trajectories are in general dense, and movement is characterized by spatio-temporal features and sequential patterns, social media trajectories are mostly sparse, and we claim that the moving object can be characterized simply by the frequency of visits. In this paper we propose a simple, effective, and efficient trajectory classification method based on POI frequency. With experiments on three real datasets we show that the proposed method outperforms the state of the art and is suitable for large amounts of data.},
	address = {New York, NY, USA},
	author = {Vicenzi, Francisco and Petry, Lucas May and Silva, Camila Leite da and Alvares, Luis Otavio and Bogorny, Vania},
	booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3341105.3374045},
	isbn = {9781450368667},
	keywords = {trajectory data mining, trajectory classification, social media trajectory classification, multiple aspect trajectory, frequency-based trajectory classification},
	location = {Brno, Czech Republic},
	numpages = {8},
	pages = {624–631},
	publisher = {Association for Computing Machinery},
	series = {SAC '20},
	title = {Exploring frequency-based approaches for efficient trajectory classification},
	url = {https://doi.org/10.1145/3341105.3374045},
	year = {2020},
}

@inproceedings{10.1145/3341105.3374062,
	abstract = {Air Transport Network (ATN) has essential societal and economic functions. One important characteristic of ATN is its dynamic structure since the timings of departure and arrival of flights vary considerably. Static representation, measures and frameworks limit the study of certain properties of these networks. Here, we develop and demonstrate an approach to characterize the robustness of temporal ATN. We employ the following framework: 1) represent the USA flights considering the time-scheduled as a temporal network; 2) analyze the main airports ranked by the adapted centrality measures in different timestamps; 3) employ attack strategies in the top-ranked airport evaluating resilience. We demonstrate when the time is considered there are variations in the airport rank by centrality measures that are not captured by static approaches. Moreover, while the giant component is not affected by the time considered, the efficiency, which measures the time duration, drops significantly. The robustness measure indicates attacks considering the betweenness is the most damaged. This work encompasses a real scenario of ATN representation and contributes to the study of directed and temporal networks.},
	address = {New York, NY, USA},
	author = {Sano, Humberto Hayashi and Berton, Lilian},
	booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3341105.3374062},
	isbn = {9781450368667},
	keywords = {time varying network, temporal network, robustness, centrality measures, air transportation network},
	location = {Brno, Czech Republic},
	numpages = {4},
	pages = {1881–1884},
	publisher = {Association for Computing Machinery},
	series = {SAC '20},
	title = {Topology and robustness analysis of temporal air transport network},
	url = {https://doi.org/10.1145/3341105.3374062},
	year = {2020},
}

@inproceedings{10.1145/3341105.3374120,
	abstract = {This work starts from the hypothesis that spatial dynamics and the functions (cover or use) of geographical objects could be, partly, explained or anticipated by the history of their functions and co-localizations changes. Hence, an approach relying on association rules mining for the extraction of explicative/predictive models of territorial evolution is proposed. In order to deal with the asymmetry of the used learning data, we proposed to adapt the supports assignment process for the MSApriori and we also proposed a new multiple minimum support based algorithm called BERA. Applied on study cases from the Corine Land Cover database between 1990 and 2012, the proposed mining methods proved their worth in the management of data imbalance and the generated rules highlight realistic urban dynamics.},
	address = {New York, NY, USA},
	author = {Gharbi, Asma and de Runz, Cyril and Akdag, Herman and Faiz, Sami},
	booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3341105.3374120},
	isbn = {9781450368667},
	keywords = {territorial evolution, multiple minimum supports, class association rules, MSApriori},
	location = {Brno, Czech Republic},
	numpages = {3},
	pages = {632–634},
	publisher = {Association for Computing Machinery},
	series = {SAC '20},
	title = {Mining association rules in asymmetric data for territorial evolution modeling},
	url = {https://doi.org/10.1145/3341105.3374120},
	year = {2020},
}

@inproceedings{10.1145/3341105.3374125,
	abstract = {The understanding of daily human activity is an active research topic. Thanks to GPS and smartphones, human movements can be monitored and analyzed. In addition, by exploiting Linked Open Data and user personal data, semantic labels and annotations can be added to movements. Thus, semantic trajectories can be considered as sequences of timestamped activities where each activity is described by a semantic label. In this context, a major challenge is the comparison of such semantic trajectories, looking to extract and learning similar human mobility behaviors. We propose CED (Contextual Edit Distance), a generic similarity measure for semantic sequences comparison which improve the Edit Distance to take into account the context similarity between elements in the sequence. CED is configurable to any sequence data and business needs.},
	address = {New York, NY, USA},
	author = {Moreau, Clement and Devogele, Thomas and Peralta, Veronika and Etienne, Laurent},
	booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3341105.3374125},
	isbn = {9781450368667},
	keywords = {similarity measure, semantic trajectory, human behavior analysis, edit distance, clustering},
	location = {Brno, Czech Republic},
	numpages = {3},
	pages = {635–637},
	publisher = {Association for Computing Machinery},
	series = {SAC '20},
	title = {A contextual edit distance for semantic trajectories},
	url = {https://doi.org/10.1145/3341105.3374125},
	year = {2020},
}

@inproceedings{10.1145/3341162.3349328,
	abstract = {In this work, we propose P-Loc, a device-free indoor localization system based on power-line network within the building. P-Loc measures the electromagnetic (EM) coupling between a human body and existing power-lines, which are simultaneously used for electric power transmission. To avoid the impact of AC mains and noise from other electrical sources, we inject a signal into the ground-line to generate the occupant location fingerprint of a specific frequency. A 0.48m average error distance is obtained in the preliminary experiments.},
	address = {New York, NY, USA},
	author = {Zhou, Tian and Zhang, Yue and Chen, Xinlei and Mosalam, Khaild M. and Noh, Hae Young and Zhang, Pei and Zhang, Lin},
	booktitle = {Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers},
	doi = {10.1145/3341162.3349328},
	isbn = {9781450368698},
	keywords = {power-line network, indoor localization, electromagnetic coupling},
	location = {London, United Kingdom},
	numpages = {5},
	pages = {611–615},
	publisher = {Association for Computing Machinery},
	series = {UbiComp/ISWC '19 Adjunct},
	title = {P-Loc: a device-free indoor localization system utilizing building power-line network},
	url = {https://doi.org/10.1145/3341162.3349328},
	year = {2019},
}

@inproceedings{10.1145/3343031.3350585,
	abstract = {We present a browsing interface that allows for an audiovisual exploration of regional music taste around the world. We exploit a total of 10,758,121 geolocated tweets about music. The web-based geo-aware visualization and auralization called Tastalyzer enables exploring and analyzing music taste on a fine-grained geographical level, such as (i) comparing rural and corresponding urban music taste within an agglomeration (city) or (ii) comparing the music taste in a target region (agglomeration) to the taste of the country the region is part of and (iii) to the global music taste.},
	address = {New York, NY, USA},
	author = {Bauer, Christine and Schedl, Markus and Angerer, Vera and Wegenkittl, Stefan},
	booktitle = {Proceedings of the 27th ACM International Conference on Multimedia},
	doi = {10.1145/3343031.3350585},
	isbn = {9781450368896},
	keywords = {user interface, twitter, music taste, music browsing, cultural differences, audiovisual exploration},
	location = {Nice, France},
	numpages = {3},
	pages = {1044–1046},
	publisher = {Association for Computing Machinery},
	series = {MM '19},
	title = {Tastalyzer: Audiovisual Exploration of Urban and Rural Variations in Music Taste},
	url = {https://doi.org/10.1145/3343031.3350585},
	year = {2019},
}

@inproceedings{10.1145/3344341.3368802,
	abstract = {A majority of the data generated in several domains is geotagged. These data also have a chronological component associated with them. Pervasive data generation and collection efforts have led to an increase in data volumes. These data hold the potential to unlock valuable insights. To facilitate such knowledge extraction in a timely manner, the underlying file system must satisfy several objectives. In this study, we present Atlas, a distributed file system designed specifically for spatiotemporal data. Atlas includes several capabilities that are suited for performing large-scale analyses: aligning dispersion with data access patterns, load balancing storage, and facilitating interoperation with analytical engines such as Hadoop and Spark. Our empirical benchmarks profile several aspects of Atlas, and demonstrate the suitability of our methodology.},
	address = {New York, NY, USA},
	author = {Rammer, Daniel and Lee Pallickara, Sangmi and Pallickara, Shrideep},
	booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing},
	doi = {10.1145/3344341.3368802},
	isbn = {9781450368940},
	keywords = {analytics, file systems, hdfs, spatiotemporal data},
	location = {Auckland, New Zealand},
	numpages = {10},
	pages = {11–20},
	publisher = {Association for Computing Machinery},
	series = {UCC'19},
	title = {ATLAS: A Distributed File System for Spatiotemporal Data},
	url = {https://doi.org/10.1145/3344341.3368802},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359062,
	abstract = {In the coming era of connected autonomous vehicles, data-driven traffic optimization will reach its full potential. By collecting highly detailed real-time traffic data from sensors and vehicles, a traffic management system will have the full view of the entire road network, allowing it to plan traffic in a virtual world that replicates the real road network. This will bring significant innovations to transport-domain applications. We prototype a traffic management system that can perform traffic optimization with connected autonomous vehicles. We propose two route assignment algorithms that aim to reduce traffic delays by reducing intersecting routes. The proposed algorithms and two state-of-the-art route assignment algorithms are implemented in the prototype system. We evaluate the algorithms with both synthetic and real road networks. The experimental results show that the proposed algorithms outperform competitors in terms of the travel times of the routes.},
	address = {New York, NY, USA},
	author = {Motallebi, Sadegh and Xie, Hairuo and Tanin, Egemen and Qi, Jianzhong and Ramamohanarao, Kotagiri},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359062},
	isbn = {9781450369091},
	keywords = {Autonomous Vehicles, Route Assignment, Streaming Traffic Data, Traffic Management Systems},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {408–411},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Streaming Route Assignment for Connected Autonomous Vehicles (Systems Paper)},
	url = {https://doi.org/10.1145/3347146.3359062},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359069,
	abstract = {A natural language interface (NLI) to databases is an interface that translates a natural language question to a structured query that is executable by database management systems (DBMS). However, an NLI that is trained in the general domain is hard to apply in the spatial domain due to the idiosyncrasy and expressiveness of the spatial questions. Inspired by the machine comprehension model, we propose a spatial comprehension model that is able to recognize the meaning of spatial entities based on the semantics of the context. The spatial semantics learned from the spatial comprehension model is then injected to the natural language question to ease the burden of capturing the spatial-specific semantics. With our spatial comprehension model and information injection, our NLI for the spatial domain, named SpatialNLI, is able to capture the semantic structure of the question and translate it to the corresponding syntax of an executable query accurately. We also experimentally ascertain that SpatialNLI outperforms state-of-the-art methods.},
	address = {New York, NY, USA},
	author = {Li, Jingjing and Wang, Wenlu and Ku, Wei-Shinn and Tian, Yingtao and Wang, Haixun},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359069},
	isbn = {9781450369091},
	keywords = {Natural Language Interface, Spatial Data Science},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {339–348},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {SpatialNLI: A Spatial Domain Natural Language Interface to Databases Using Spatial Comprehension},
	url = {https://doi.org/10.1145/3347146.3359069},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359070,
	abstract = {Address parsing is a critical step for map search engines. This component annotates the terms of an address query, e.g. house numbers, road names, administrative units etc., so that the address search engine can resolve the expected result. Deep recurrent models achieve state of the art performance for address parsing; however, scaling such models is problematic. They require a significant amount of term-annotated data which is expensive to acquire. In this paper, active learning significantly reduces the amount of labeled data required to train accurate address parsing models. We demonstrate the efficiency of our approach when cold-starting with human-labeled as well as synthetically-generated data.},
	address = {New York, NY, USA},
	author = {Craig, Helen and Yankov, Dragomir and Wang, Renzhong and Berkhin, Pavel and Wu, Wei},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359070},
	isbn = {9781450369091},
	keywords = {sequence models, query intent, neural networks, map search, geocoding, address tagging, active learning},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {424–427},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Scaling Address Parsing Sequence Models through Active Learning},
	url = {https://doi.org/10.1145/3347146.3359070},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359074,
	abstract = {This paper demonstrates CleanUpOurWorld; a research spatial database that is designed and deployed to collect, process, query, and visualize anthropogenic litter data. Such data has a significant importance in the field of environmental sciences due to its important use cases. We make a major on-going effort to collect and maintain such data worldwide from different sources through a community of environmental scientists and partner organizations. With the increasing volume of data, existing software packages, such as GIS software, do not scale to process, query, and visualize such data. To overcome this, CleanUpOurWorld digests datasets from diferent sources, with different formats, in a scalable backend that cleans, integrates, and unifies them in a structured form in a relational spatial database. Frontend applications are built to visualize litter data at multiple spatial resolutions.},
	address = {New York, NY, USA},
	author = {Kang, Yunfan and Zhao, Ziang and Magdy, Amr and Cowger, Win and Gray, Andrew},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359074},
	isbn = {9781450369091},
	keywords = {visualization, litter data, data management, data cleaning},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {560–563},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Scalable Multi-resolution Spatial Visualization for Anthropogenic Litter Data},
	url = {https://doi.org/10.1145/3347146.3359074},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359086,
	abstract = {In the last decade, more and more spatial data has been acquired on a global scale due to satellite missions, social media, and coordinated governmental activities. This observational data suffers from huge storage footprints and makes global analysis challenging. Therefore, many information products have been designed in which observations are turned into global maps showing features such as land cover or land use, often with only a few discrete values and sparse spatial coverage like only within cities.Traditional coding of such data as a raster image becomes challenging due to the sizes of the datasets and spatially non-local access patterns, for example, when labeling social media streams.This paper proposes GloBiMap, a randomized data structure, based on Bloom filters, for modeling low-cardinality sparse raster images of excessive sizes in a configurable amount of memory with pure random access operations avoiding costly intermediate decompression. In addition, the data structure is designed to correct the inevitable errors of the randomized layer in order to have a fully exact representation.We show the feasibility of the approach on several real-world data sets including the Global Urban Footprint in which each pixel denotes whether a particular location contains a building at a resolution of roughly 10cm globally as well as on a global Twitter sample of more than 220 million precisely geolocated tweets.},
	address = {New York, NY, USA},
	author = {Werner, Martin},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359086},
	isbn = {9781450369091},
	keywords = {Data Sparsity and Compression, Data Structures, Geographic Information Systems, Image Representation, Randomized},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {3–12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {GloBiMaps - A Probabilistic Data Structure for In-Memory Processing of Global Raster Datasets},
	url = {https://doi.org/10.1145/3347146.3359086},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359088,
	abstract = {In this work we propose DeepTrip -- an end-to-end method for better understanding of the underlying human mobility and improved modeling of the POIs' transitional distribution in human moving patterns. DeepTrip consists of: a Trip Encoder to embed a given route into a latent variable with a recurrent neural network (RNN); and a Trip Decoder to reconstruct this route conditioned on an optimized latent space. Simultaneously, we define an Adversarial Net composed of a generator and critic, which generates a representation for a given query and uses a critic to distinguish the trip representation generated from Trip Encoder and query representation obtained from Adversarial Net. DeepTrip enables regularizing the latent space and generalizing users' complex check-in preference. We demonstrate the effectiveness and efficiency of the proposed model, and the experimental evaluations show that DeepTrip outperforms the state-of-the-art baselines on various evaluation metrics.},
	address = {New York, NY, USA},
	author = {Gao, Qiang and Trajcevski, Goce and Zhou, Fan and Zhang, Kunpeng and Zhong, Ting and Zhang, Fengli},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359088},
	isbn = {9781450369091},
	keywords = {trip recommendation, generative adversarial net, auto-encoder},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {444–447},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {DeepTrip: Adversarially Understanding Human Mobility for Trip Recommendation},
	url = {https://doi.org/10.1145/3347146.3359088},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359095,
	abstract = {High resolution Digital Elevation models, such as the grid terrain model of Denmark with more than 200 billion measurements, is a basic requirement for water flow modelling and flood risk analysis. However, a large number of modifications often need to be made to even very accurate terrain models, before they can be used in realistic flow modeling. This include removal of bridges, which otherwise act as dams in flow modeling, and inclusion of culverts that transport water underneath roads. For this reason, there is list of known hydrological corrections for the danish model. However, producing this list is a slow an expensive process, since it is to a large extent done manually, often with only local input. In this paper we propose a new algorithmic approach based on machine learning and convolutional neural networks for automatically detecting hydrological corrections on large terrain data. Our model is able to detect most known hydrological corrections and quite a few more that should have been included in the original list.},
	address = {New York, NY, USA},
	author = {Arge, Lars and Gr\o{}nlund, Allan and Svendsen, Svend Christian and Tranberg, Jonas},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359095},
	isbn = {9781450369091},
	keywords = {neural networks, hydrological conditioning, geographic information systems, flow modelling},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {464–467},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Learning to Find Hydrological Corrections},
	url = {https://doi.org/10.1145/3347146.3359095},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359096,
	abstract = {Travel time estimation is a critical task, useful to many urban applications at the individual citizen and the stakeholder level. This paper presents a novel hybrid algorithm for travel time estimation that leverages historical and sparse real-time trajectory data. Given a path and a departure time we estimate the travel time taking into account the historical information, the real-time trajectory data and the correlations among different road segments. We detect similar road segments using historical trajectories, and use a latent representation to model the similarities. Our experimental evaluation demonstrates the effectiveness of our approach.},
	address = {New York, NY, USA},
	author = {Zygouras, Nikolaos and Panagiotou, Nikolaos and Li, Yang and Gunopulos, Dimitrios and Guibas, Leonidas},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359096},
	isbn = {9781450369091},
	keywords = {urban GPS data, real-time travel time estimation, pathlets, matrix factorization, data sparsity, Gaussian processes},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {99–108},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {HTTE: A Hybrid Technique For Travel Time Estimation In Sparse Data Environments},
	url = {https://doi.org/10.1145/3347146.3359096},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359100,
	abstract = {Spatial hotspot discovery aims at discovering regions with statistically significant concentration of activities. It has shown great value in many important societal applications such as transportation engineering, public health, and public safety. This paper formulates the problem of Linear Hotspot Detection on All Simple Paths (LHDA) which identifies hotspots from the complete set of simple paths enumerated from a given spatial network. LHDA overcomes the limitations of existing methods which miss hotspots that naturally occur along linear simple paths on a road network. To address the computational challenges, we propose a novel algorithm named bidirectional fragment-multi-graph traversal (ASP_FMGT) and two path reduction approaches ASP_NR and ASP_HD. Experimental analyses show that ASP_FMGT has substantially improved performance over state-of-the-art approach (ASP_Base) while keeping the solution complete and correct. Moreover, a case study on real-world datasets showed that ASP_FMGT outperforms existing approaches.},
	address = {New York, NY, USA},
	author = {Tang, Xun and Gupta, Jayant and Shekhar, Shashi},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359100},
	isbn = {9781450369091},
	keywords = {spatial networks, spatial hotspot detection, Spatial data mining},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {476–479},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Linear Hotspot Discovery on All Simple Paths: A Summary of Results},
	url = {https://doi.org/10.1145/3347146.3359100},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359102,
	abstract = {Given a transportation network with node and edge capacity constraints, initial node occupancy, destination locations, and the conflict resolution parameter k, the Conflict-Free Evacuation Route Planner (CF-ERP) problem finds evacuation routes that can minimize the evacuation time and the number of movement conflicts along the routes. CF-ERP is important for many societal applications, such as evacuation management and preparation in case of natural or man-made disasters. The problem is computationally challenging due to the large size of the transportation network and the constraints. Related work has considered the evacuation routing problem either solely as a network flow optimization problem or a conflict minimization problem, but not both. In this paper, we propose novel approaches that can produce evacuation routes to minimize the evacuation time and the number of movement conflicts. Experiments and a case study on real-world datasets from Florida show the effectiveness and efficiency of the proposed approaches.},
	address = {New York, NY, USA},
	author = {Herschelman, Roxana and Yang, KwangSoo},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359102},
	isbn = {9781450369091},
	keywords = {Spatial Graph Algorithms, Evacuation Route Planning, Constrained optimization},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {480–483},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Conflict-Free Evacuation Route Planner},
	url = {https://doi.org/10.1145/3347146.3359102},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359107,
	abstract = {GPS-based services (e.g. Google Maps) are very popular in our daily life, while there are still many GPS-denied environments (e.g. indoor and underground scenarios) in which they cannot be used. In these situations, localization and navigation are still important, for example in emergency evacuation or indoor navigation. In this paper we aim to solve the problem of localizing and navigating humans or robots in GPS-denied environments based on landmarks. Our work is inspired by human daily communications about localization and navigation, for example someone who has been to a shopping mall many times can localize and guide another person to get to a certain shop via conversations over the cellphone. Our goal is to build a system with the same capability. We propose a system that relies on qualitative information of places (e.g. the direction relations between landmarks involved in route descriptions), where localization can be achieved in an interactive manner and by analyzing observations provided by users. Our system decides the "best" route from one place to another by three factors: the number of landmarks; the number of ambiguous turns; and the qualitative distance (e.g. near and far). According to the experimental results, the number of requeries in the interactive localization process is acceptable and our route planning algorithm outperforms previous methods in several cases.},
	address = {New York, NY, USA},
	author = {Hua, Hua and Zhang, Peng and Renz, Jochen},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359107},
	isbn = {9781450369091},
	keywords = {reliable navigation, qualitative spatial reasoning, qualitative relative directions, landmark-based navigation, Interactive localization},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {23–32},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Qualitative Place Maps for Landmark-based Localization and Navigation in GPS-denied Environments},
	url = {https://doi.org/10.1145/3347146.3359107},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359111,
	abstract = {Center-based clustering, in particular k-means clustering, is frequently used for point data. Its advantages include that the resulting clustering is often easy to interpret and that the cluster centers provide a compact representation of the data. Recent theoretical advances have been made in generalizing center-based clustering to trajectory data. Building upon these theoretical results, we present practical algorithms for center-based trajectory clustering.},
	address = {New York, NY, USA},
	author = {Buchin, Kevin and Driemel, Anne and van de L'Isle, Natasja and Nusser, Andr\'{e}},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359111},
	isbn = {9781450369091},
	keywords = {Algorithms and Data Structures, Clustering, Computational Geometry, Trajectories},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {496–499},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {klcluster: Center-based Clustering of Trajectories},
	url = {https://doi.org/10.1145/3347146.3359111},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359347,
	abstract = {We consider the problem of accurately representing geographic networks at reduced coordinate precision. We require that vertices are placed on a grid and the network topology is retained, that is, we are not allowed to introduce intersections or collapse faces. Minimizing the "rounding error" in this setting is known to be NP-hard and no practical methods, even heuristic, are known. We demonstrate a two-stage simulated annealing algorithm that focuses on finding a feasible solution first, then switches to optimizing the rounding error; a straightforward annealing approach without stage one has difficulty finding any feasible solution at all. We discuss various feasibility procedures and evaluate their applicability on geographic networks. Datasets and an implementation in C++ are available at: https://github.com/tcvdijk/armstrong.},
	address = {New York, NY, USA},
	author = {van Dijk, Thomas C. and L\"{o}ffler, Andre},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359347},
	isbn = {9781450369091},
	keywords = {simulated annealing, optimisation, graph algorithms},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {239–248},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Practical Topologically Safe Rounding of Geographic Networks},
	url = {https://doi.org/10.1145/3347146.3359347},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359348,
	abstract = {Automatic Extraction of road network from satellite images is a goal that can benefit and even enable new technologies. Methods that combine machine learning (ML) and computer vision have been proposed in recent years which make the task semi-automatic by requiring the user to provide curated training samples. The process can be fully automatized if training samples can be produced algorithmically. In this work, we develop such a technique by infusing a persistence-guided discrete Morse based graph reconstruction algorithm into ML framework. We elucidate our contributions in two phases. First, in a semi-automatic framework, we combine a discrete-Morse based graph reconstruction algorithm with an existing CNN framework to segment input satellite images. We show that this leads to reconstructions with better connectivity and less noise. Next, in a fully automatic framework, we leverage the power of the discrete-Morse based graph reconstruction algorithm to train a CNN from a collection of images without labelled data and use the same algorithm to produce the final output from the segmented images created by the trained CNN. We apply the discrete-Morse based graph reconstruction algorithm iteratively to improve the accuracy of the CNN. We show experimental results on datasets from SpaceNet Challenge. Full version of the paper appears in [8].},
	address = {New York, NY, USA},
	author = {Dey, Tamal K. and Wang, Jiayuan and Wang, Yusu},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359348},
	isbn = {9781450369091},
	keywords = {topological method, satellite images, map generation, machine learning, Morse complex},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {520–523},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Road Network Reconstruction from satellite images with Machine Learning Supported by Topological Methods},
	url = {https://doi.org/10.1145/3347146.3359348},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359351,
	abstract = {Several industries rely on accurate and efficient processing of 3D spatial queries over increasingly large datasets for decision optimization and exploration purposes. Examples include clinical diagnosis supported by 3D imaging of human tissues, numerical simulation of aerodynamics during the design of aircraft and vehicles, and the search for profitable deposits of minerals, oil and gas guided by 3D maps extrapolated from dense collections of rock samples. Despite the clear demand for spatial data processing in 3D space, existing systems supporting these organizations are scarce and scale poorly with data volume. This paper presents a GPU-based acceleration engine for SQL database management systems that can serve spatial queries orders of magnitude faster than solutions based on traditional software stacks.},
	address = {New York, NY, USA},
	author = {Real, Lucas C. Villa and Silva, Bruno and Meliksetian, Dikran S. and Sacchi, Kaique},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359351},
	isbn = {9781450369091},
	keywords = {3D geometries, GIS, GPU, hardware accelerators, spatial databases},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {199–208},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Large-scale 3D geospatial processing made possible},
	url = {https://doi.org/10.1145/3347146.3359351},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359356,
	abstract = {Imprecise composite location references formed using ad hoc spatial expressions in English text makes the geocoding task challenging for both inference and evaluation. Typically such spatial expressions fill in unestablished areas with new toponyms for finer spatial referents. For example, the spatial extent of the ad hoc spatial expression "north of" or "50 minutes away from" in relation to the toponym "Dayton, OH" refers to an ambiguous, imprecise area, requiring translation from this qualitative representation to a quantitative one with precise semantics using systems such as WGS84. Here we highlight the challenges of geocoding such referents and propose a general formal representation that employs background knowledge, semantic approximations and rules, and fuzzy linguistic variables. We also discuss an appropriate evaluation technique for the task that is based on human contextualized and subjective judgment.},
	address = {New York, NY, USA},
	author = {Al-Olimat, Hussein S. and Shalin, Valerie L. and Thirunarayan, Krishnaprasad and Sain, Joy Prakash},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359356},
	isbn = {9781450369091},
	keywords = {fuzzy spatial extents, geocoding, spatial expressions},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {75–78},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Towards Geocoding Spatial Expressions (Vision Paper)},
	url = {https://doi.org/10.1145/3347146.3359356},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359357,
	abstract = {The experimental research developed in this paper introduces a hybrid temporal GIS representation for studying the dynamics of a coastal seabed. The approach is based on a dual temporal GIS data model that combines the object and field views of space. The first concept is the one of an object-field defined as a field in which each location is associated to one or more geographic objects type. The second key concept is the field-object defined as an object with internal heterogeneity conceptualized as a field. The main idea behind this approach is to provide a flexible representation that supports field and object evolutions. A series of spatio-temporal queries have been categorized and specified on top of the hybrid representation and shows the interest of the combination of the field and object abstractions at the data manipulation level. The approach has been applied to a coastal data environment, and implemented within the PostgreSQL RDBMS and its spatial extension PostGIS.},
	address = {New York, NY, USA},
	author = {Hamdani, Younes and Thibaud, R\'{e}my and Claramunt, Christophe},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359357},
	isbn = {9781450369091},
	keywords = {Coastal Dynamics, Field-Object, Hybrid GIS, Object-Field},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {584–587},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {A Hybrid Temporal GIS Representation for Coastal Dynamics},
	url = {https://doi.org/10.1145/3347146.3359357},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359362,
	abstract = {Zoning Improvement Plan (ZIP) Codes provide a sub-division of space. Interestingly, the ZIP code area polygons for different data sources do not match, resulting in uncertainty for a range of services that rely on such data. This paper presents a system that employs traditional classification methods to map a given spatial coordinate to a distribution of ZIP-codes using various public available ZIP-code maps as predictors, and using the (not publicly available) United States Postal Service (USPS) map as an authoritative ground truth. We show that large sets of microblog data, from which we extract potential ZIP-codes, can significantly improve classification accuracy despite the noise of such data. The demonstrator allows users to select locations on a map of Orlando, FL, view the resulting distribution of ZIP-codes predicted for this location, compare the results to the ground-truth, and view the microblogs that have enriched the result. A focus will be on showing that the signal present in large, noisy, and 99.99\% unrelated microblog data can indeed be used to improve reverse ZIP code geo-coding.},
	address = {New York, NY, USA},
	author = {Khan, Tunaggina Subrina and Kabir, Anowarul and Pfoser, Dieter and Z\"{u}fle, Andreas},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359362},
	isbn = {9781450369091},
	keywords = {Geocoding, Location Based Services, Microblog Data, Reverse Geocoding, ZIP Code Classification, ZIP Codes},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {588–591},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {CrowdZIP: A System to Improve Reverse ZIP Code Geocoding using Spatial and Crowdsourced Data (Demo Paper)},
	url = {https://doi.org/10.1145/3347146.3359362},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359365,
	abstract = {OpenStreetMap (OSM) is a popular community-driven mapping platform with voluntary contributions from (amateur) cartographers. However, it is a difficult process for the cartographer to identify the areas where she can best contribute to OSM. Furthermore, the current OSM spatial entities are missing many tags; for example, top three road network tags, Name, Source, and Surface, are available only for the 10\% of the total road segments. Our paper aims to improve the quantity and quality of the road network tags by actively pushing the nearest road segments for the cartographer to be mapped. We propose a push-based spatial crowdsourcing method to achieve this objective, and validate it by focusing on road segments in OSM. Specifically, we formally define the batch-based maximum road segment task assignment problem and suggest methods based on heuristics like travel distance and road segment task grouping. Finally, our experimental evaluation verify the applicability of our assignment solutions by comparing the resulting number of assigned tasks. With regard to the number of assigned road segments, our junctions-based and road segment-based heuristic methods, outperform the baseline methods by five and two times, respectively.},
	address = {New York, NY, USA},
	author = {Gummidi, Srinivasa Raghavendra Bhuvan and Pedersen, Torben Bach and Xie, Xike and Zim\'{a}nyi, Esteban},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359365},
	isbn = {9781450369091},
	keywords = {OpenStreetMap, Road Network, Semantic Tags, Spatial Crowdsourcing, Task Assignment},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {532–535},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Push-based Spatial Crowdsourcing for Enriching Semantic Tags in OpenStreetMap},
	url = {https://doi.org/10.1145/3347146.3359365},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359366,
	abstract = {Searching for a parking spot in metropolitan areas is a great challenge comparable to the Hunger Games, especially in highly populated areas such as downtown districts and job centers. On-street parking is often a cost-effective choice compared to parking facilities such as garages and parking lots. However, limited space and complex parking regulation rules make the search process of on-street parking very difficult. To this end, we propose a data-driven framework for understanding and predicting the spatiotemporal availability of on-street parking using the NYC parking tickets open data, points of interest (POI) data and human mobility data. Four popular types of spatial analysis units (i.e., point, street, census tract, and grid) are used to examine the effects of spatial scale in machine learning predictive models. The results show that random forest works the best with the highest accuracy scores for the spatiotemporal availability classification across all four spatial analysis scales.},
	address = {New York, NY, USA},
	author = {Li, Mingxiao and Gao, Song and Liang, Yunlei and Marks, Joseph and Kang, Yuhao and Li, Moyin},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359366},
	isbn = {9781450369091},
	keywords = {data fusion, machine learning, spatiotemporal analytics, urban computing},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {536–539},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {A Data-Driven Approach to Understanding and Predicting the Spatiotemporal Availability of Street Parking},
	url = {https://doi.org/10.1145/3347146.3359366},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359367,
	abstract = {Libraries are digitizing their collections of maps from all eras, generating increasingly large online collections of historical cartographic resources. Aligning such maps to a modern geographic coordinate system greatly increases their utility. This work presents a method for such automatic georeferencing, matching raster image content to GIS vector coordinate data. Given an approximate initial alignment that has already been projected from a spherical geographic coordinate system to a Cartesian map coordinate system, a probabilistic shape-matching scheme determines an optimized match between the GIS contours and ink in the binarized map image. Using an evaluation set of 20 historical maps from states and regions of the U.S., the method reduces average alignment RMSE by 12\%.},
	address = {New York, NY, USA},
	author = {Howe, Nicholas R. and Weinman, Jerod and Gouwar, John and Shamji, Aabid},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359367},
	isbn = {9781450369091},
	keywords = {GIS, georeferencing, historical maps, vector-image alignment},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {540–543},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Deformable Part Models for Automatically Georeferencing Historical Map Images},
	url = {https://doi.org/10.1145/3347146.3359367},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359370,
	abstract = {Digitization of geographic regions, such as bodies of water, from remotely sensed imagery is a highly demanded yet arduous task. Though many automatic approaches for such digitization exist, they are typically geared towards specific regions of interest and sensing platforms, requiring complex workflow specification, parameter tuning, and/or manual training set creation. Moreover, the automatic digitization is performed all at once, making correction overwhelming when the result is not fully accurate. In this study, a general-purpose methodology that aims to greatly reduce the burden of geographic region digitization is proposed. This methodology specifies an interface for a human-machine team that exploits an incremental approach via online and active learning. With no prior training data or workflow definition, an effective pixel-level classifier is built in stride with a human user's adjustments to the machine's automatic vertex placement via novel interactive piecewise-linear contours. Several contour implementations specifically tailored towards geographic regions are presented along with results showing the effectiveness of each implementation. These contours work by spatially constraining the placement of vertices such that a machine implementation may effectively classify and train on pixel-level data for vertex placement after human verification or correction. An implementation of the methodology that uses a K nearest neighbors classifier and a simple instance-based estimation of uncertainty is presented along with several automatic vertex insertaion and placement strategies. Results show that the presented implementation succeeds in helping the user effectively annotate with no prior training data, yielding a vertex placement accuracy of 84\% overall. Finally, conclusions and current research directions initiated from our findings are discussed.},
	address = {New York, NY, USA},
	author = {Michael, Chris J. and Dennis, Steven M. and Maryan, Corey and Irving, Samuel and Palmsten, Margaret L.},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359370},
	isbn = {9781450369091},
	keywords = {geographic region digitization, human-centered machine learning, human-machine teams},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {259–268},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {A General Framework for Human-Machine Digitization of Geographic Regions from Remotely Sensed Imagery},
	url = {https://doi.org/10.1145/3347146.3359370},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359375,
	abstract = {In this paper, we describe Multi-Itinerary Optimization (MIO) - a novel Bing maps service that automates the process of building itineraries for multiple agents while optimizing their routes to save travel time or distance. MIO can be used by organizations with a fleet of vehicles and drivers, mobile salesforce, or a team of personnel in the field in order to maximize workforce efficiency. MIO accounts for service time windows, duration, and priority, as well as traffic conditions between locations, resulting in challenging algorithmic problems at multiple levels (e.g., calculating travel-time distance matrices at scale, scheduling services for multiple agents).To support an end-to-end cloud service with turnaround times of a few seconds, our algorithm design targets a sweet spot between accuracy and performance. Towards that end, we build a scalable solution based on the ALNS meta-heuristic. Our experiments show that accounting for traffic significantly improves solution quality: MIO not only avoids violating time-window constraints, but also completes up to 17\% more services compared to traffic-agnostic mechanisms. Further, our solution generates itineraries with better accuracy than both a cutting-edge heuristic (LKH3) and an Integer-Programming based algorithm, with twice and orders-of-magnitude faster running times, respectively.},
	address = {New York, NY, USA},
	author = {Cristian, Alexandru and Marshall, Luke and Negrea, Mihai and Stoichescu, Flavius and Cao, Peiwei and Menache, Ishai},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359375},
	isbn = {9781450369091},
	keywords = {route optimization, time-dependent travel, traffic distance matrix},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {279–288},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Multi-Itinerary Optimization as Cloud Service (Industrial Paper)},
	url = {https://doi.org/10.1145/3347146.3359375},
	year = {2019},
}

@inproceedings{10.1145/3347146.3359382,
	abstract = {With the increasing awareness towards protecting environment, people are paying more attention to the electric vehicles (EVs). Accompanying the rapid growing number of EVs, challenges raise at the same time about how to place EV chargers (EVC), within a city, to satisfy multiple types of charging demand. To provide a better EVC station deployment plan to benefit the whole society, we propose a problem called Social-Aware Optimal Electric Vehicle Charger Deployment (SOCD) on road network. The SOCD problem is hard and different from existing work in three aspects, 1) we assume that the charging demand should be satisfied not only in urban areas but also in relatively rural areas; 2) our work is the first one that considers an EVC station should have multiple types of charging plugs, which is more reasonable in real world; 3) different from the regional deployment solutions in previous literature, our SOCD directly works on a real road network and EVC stations are placed at appropriate POIs laying on the road network. We show that the SOCD problem is NP-hard. To deal with the hardness, we design two heuristic algorithms whose efficiency and effectiveness can be experimentally demonstrated. Furthermore, we investigate the incremental case, that is, given an existing EVC station deployment plan and extra more budget, we need to decide where and how many to place more chargers. Finally, we conduct extensive experiments on real road network of Shanghai to demonstrate both effectiveness and efficiency of our algorithms.},
	address = {New York, NY, USA},
	author = {Liu, Qiyu and Zeng, Yuxiang and Chen, Lei and Zheng, Xiuwen},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3359382},
	isbn = {9781450369091},
	keywords = {Electric Vehicle, Road Network, Social-Aware},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {398–407},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Social-Aware Optimal Electric Vehicle Charger Deployment on Road Network},
	url = {https://doi.org/10.1145/3347146.3359382},
	year = {2019},
}

@inproceedings{10.1145/3347146.3363348,
	abstract = {We present a weighted sampling strategy for distributing a system of taxi agents on a road network. We consider a setting, in which each agent operates independently, following a prescribed strategy based on historical data. Furthermore, customer requests appear dynamically and are assigned to the closest unoccupied taxi agent.We demonstrate that in this setting a simple sampling strategy based on the spatial distribution of historical data performs well in minimizing the average time that agents are unoccupied. The strategy is evaluated on taxi trip data in Manhattan and compared to various, more complex strategies.},
	address = {New York, NY, USA},
	author = {Buchin, Kevin and Kostitsyna, Irina and Custers, Bram and Struijs, Martijn},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3363348},
	isbn = {9781450369091},
	keywords = {GIS Cup, dial-a-ride, dynamic scheduling, multi-agent system, taxi routing, trajectories},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {616–619},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {A Sampling-based Strategy for Distributing Taxis in a Road Network for Occupancy Maximization (GIS Cup)},
	url = {https://doi.org/10.1145/3347146.3363348},
	year = {2019},
}

@inproceedings{10.1145/3347146.3363350,
	abstract = {Congested traffic wastes billions of liters of fuel and is a significant contributor to Green House Gas (GHG) emissions. Although convenient, ride sharing services such as Uber and Lyft are becoming a significant contributor to these emissions not only because of added traffic but by spending time on the road while waiting for passengers. To help improve the impact of ride sharing, we propose an algorithm to optimize the efficiency of drivers searching for customers. In our model, the main goal is to direct drivers represented as idle agents, i.e., not currently assigned a customer or resource, to locations where we predict new resources to appear. Our approach uses non-negative matrix factorization (NMF) to model and predict the spatio-temporal distributions of resources. To choose destinations for idle agents, we employ a greedy heuristic that strikes a balance between distance greed, i.e., to avoid long trips without resources and resource greed, i.e., to move to a location where resources are expected to appear following the NMF model. To ensure that agents do not oversupply areas for which resources are predicted and under supply other areas, we randomize the destinations of agents using the predicted resource distribution within the local neighborhood of an agent. Our experimental evaluation shows that our approach reduces the search time of agents and the wait time of resources using real-world data from Manhattan, New York, USA.},
	address = {New York, NY, USA},
	author = {Kim, Joon-Seok and Pfoser, Dieter and Z\"{u}fle, Andreas},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3363350},
	isbn = {9781450369091},
	keywords = {Non-negative matrix factorization, discrete event simulation, distance-aware search, simulation, spatio-temporal search},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {624–627},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Distance-Aware Competitive Spatiotemporal Searching Using Spatiotemporal Resource Matrix Factorization (GIS Cup)},
	url = {https://doi.org/10.1145/3347146.3363350},
	year = {2019},
}

@inproceedings{10.1145/3347146.3363463,
	abstract = {This paper proposes an automatic system to generate a large amount of data for the training of text detection systems for historical maps. The system takes online maps as input and learns a conditional GAN model, to generate realistic historical map images from existing geographic datasets. Then the system uses the generated images as the base map and inserts synthetic text. Since the system has the control of text content, font style, and location, the system can obtain ground truth information (minimum bounding boxes) of the synthetic text. To overcome the challenge of content mismatch, the proposed system uses a novel loss function to encourage the generation of historical cartographic symbols in the foreground areas and discourage the generation in the background. The final output is a set of images resembling historical maps and the minimum bounding boxes around text regions on the images as annotations.},
	address = {New York, NY, USA},
	author = {Li, Zekun},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3363463},
	isbn = {9781450369091},
	keywords = {Generative Adversarial Networks, Historical Map Processing},
	location = {Chicago, IL, USA},
	numpages = {2},
	pages = {610–611},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Generating Historical Maps from Online Maps},
	url = {https://doi.org/10.1145/3347146.3363463},
	year = {2019},
}

@inproceedings{10.1145/3347146.3363464,
	abstract = {This study investigates human activity community in a city by conceptualizing it as a network embedding problem. In order to learn the latent representations of activity-travel patterns from individual daily trajectories, network embedding learns a vector space representation for each type of activity place as a node connected by movement links to preserve the structure of individual activities. The proposed approach is applied to mobile positioning data at the individual level obtained for a weekday from volunteers at Guangzhou City. Assessments are conducted to validate individual decision making for several types of activities by a field survey. This study contributes to a general framework for discovering individual activity-travel patterns from human movement trajectories.},
	address = {New York, NY, USA},
	author = {Huang, Tianyuan},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3363464},
	isbn = {9781450369091},
	keywords = {Geographic knowledge discovery, Graph representation, Network embedding, Urban planning},
	location = {Chicago, IL, USA},
	numpages = {2},
	pages = {612–613},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {SRC: Discovering Human Activity Community in A City},
	url = {https://doi.org/10.1145/3347146.3363464},
	year = {2019},
}

@inproceedings{10.1145/3347146.3368469,
	abstract = {Information, not just data, is key to today's global challenges. To solve these challenges requires not only advancing geospatial and big data analytics but requires new analysis and decision-making environments that enable reliable decisions from trustable, understandable information that go beyond current approaches to machine learning and artificial intelligence. These environments are successful when they effectively couple human decision making with advanced, guided spatial analytics in human-computer collaborative discourse and decision making (HCCD). Our HCCD approach builds upon visual analytics, natural scale templates, traceable information, human-guided analytics, and explainable and interactive machine learning, focusing on empowering the decisionmaker through interactive visual spatial analytic environments where non-digital human expertise and experience can be combined with state-of-the-art and transparent analytical techniques. When we combine this approach with real-world application-driven research, not only does the pace of scientific innovation accelerate, but impactful change occurs. I'll describe how we have applied these techniques to challenges in sustainability, security, resiliency, public safety, and disaster management.},
	address = {New York, NY, USA},
	author = {Ebert, David S.},
	booktitle = {Proceedings of the 27th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3347146.3368469},
	isbn = {9781450369091},
	keywords = {human-computer collaborative disclosure and decision making (HCCD), trusted information, visual spatial analytics},
	location = {Chicago, IL, USA},
	numpages = {1},
	pages = {2},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '19},
	title = {Visual Spatial Analytics and Trusted Information for Effective Decision Making},
	url = {https://doi.org/10.1145/3347146.3368469},
	year = {2019},
}

@inproceedings{10.1145/3348445.3348479,
	abstract = {The government agencies require decision support information before commencing their community development projects in rural areas. However, such information is not always available or does not meet their requirements. This research presents the design and development of the WebGIS, which is intended to store and provide spatially related household information for government agencies. This research has been conducted as a part of a provincial big data project. In this research, the spatial database system and the data visualization of the database were designed and developed by focusing on the details of each house in the targeted villages. The data were collected by the researchers from the study areas, which comprised 5 villages in Loei and Khonkaen Provinces in Thailand. The important household and location data were collected and combined with the community data from the Community Development Office. The GIS was developed using QGIS where the geolocation of each house in the villages was applied on the map derived from Google map. The data were analyzed and visualized in different formats such as color, table, and graph in order to establish the data classification and summarization. The system and data were finally evaluated by the Community Development Office and community leaders in terms of system performance and data accuracy.},
	address = {New York, NY, USA},
	author = {Puarungroj, Wichai and Phromkhot, Suchada and Boonsirisumpun, Narong and Pongpatrakant, Pathapong and Sangpradid, Satith},
	booktitle = {Proceedings of the 7th International Conference on Computer and Communications Management},
	doi = {10.1145/3348445.3348479},
	isbn = {9781450371957},
	keywords = {WebGIS, Village improvement, Spatial database, Geographic information systems, Community development},
	location = {Bangkok, Thailand},
	numpages = {4},
	pages = {115–118},
	publisher = {Association for Computing Machinery},
	series = {ICCCM '19},
	title = {WebGIS for Managing Household Data within a Provincial Big Data Project},
	url = {https://doi.org/10.1145/3348445.3348479},
	year = {2019},
}

@article{10.1145/3349264,
	abstract = {Navigation assistive technologies are designed to support people with visual impairments during mobility. In particular, turn-by-turn navigation is commonly used to provide walk and turn instructions, without requiring any prior knowledge about the traversed environment. To ensure safe and reliable guidance, many research efforts focus on improving the localization accuracy of such instruments. However, even when the localization is accurate, imprecision in conveying guidance instructions to the user and in following the instructions can still lead to unrecoverable navigation errors. Even slight errors during rotations, amplified by the following frontal movement, can result in the user taking an incorrect and possibly dangerous path.In this article, we analyze trajectories of indoor travels in four different environments, showing that rotation errors are frequent in state-of-art navigation assistance for people with visual impairments. Such errors, caused by the delay between the instruction to stop rotating and when the user actually stops, result in over-rotation. To compensate for over-rotation, we propose a technique to anticipate the stop instruction so that the user stops rotating closer to the target rotation. The technique predicts over-rotation using a deep learning model that takes into account the user’s current rotation speed, duration, and angle; the model is trained with a dataset of rotations performed by blind individuals. By analyzing existing datasets, we show that our approach outperforms a naive baseline that predicts over-rotation with a fixed value. Experiments with 11 blind participants also show that the proposed compensation method results in lower rotation errors (18.8° on average) compared to the non-compensated approach adopted in state-of-the-art solutions (30.1°).},
	address = {New York, NY, USA},
	articleno = {19},
	author = {Ahmetovic, Dragan and Mascetti, Sergio and Bernareggi, Cristian and Guerreiro, Jo\~{a}o and Oh, Uran and Asakawa, Chieko},
	doi = {10.1145/3349264},
	issn = {1936-7228},
	issue_date = {December 2019},
	journal = {ACM Trans. Access. Comput.},
	keywords = {Turn-by-turn navigation, navigation assistance, orientation 8 mobility},
	month = {dec},
	number = {4},
	numpages = {19},
	publisher = {Association for Computing Machinery},
	title = {Deep Learning Compensation of Rotation Errors During Navigation Assistance for People with Visual Impairments or Blindness},
	url = {https://doi.org/10.1145/3349264},
	volume = {12},
	year = {2019},
}

@inproceedings{10.1145/3349341.3349503,
	abstract = {With the development of Internet of Things, 3D GIS, BIM and other technologies, 3D GIS has been widely used in urban planning, urban design, public safety and urban engineering. At the same time, 3D GIS provides a reliable tool for disaster prevention and mitigation, emergency response and management, especially for emergency disposal in urban areas. Real-time environment and analysis tools provided by 3D GIS can realize disaster event recurrence, deduction and disaster management analysis, and provide effective strategic support for disaster reduction and emergency response.This paper mainly analyses the structure of urban disaster emergency response system, studies the urban logical structure and analysis the method based on network structure, extracts the most important factors and information in urban emergency management through the analysis of the complex structure inside the city, establishes the three-dimensional urban environment and urban network logical model by using 3D GIS technology, and simulates the process of urban emergency disposal. Meanwhile, the feasibility of the application of the system in the whole urban emergency management system is investigated through the construction of the urban emergency response system with community as the unit.},
	address = {New York, NY, USA},
	author = {Cui, Bo and Wen, Xin and Zhang, Dongdong},
	booktitle = {Proceedings of the 2019 International Conference on Artificial Intelligence and Computer Science},
	doi = {10.1145/3349341.3349503},
	isbn = {9781450371506},
	keywords = {Urban Underground Space, Internet of Things (IoT), Disaster Intelligence Emergency Response System},
	location = {Wuhan, Hubei, China},
	numpages = {5},
	pages = {745–749},
	publisher = {Association for Computing Machinery},
	series = {AICS 2019},
	title = {The Application of Intelligent Emergency Response System for Urban Underground Space Disasters Based on 3D GIS, BIM and Internet of Things},
	url = {https://doi.org/10.1145/3349341.3349503},
	year = {2019},
}

@inproceedings{10.1145/3356392.3365218,
	abstract = {In natural outdoor environments, the shape of the surface terrain is an important factor in selecting a traversal path, both when operating off-road vehicles and maneuvering on foot. With the increased availability of digital elevation models for outdoor terrain, new opportunities exist to exploit this contextual information to improve automated path prediction. In this paper, we investigate predictive neural network models for outdoor trajectories that traverse terrain with known surface topography. We describe a method of encoding digital surface models as vectors in latent space using Wasserstein Autoencoders, and their use in convolutional neural networks that predict future trajectory positions from past trajectory data. We observe gains in predictive performance across three experiments, using both synthetic and recorded trajectories on real-world terrain.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Feng, Andrew and Gordon, Andrew S.},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Computing with Multifaceted Movement Data},
	doi = {10.1145/3356392.3365218},
	isbn = {9781450369510},
	keywords = {latent representation, neural networks, trajectory prediction},
	location = {Chicago, IL, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {MOVE'19},
	title = {Latent Terrain Representations for Trajectory Prediction},
	url = {https://doi.org/10.1145/3356392.3365218},
	year = {2019},
}

@inproceedings{10.1145/3356392.3365220,
	abstract = {In the analysis and visualisation of clustered spatial trajectories, the computation of a representative trajectory for a given cluster of data trajectories plays an important role. Usually, such a representative trajectory is computed based upon the data trajectories' spatial characteristics only, e. g., as an average, median, or central trajectory. However, in many cases, the input data is enriched by various types of semantic information which may document characteristics of the trajectories as well. We present an approach to inferring representative trajectories for a given cluster of trajectories. Our approach constructs an extended finite state machine describing the spatial and non-spatial properties of the data trajectories in a given cluster. This extended finite state machine then can be used to generate a representative trajectory exhibiting characteristic changes in spatial and non-spatial properties. The extended finite state machine constructed is annotated with these changes, hence enabling domain experts to further analyse and assess the constructed representative trajectory.},
	address = {New York, NY, USA},
	articleno = {3},
	author = {Seep, Jana and Vahrenhold, Jan},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Computing with Multifaceted Movement Data},
	doi = {10.1145/3356392.3365220},
	isbn = {9781450369510},
	keywords = {Representative Trajectory, Trajectory Analysis},
	location = {Chicago, IL, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {MOVE'19},
	title = {Inferring Semantically Enriched Representative Trajectories},
	url = {https://doi.org/10.1145/3356392.3365220},
	year = {2019},
}

@inproceedings{10.1145/3356392.3365222,
	abstract = {Movement data, as captured by myriad sensors, has been growing exponentially. Hence, multidisciplinary approaches for analyzing movement has become feasible. Though, movement pertains to a large variety of domains and applications, the focus of this position paper is understanding human movement (mobility) in various forms. We position maps as heterogeneous, multidimensional and digital representation of reality and advocate their role in contextualizing movement. We overview the main problems for analyzing human mobility with special attention to movement in context, leveraging heterogeneous data. We review the state-of-the-art in solving these problems and describe remaining open problems and challenges for future work. Finally, we offer a view of existing as well as future mapping and location services that could enable these.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Derin, Onur and Mitra, Aniket and Stroila, Matei and Custers, Bram and Meulemans, Wouter and Roeloffzen, Marcel and Verbeek, Kevin},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Computing with Multifaceted Movement Data},
	doi = {10.1145/3356392.3365222},
	isbn = {9781450369510},
	keywords = {Movement data, context, maps, trajectories},
	location = {Chicago, IL, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {MOVE'19},
	title = {Understanding Movement in Context with Heterogeneous Data},
	url = {https://doi.org/10.1145/3356392.3365222},
	year = {2019},
}

@inproceedings{10.1145/3356395.3365542,
	abstract = {Natural hazards have been resulting in severe damage to our cities, and flooding is one of the most disastrous in the U.S and worldwide. Therefore, it is critical to develop efficient methods for risk and damage assessments after natural hazards, such as flood depth estimation. Existing works primarily leverage photos and images capturing flood scenes to estimate flood depth using traditional computer vision and machine learning techniques. However, the advancement of deep learning (DL) methods make it possible to estimate flood depth more accurate. Therefore, based on state-of-the-art DL technique (i.e., Mask R-CNN) and publicly available images from the Internet, this study aims to investigate and improve the flood depth estimation. Specifically, human objects are detected and segmented from flooded images to infer the floodwater depth. This study provides a new framework to extract critical information from large accessible online data for rescue teams or even robots to carry out appropriate plans for disaster relief and rescue missions in the urban area, shedding lights on the real-time detection of the flood depth.},
	address = {New York, NY, USA},
	author = {Meng, Zonglin and Peng, Bo and Huang, Qunying},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances on Resilient and Intelligent Cities},
	doi = {10.1145/3356395.3365542},
	isbn = {9781450369541},
	keywords = {Detection, Flood depth, Mask R-CNN, Resilient},
	location = {Chicago, IL, USA},
	numpages = {4},
	pages = {37–40},
	publisher = {Association for Computing Machinery},
	series = {ARIC'19},
	title = {Flood Depth Estimation from Web Images},
	url = {https://doi.org/10.1145/3356395.3365542},
	year = {2019},
}

@inproceedings{10.1145/3356395.3365598,
	abstract = {The amount of spatial data acquired from crowdsourced platforms, mobile devices, sensors and cartographic agencies has grown exponentially over the past few years. Nearly half of the spatial data available currently are stored and processed through large relational databases. Due to a lack of generic open source tools, researchers and analysts often have difficulty in extracting and analyzing large amounts of spatial data from traditional databases. In order to overcome this challenge, the most effective way is to perform the analysis directly in the database, which enables quick retrieval and visualization of spatial data stored in relational databases. Also, working in-database reduces the network overhead, as users do not need to replicate the complete data into their local system. While a number of spatial analysis libraries are readily available, they do not work in-database, and typically require additional platform-specific software. Our goal is to bridge this gap by developing a new method through an open source software to perform fast and seamless spatial analysis without having to store the data in-memory. We propose a framework implemented in Python, which embeds geospatial analytics into a spatial database (i.e. IBM DB2 ®). The framework internally translates the spatial functions written by the user into SQL queries, which follow the standards of Open Geospatial Consortium (OGC) and can operate on single as well as multiple geometries. We then demonstrate how to combine the results of spatial operations with visualization methods such as choropleth maps within Jupyter notebooks. Finally, we elaborate upon the benefits of our approach via a real-world use case, in which we analyze crime hotspots in New York City using the in-database spatial functions.},
	address = {New York, NY, USA},
	author = {Roy, Avipsa and Fouch\'{e}, Edouard and Morales, Rafael Rodriguez and M\"{o}hler, Gregor},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances on Resilient and Intelligent Cities},
	doi = {10.1145/3356395.3365598},
	isbn = {9781450369541},
	keywords = {In-database analytics, Maps, Python, crime analysis, geospatial analytics, spatial data},
	location = {Chicago, IL, USA},
	numpages = {8},
	pages = {17–24},
	publisher = {Association for Computing Machinery},
	series = {ARIC'19},
	title = {In-Database Geospatial Analytics using Python},
	url = {https://doi.org/10.1145/3356395.3365598},
	year = {2019},
}

@inproceedings{10.1145/3356470.3365528,
	abstract = {The confluence of spatial statistics and simulation has stimulated the study of geographic phenomena for decades. However, the inclusion of the temporal dimension has been insufficiently addressed within the domain of geographic information science. This research focuses on spatiotemporal point pattern analysis to gain insight into this issue and discusses new approaches to facilitate their visualization and analysis in space and time. These methods provide support for statistically robust estimation on spatiotemporally explicit characteristics of point patterns using Monte Carlo simulation. We present a case study featuring spatiotemporal point pattern analysis within the context of dengue fever in the city of Cali, Colombia. Specifically, we use local Ripley's K function to estimate the spatiotemporal signature of dengue and find the scales at which clustering is most significant. We use this information to select bandwidths for adaptive space-time kernel density estimation. The analysis results indicate that simulation is pivotal to promoting our understanding of space-time complexity in dynamic spatial phenomena, represented by the dengue fever.},
	address = {New York, NY, USA},
	author = {Hohl, Alexander and Chen, Peilin},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
	doi = {10.1145/3356470.3365528},
	isbn = {9781450369565},
	keywords = {Monte Carlo, Ripley's K, kernel density estimation, simulation, space-time},
	location = {Chicago, Illinois},
	numpages = {8},
	pages = {16–23},
	publisher = {Association for Computing Machinery},
	series = {GeoSim '19},
	title = {Spatiotemporal simulation: local Ripley's K function parameterizes adaptive kernel density estimation},
	url = {https://doi.org/10.1145/3356470.3365528},
	year = {2019},
}

@inproceedings{10.1145/3356470.3365529,
	abstract = {With natural disasters have become large scale, diversified, and frequent, the indirect economic damage due to interruption of supply chain tends to be large. Therefore, it is important to recover as quickly as possible for companies after disasters. In this paper, we use reinforcement learning to optimize a company's action strategy so that it can efficiently recover the inter-firm transaction network in the supply chain after large-scale urban flooding. The agent holds information on disaster and supply chains obtained from inter-firm transaction data and flood simulation analysis data, enabling us to create a simulation with detailed urban infrastructure information by using the high-dimensional data to construct detailed spatial states. The paper also proposes an action policy for companies based on multi-agent deep reinforcement learning to optimize the behavior of companies in the recovery process. This work bridges the divide between high-dimensional data set input and post-disaster behaviors, enabling an artificial agent to learn the best action to take after a disaster. Our results are as follows. Through learning, agents can recover efficiently after a disaster. Companies tend to secure alternative business partners first and then perform recovery work and business expansion.},
	address = {New York, NY, USA},
	author = {Yang, Shaofeng and Ogawa, Yoshiki and Ikeuchi, Koji and Akiyama, Yuki and Shibasaki, Ryosuke},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
	doi = {10.1145/3356470.3365529},
	isbn = {9781450369565},
	keywords = {deep reinforcement learning, disaster management, inter-firm transaction data, multi-agent, supply chain disruption},
	location = {Chicago, Illinois},
	numpages = {4},
	pages = {24–27},
	publisher = {Association for Computing Machinery},
	series = {GeoSim '19},
	title = {Firm-level behavior control after large-scale urban flooding using multi-agent deep reinforcement learning},
	url = {https://doi.org/10.1145/3356470.3365529},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365227,
	abstract = {We show how to infer land use from mobility traces. Instead of using manual surveys or aerial image interpretation, our research shows how to use mobility traces to infer the number of businesses and residences in each cell of a grid on the ground. Our mobility traces give the location of people at different times of day. Using mobility data from 1.8 million users, we build regression models that map from average hourly occupancy in a grid cell to the number of businesses and residences inside the cell. We show which hours of the day are most indicative of the contents of a cell.},
	address = {New York, NY, USA},
	author = {Krumm, John and Krumm, Kora},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365227},
	isbn = {9781450369572},
	keywords = {ground use, land use, location, mobility traces},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {1–4},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {Land Use Inference from Mobility Traces},
	url = {https://doi.org/10.1145/3356471.3365227},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365228,
	abstract = {E-taxies (ETs) are facing great challenges such as short driving range, long charging time and sparse charging stations, thus hamper its acceptance by fuel taxi drivers. This study presents a novel spatial-temporal intelligent recommendation system for e-taxi drivers to improve their net revenue. The knowledge of taxi travels, including the probability of picking-up passengers and destinations, is learned from fuel taxies' raw GPS trajectories to estimate the expected net revenue (ENR) of the e-taxi. Consecutive actions of ET drivers are modeled by action trees to find the best route going to a recharge or cruising along some roads. An online recommendation querying subsystem is developed for high-efficient real-time recommendation. An experiment in Shenzhen using GPS trajectories of 16, 146 fuel taxies is conducted to evaluate the performance. The result shows that, by adopting the proposed system, the net revenue per unit working time of the ET drivers is up to 91.4\% better than real-world fuel taxi drivers.},
	address = {New York, NY, USA},
	author = {Mai, Ke and Tu, Wei and Li, Qingquan and Zhao, Tianzhong and Zhang, Yatao and Ye, Haoyu},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365228},
	isbn = {9781450369572},
	keywords = {Action tree search, Electric taxies, GPS trajectories, Taxi recommendation},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {5–8},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {STIETR: Spatial-temporal Intelligent E-Taxi Recommendation System Using GPS Trajectories},
	url = {https://doi.org/10.1145/3356471.3365228},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365230,
	abstract = {Collecting economic data like household income through traditional survey methods is expensive and time consuming making it scarce, especially for developing countries. Satellite nighttime light data and geospatial factors like the distance from a major metropolitan area have been found to correlate with a myriad of economic indicators. However, no studies have incorporated such variables to estimate the median household income for administrative units of a country. We initially performed regression analysis by taking sub-district median household incomes of Thailand as the dependent variable. The independent variables chosen were nighttime light statistics, Euclidean distances from two major metropolitan provinces, population density estimates, and vehicle road density, all calculated from geospatial data. The regression model yielded a R2 score of 0.57. This result showed that the independent variables can explain a good portion of the variability of median income. Building on this result, we used K-Means clustering to discretize the median income to 3 ordinal levels to form a classification problem. Using these levels as the target, we propose a machine learning approach that incorporates the aforementioned independent variables to estimate median income levels of sub-districts of Thailand. Our classifier achieved a F1 score of 0.82. Our study shows the robustness of satellite and geospatial data in classifying low and high income regions at a granularity useful for policy makers.},
	address = {New York, NY, USA},
	author = {Dorji, Ugyen Jigten and Plangprasopchok, Anon and Surasvadi, Navaporn and Siripanpornchana, Chaiyaphum},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365230},
	isbn = {9781450369572},
	keywords = {income estimation, machine learning, nighttime light data, spatial data},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {11–14},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {A machine learning approach to estimate median income levels of sub-districts in Thailand using satellite and geospatial data},
	url = {https://doi.org/10.1145/3356471.3365230},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365233,
	abstract = {Illegal parking is a critical problem in large, growing cities. Currently, the responsibility for detecting illegally parked vehicles has been left to law enforcement, which often requires manual inspection. To improve the efficiency of law enforcement for vehicle parking management, we propose a web-based analytic platform that leverages recent advancements in computer vision. This proposed platform provides an algorithm to improve the performance of detecting vehicle license plates from videos, based on an existing deep learning approach. Also, we provide a method to estimate vehicle parking locations. This platform is applicable for videos of security patrolling. End-users can define restricted zones via a map-based interface and all vehicles located in these areas can be efficiently identified once patrolling videos are received. This system is evaluated by two videos captured in real-world parking lots. The results indicate that the proposed platform can successfully identify vehicle plate numbers and estimate their parking locations to support the management of urban parking infrastructure.},
	address = {New York, NY, USA},
	author = {Yin, Zhengcong and Xiong, Haoyi and Zhou, Xun and Goldberg, Daniel W. and Bennett, Dave and Zhang, Chong},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365233},
	isbn = {9781450369572},
	keywords = {GeoAI, Illegal Parking, Location Estimation, Plate Number Extraction},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {32–35},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {A Deep Learning based Illegal Parking Detection Platform},
	url = {https://doi.org/10.1145/3356471.3365233},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365234,
	abstract = {To facilitate the reconstruction of high-quality road networks, intersections as the key locations provide valuable information about the network topology. However, only a few efforts have been made on the data-driven automatic detection of intersections from, e.g., large-scale GPS trajectories. To bridge the gap, we propose a machine learning based intersection detection approach based on large-scale real-world GPS trajectories of drivers from the Grab ride-hailing service. Instead of representing locations with vector descriptors, we innovatively propose a graph representation that models a location together with its local surroundings to improve the descriptiveness of the location descriptors. Moreover, we present a multi-scale graph convolutional network (GCN) to generate robust graph-level descriptors, followed by logistic regression to discriminate intersections from non-intersections. The experimental results show that our proposed multi-scale graph model outperforms the conventional multi-scale vector representation by 8.5\%. Appealingly, the proposed graph representation can be considered as a general location descriptor, which can be used in a variety of geo-based applications other than intersection detection for location modeling.},
	address = {New York, NY, USA},
	author = {Yin, Yifang and Sunderrajan, Abhinav and Huang, Xiaocheng and Varadarajan, Jagannadan and Wang, Guanfeng and Sahrawat, Dhruva and Zhang, Ying and Zimmermann, Roger and Ng, See-Kiong},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365234},
	isbn = {9781450369572},
	keywords = {Intersection detection, graph convolutional network, large-scale real-world GPS data, multi-scale feature fusion},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {36–39},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {Multi-scale Graph Convolutional Network for Intersection Detection from GPS Trajectories},
	url = {https://doi.org/10.1145/3356471.3365234},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365235,
	abstract = {Urban flood mapping is essential for disaster rescue and relief missions, reconstruction efforts, and financial loss evaluation. Much progress has been made to map the extent of flooding with multi-source remote sensing imagery and pattern recognition algorithms. However, urban flood mapping at high spatial resolution remains a major challenge due to three main reasons: (1) the very high resolution (VHR) optical remote sensing imagery often has heterogeneous background involving various ground objects (e.g., vehicles, buildings, roads, and trees), making traditional classification algorithms fail to capture the underlying spatial correlation between neighboring pixels within the flood hazard area; (2) traditional flood mapping methods with handcrafted features as input cannot fully leverage massive available data, which requires robust and scalable algorithms; and (3) due to inconsistent weather conditions at different time of data acquisition, pixels of the same objects in VHR optical imagery could have very different pixel values, leading to the poor generalization capability of classical flood mapping methods. To address this challenge, this paper proposed a residual patch similarity convolutional neural network (ResPSNet) to map urban flood hazard zones using bi-temporal high resolution (3m) pre- and post-flooding multispectral surface reflectance satellite imagery. Besides, remote sensing specific data augmentation was also developed to remove the impact of varying illuminations due to different data acquisition conditions, which in turn further improves the performance of the proposed model. Experiments using the high resolution imagery before and after the 2017 Hurricane Harvey flood in Houston, Texas, showed that the developed ResPSNet model, along with associated remote sensing specific data augmentation method, can robustly produce flood maps over urban areas with high precision (0.9002), recall (0.9302), F1 score (0.9128), and overall accuracy (0.9497). The research sheds light on multitemporal image fusion for high precision image change detection, which in turn can be used for monitoring natural hazards.},
	address = {New York, NY, USA},
	author = {Peng, Bo and Liu, Xinyi and Meng, Zonglin and Huang, Qunying},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365235},
	isbn = {9781450369572},
	keywords = {Flood mapping, deep learning, flood extent estimation, patch similarity, residual learning},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {8},
	pages = {40–47},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {Urban Flood Mapping with Residual Patch Similarity Learning},
	url = {https://doi.org/10.1145/3356471.3365235},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365237,
	abstract = {Traffic rules characterize the relationship of connected roads, and are fundamental data in map. They play an important role in route planning, driver navigation, and autonomous driving system. The traffic rule update is a key ability of cartography since they change day by day in real world. Trajectory mining based method has been proposed to update traffic rule. However, it is usually effected by the trajectory outliers and hard to improve the accuracy. In this paper, we consider using data from a different source, the street images captured by driving vehicle recorders (DVR), and propose a traffic sign driven system to update the rules. To collect candidate traffic rule changes, we train an object detection model to detect the traffic signs in street images. To improve the flexibility of the system, we propose a model compression method to reduce the model size, and integrate it into DVR. Finally, we propose a spatio-temporal attention method to cluster the recognized rules. Our system supports the updating of many types of traffic rules, such as no left/right/u turn, no parking, speed limit, etc., and has high extendibility. We validate our image recognition algorithm in a real world dataset, and it achieves a precision of 99.2\% and recall rate of 95.1\% for image level output. It demonstrates the advantage of our proposed system.},
	address = {New York, NY, USA},
	author = {Xing, Tengfei and Gu, Yang and Song, Zhichao and Wang, Zhihui and Meng, Yiping and Ma, Nan and Xu, Pengfei and Hu, Runbo and Chai, Hua},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365237},
	isbn = {9781450369572},
	keywords = {deep learning, heterogeneous platform deployment, map update, road network structure, traffic sign recognition},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {52–55},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {A Traffic Sign Discovery Driven System for Traffic Rule Updating},
	url = {https://doi.org/10.1145/3356471.3365237},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365239,
	abstract = {The success of Just-in-Time Adaptive Interventions (JITAIs) is contingent on the application's ability to understand, predict, and intervene upon the health behavior of interest. Advances in mobile technologies and wearable sensors are presenting further opportunities for "in the moment" intervention, however, integration of spatial and complex temporal concepts have been minimal in JITAIs to date. Here we share thoughts on how health behaviors can be contextualized in space and time with the purpose of predicting health behaviors through the use of GeoAI JITAI models, and highlight future challenges that need to be addressed.},
	address = {New York, NY, USA},
	author = {Yang, Jiue-An and Jankowska, Marta M.},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365239},
	isbn = {9781450369572},
	keywords = {GIS, GeoAI JITAI, health behavior, machine learning},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {3},
	pages = {66–68},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {Contextualizing Space and Time for GeoAI JITAIs (Just-in-Time Adaptive Interventions)},
	url = {https://doi.org/10.1145/3356471.3365239},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365240,
	abstract = {The application of deep learning techniques to remote sensing and geospatial data is a burgeoning area of research. However, most state-of-the-art systems have their roots in computer vision with procedures that do not necessarily lend themselves to remote sensing data. On the contrary, managing geospatial data require handling different projections, spatial resolutions and data formats. We present Keras Spatial, a python package for preprocessing and augmenting geospatial data. Keras Spatial provides three main components (1) a spatial data generator class, which is similar to the Keras image data generator, (2) a GeoDataFrame for storing training samples boundaries and properties, (3) a callback mechanism for on-the-fly reprojection and data augmentation. The novelty of the developed package arises due to the flexibility to combine the components in different ways to solve a variety of data preparation problems. We demonstrate the usage of Keras Spatial package by preparing digital elevation data for a segmentation model, and replacing conventional manual preprocessing steps, such as tiling rasters to samples, masking samples outside of the study area, and adding digital elevation model derivatives. We also discuss advanced data preprocessing features of this package, such as accessing remote data source directly, combining different input rasters data regardless of their native projection and resolution, and decoupling the model configuration, input layer dimensions, from the geographic scale, where the latter feature allows training the same model to recognize geographic objects that exist at hierarchical scales.},
	address = {New York, NY, USA},
	author = {Soliman, Aiman and Terstriep, Jeffrey},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365240},
	isbn = {9781450369572},
	keywords = {Deep Learning, Geospatial Big Data, Image Preprocessing, Remote Sensing, Scientific Reproducibility},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {8},
	pages = {69–76},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {Keras Spatial: Extending deep learning frameworks for preprocessing and on-the-fly augmentation of geospatial data},
	url = {https://doi.org/10.1145/3356471.3365240},
	year = {2019},
}

@inproceedings{10.1145/3356471.3365243,
	abstract = {Real-time tweets can provide useful information on evolving events and situations. Geotagged tweets are especially useful, as they indicate the location of origin and provide geographic context. However, only a small portion of tweets are geotagged, limiting their use for situational awareness. In this paper, we adapt, improve, and evaluate a state-of-the-art deep learning model for city-level geolocation prediction, and integrate it with a visual analytics system tailored for real-time situational awareness. We provide computational evaluations to demonstrate the superiority and utility of our geolocation prediction model within an interactive system.},
	address = {New York, NY, USA},
	author = {Snyder, Luke S. and Karimzadeh, Morteza and Chen, Ray and Ebert, David S.},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3356471.3365243},
	isbn = {9781450369572},
	keywords = {Geolocation prediction, deep learning, twitter},
	location = {<conf-loc>, <city>Chicago</city>, <state>IL</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {85–88},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '19},
	title = {City-level Geolocation of Tweets for Real-time Visual Analytics},
	url = {https://doi.org/10.1145/3356471.3365243},
	year = {2019},
}

@inproceedings{10.1145/3356473.3365189,
	abstract = {We consider a community finding problem called Co-located Community Detection (CCD) over geo-social networks, which retrieves communities that satisfy both high structural tightness and spatial closeness constraints. To provide a solution that benefits from existing studies on community detection, we decouple the spatial constraint from graph structural constraint and propose a uniform CCD framework which gives users the freedom to choose customized measurements for social cohesiveness (e.g., k-core or k-truss). For the spatial closeness constraint, we apply the bounded radius spatial constraint and develop an exact algorithm together with effective pruning rules. To further improve the efficiency and make our framework scale to a very large scale of data, we propose a near-linear time approximation algorithm with a constant approximation ratio (√2). We conduct extensive experiments on both synthetic and real-world datasets to demonstrate the efficiency and effectiveness of our algorithms.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Zheng, Xiuwen and Liu, Qiyu and Gupta, Amarnath},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Local Events and News},
	doi = {10.1145/3356473.3365189},
	isbn = {9781450369589},
	keywords = {Big Spatial Data, Co-located Community Detection, Computational Geometry, Geo-Social Network},
	location = {Chicago, IL, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {LENS'19},
	title = {Scalable Community Detection over Geo-Social Network},
	url = {https://doi.org/10.1145/3356473.3365189},
	year = {2019},
}

@inproceedings{10.1145/3356473.3365191,
	abstract = {Location-Based Social Networks (LBSNs) are an emerging kind of social network in which users can share their position with others and talk about visited places, providing comments and recommendations. Some LBSNs encourage the voluntary submission of place reviews by offering to users some sort of reward for this activity. However, soon or later this possibility has lead to fraudulent behaviours, in which attackers try to perform fake check-ins in order to increase the obtained rewards without actually visiting any place. Several different solutions have been proposed for distinguishing between real and fake check-ins with a certain degree of confidence. In this paper, we propose an alternative solution based on the use of the emerging blockchain technology, where a decentralized service can provide reliable presence claims for users.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Migliorini, Sara and Gambini, Mauro and Belussi, Alberto},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Analytics for Local Events and News},
	doi = {10.1145/3356473.3365191},
	isbn = {9781450369589},
	keywords = {Blockchain, Fake Check-in, Location-Based Social Network, Proof of Location, Smart Contracts},
	location = {Chicago, IL, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {LENS'19},
	title = {A Blockchain-based Solution to Fake Check-ins in Location-Based Social Networks},
	url = {https://doi.org/10.1145/3356473.3365191},
	year = {2019},
}

@inproceedings{10.1145/3356991.3365469,
	abstract = {Better knowledge of where people live is of great importance for a wide range of studies, including disaster responses, public health, resource management, and urban planning. Given the increasing demand for population grid with improved quality, this study explores the feasibility of generating high-resolution (100m) population grids in the Conterminous U.S. (CONUS) using a total of 125 million building footprints recently released by Microsoft. Those building footprints were used to disaggregate census tract population of the latest ACS 5-year estimates (2013-2017). Land use dataset from the OpenStreetMap (OSM) was applied to trim raw buildings footprints by removing those that are not likely residential. Weighting scenarios were designed, with which a dasymetric model was applied to disaggregate the ACS census tract estimates into a 100m population grid product. The results suggest that building footprints as a weighting layer, particularly footprint size after trimming, outperforms other commonly used weighting layers and is able to capture the great heterogeneity of population distribution at the micro-level. This study provides valuable experience in developing high-resolution population grid products that can benefit a wide range of studies in need of spatially explicit population data.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Huang, Xiao and Wang, Cuizhen and Li, Zhenlong},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3356991.3365469},
	isbn = {9781450369602},
	keywords = {dasymetric mapping, microsoft building footprints, population disaggregation, population grid},
	location = {Chicago, Illinois},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '19},
	title = {High-resolution population grid in the CONUS using microsoft building footprints: a feasibility study},
	url = {https://doi.org/10.1145/3356991.3365469},
	year = {2019},
}

@inproceedings{10.1145/3356991.3365470,
	abstract = {Geoparsing is an important task in geographic information retrieval. A geoparsing system, known as a geoparser, takes some texts as the input and outputs the recognized place mentions and their location coordinates. In June 2019, a geoparsing competition, Toponym Resolution in Scientific Papers, was held as one of the SemEval 2019 tasks. The winning teams developed neural network based geoparsers that achieved outstanding performances (over 90\% precision, recall, and F1 score for toponym recognition). This exciting result brings the question "are we there yet?", namely have we achieved high enough performances to possibly consider the problem of geoparsing as solved? One limitation of this competition is that the developed geoparsers were tested on only one dataset which has 45 research articles collected from the particular domain of Bio-medicine. It is known that the same geoparser can have very different performances on different datasets. Thus, this work performs a systematic evaluation of these state-of-the-art geoparsers using our recently developed benchmarking platform EUPEG that has eight annotated datasets, nine baseline geoparsers, and eight performance metrics. The evaluation result suggests that these new geoparsers indeed improve the performances of geoparsing on multiple datasets although some challenges remain.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Wang, Jimin and Hu, Yingjie},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3356991.3365470},
	isbn = {9781450369602},
	keywords = {EUPEG, GeoAI, benchmarking platform, contextual word embedding, deep learning, geoparsing, long short-term memory},
	location = {Chicago, Illinois},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '19},
	title = {Are we there yet? evaluating state-of-the-art neural network based geoparsers using EUPEG as a benchmarking platform},
	url = {https://doi.org/10.1145/3356991.3365470},
	year = {2019},
}

@inproceedings{10.1145/3356991.3365471,
	abstract = {In this paper we use network analysis to identify qualitative "neighbors" for toponyms in an eighteenth-century French encyclopedia, but could apply to any entry-based text with annotated toponyms. This method draws on relations in a corpus of articles, which improves disambiguation at a later stage with an external resource. We suggest the network as an alternative to geospatial representation, a useful proxy when no historical gazetteer exists for the source material's period. Our first experiments have shown that this approach goes beyond a simple text analysis and is able to find relations between toponyms that are not co-occurring in the same documents. Network relations are also usefully compared with disambiguated toponyms to evaluate geographical coverage, and the ways that geographical discourse is expressed, in historical texts.},
	address = {New York, NY, USA},
	articleno = {3},
	author = {Moncla, Ludovic and McDonough, Katherine and Vigier, Denis and Joliveau, Thierry and Brenon, Alice},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3356991.3365471},
	isbn = {9781450369602},
	keywords = {digital humanities, geographic information retrieval, toponym disambiguation},
	location = {Chicago, Illinois},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '19},
	title = {Toponym disambiguation in historical documents using network analysis of qualitative relationships},
	url = {https://doi.org/10.1145/3356991.3365471},
	year = {2019},
}

@inproceedings{10.1145/3356994.3365499,
	abstract = {A sketch map is a generalized simple map using abstractive and graphical symbols to provide the information for a specific purpose, e.g., tourist information or a guidance for commercial facilities, and it becomes popular and be widely used due to its good readability. A sketch map consists of the only necessary (or requested) facilities, such as restaurants or parking lots, and their adjacent necessary roads. This implies that to generate a sketch map, we have to select the necessary roads and remove the others, which is called a road generalization. However, it is a difficult problem to automatically select all the necessary roads when the facilities are given.In this paper, we propose a road generalization method based on the strokes [18, 19] to automatically select all the necessary roads in order to support a generation of a sketch map when the facilities are given. The proposed method has several features as follows: (1) classifies the stroke networks into two layers: global stroke networks and local stroke networks to improve the perceptibility (i.e., readability) of a sketch map, (2) guarantees the reachability to the facilities, and (3) avoids a generation of a redundant detour path to the facilities.We implement the prototype system based on the proposed method and compare with the previous method. From the experiment evaluations, we show that the proposed method decreases the average distance to the given facilities by about 60\%. Moreover, by using local stroke networks, the reachability to the facilities is improved from about 0.888 to over 0.979 (up to 1.0).},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Kim, Yonghwan and Fukuyasu, Hiroaki and Yamamoto, Daisuke and Takahashi, Naohisa},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3356994.3365499},
	isbn = {9781450369633},
	keywords = {generalized maps, road generalization, sketch maps, stroke networks},
	location = {Chicago, Illinois},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '19},
	title = {A road generalization method using layered stroke networks},
	url = {https://doi.org/10.1145/3356994.3365499},
	year = {2019},
}

@inproceedings{10.1145/3356994.3365502,
	abstract = {Trip planning services are employed extensively by users to compute paths between locations and navigate within a road network. In some real-world scenarios such as planning for a hiking trip or running training, users usually require personalized trip planning. Although some existing systems can recommend trips that other users have posted, along with a set of ratings w.r.t. the difficulty of the route, conditions, or the enjoyment it provides. Very often though users want to define a custom trip that fits their personal needs, for which existing systems are unable to provide any rating. In this paper we therefore define the problem of inferring ratings for custom trips. We also outline a solution to infer ratings by utilizing the ratings of trips previously posted by users and their similarity with a given custom trip. Finally, we present the results of preliminary experiments were we evaluate the efficiency of our proposed approach on inferring ratings for trips related to hiking and other similar activities.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Chondrogiannis, Theodoros and Ge, Mouzhi},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3356994.3365502},
	isbn = {9781450369633},
	keywords = {mapping services, route planning, route recommendation},
	location = {Chicago, Illinois},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '19},
	title = {Inferring ratings for custom trips from rich GPS traces},
	url = {https://doi.org/10.1145/3356994.3365502},
	year = {2019},
}

@inproceedings{10.1145/3356995.3364536,
	abstract = {Real-world GPS trajectory datasets are essential for geographical applications such as map inference, map matching, traffic detection, etc. Currently only a handful of GPS trajectory datasets are publicly available and the quality of these datasets varies. Most of the existing datasets have limited geographical coverage (a focus on China or the USA), have low sampling rates and less contextual information of the GPS pings. This paper presents Grab-Posisi, the first GPS trajectory dataset of Southeast Asia from both developed countries (Singapore) and developing countries (Jakarta, Indonesia). The data were collected very recently in April 2019 with a 1 second sampling rate, which is the highest amongst all the existing open source datasets. It also has richer contextual information, including the accuracy level, bearing, speed and labels trajectories by data acquisition source (Android or iOS phones) and driving mode (Car or Motorcycle). The dataset contains more than 88 million pings and covers more than 1 million kms. Experiments on the dataset demonstrate new challenges for various geographical applications. The dataset is of great value and a significant resource for the community to benchmark and revisit existing algorithms.},
	address = {New York, NY, USA},
	author = {Huang, Xiaocheng and Yin, Yifang and Lim, Simon and Wang, Guanfeng and Hu, Bo and Varadarajan, Jagannadan and Zheng, Shaolin and Bulusu, Ajay and Zimmermann, Roger},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Prediction of Human Mobility},
	doi = {10.1145/3356995.3364536},
	isbn = {9781450369640},
	keywords = {Datasets, GPS, Trajectory},
	location = {Chicago, IL, USA},
	numpages = {10},
	pages = {1–10},
	publisher = {Association for Computing Machinery},
	series = {PredictGIS'19},
	title = {Grab-Posisi: An Extensive Real-Life GPS Trajectory Dataset in Southeast Asia},
	url = {https://doi.org/10.1145/3356995.3364536},
	year = {2019},
}

@inproceedings{10.1145/3356995.3364540,
	abstract = {The rise of technology and the internet provides powerful means for people from all around the world to communicate and connect with one another. Online social network platforms become go-to places for users to express and share their individuality, which includes choice of activities, locations and associated timestamps. In turn, their opinions affect the point of view of others, who are in their online friendship circle. Users' increasing usage of social networks help accumulate massive amount of data that can be further explored. Particularly, this type of data attracts and allows researchers, who are interested in studying and understanding how social factors and previous experience influence user behavior in term of activity-related travel choice. In this paper, the goal is to utilize such rich data sources to build a model that predicts user next activity. Such model contributes a powerful tool for integrating the location prediction with transportation planning and operations process. Besides, it is valuable in commercial applications to create better recommendation system with higher accuracy and ultimately attract more customers to partnering businesses. By studying the dataset, which contains millions of historical check-ins from thousands of users, it is possible to derive information that are useful in predicting user next activity. The proposed approach applies machine learning techniques on the collected features to deliver highly accurate prediction results with fast training and prediction time.},
	address = {New York, NY, USA},
	author = {To, Diem and Si, Dong and Chen, Ying},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Prediction of Human Mobility},
	doi = {10.1145/3356995.3364540},
	isbn = {9781450369640},
	keywords = {graph analysis, location prediction, machine learning, social network},
	location = {Chicago, IL, USA},
	numpages = {9},
	pages = {15–23},
	publisher = {Association for Computing Machinery},
	series = {PredictGIS'19},
	title = {Traveler's Next Activity Predication with Location-Based Social Network Data},
	url = {https://doi.org/10.1145/3356995.3364540},
	year = {2019},
}

@inproceedings{10.1145/3356998.3365777,
	abstract = {GPS position are useful to analyse movements of mobile objects. Unfortunately, the outcome can be unsatisfactory due to imprecision and signal lost. Several sensors (generic or specific ones depending on the type of vehicle) are now included into mobile objects. This article describes a new generic model that enhances the semantic trajectory model and a process to extract semantic from GPS and other sensors. This process is based on the raw data from these sensors which help identify how, why and when mobile objects are moving, in order to add semantic information to the trajectories and then design new applications such as smart GPS, reporting systems or Remote Maintenance System. This process was successfully applied to the Indre et Loire fire department and its connected ambulances.},
	address = {New York, NY, USA},
	articleno = {10},
	author = {Bisone, Frederick and Devogele, Thomas and Etienne, Laurent},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on the Use of GIS in Emergency Management},
	doi = {10.1145/3356998.3365777},
	isbn = {9781450369657},
	keywords = {trajectory data mining, semantic trajectory, semantic extraction process, emergency vehicles, GPS and sensors},
	location = {Chicago, Illinois},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {EM-GIS '19},
	title = {From raw sensor data to semantic trajectories},
	url = {https://doi.org/10.1145/3356998.3365777},
	year = {2020},
}

@inproceedings{10.1145/3356999.3365466,
	abstract = {The increasing amount of available spatial data leads to the development and spread of big data systems specifically tailored for the management and process of such kind of information. These systems usually apply a MapReduce paradigm which essentially computes the same operation on different chunks of independent data in parallel. Even if this solution fits well in most cases where the extension and complexity of each single spatial object is small w.r.t. the extension and complexity of the overall dataset, some problems arise when a dataset is composed of only few objects, each one with a great extension and complexity in terms of number of vertices. This problem is exacerbated during the computation of a spatial join or in general of topological relations. As already discussed in literature, a viable solution for this problem consists in subdividing the big and complex geometries into smaller and simpler ones before applying the MapReduce operations. This paper takes a step forward in this direction by examining how the topological relations computed on the parts can be efficiently recombined to obtain the topological relation between the two original objects.},
	address = {New York, NY, USA},
	articleno = {3},
	author = {Belussi, Alberto and Carra, Damiano and Migliorini, Sara and Negri, Mauro},
	booktitle = {Proceedings of the 8th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3356999.3365466},
	isbn = {9781450369664},
	keywords = {MapReduce, spatial big data, spatialhadoop, topological relations},
	location = {Chicago, Illinois},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '19},
	title = {Efficient MapReduce computation of topological relations for big geometries},
	url = {https://doi.org/10.1145/3356999.3365466},
	year = {2019},
}

@inproceedings{10.1145/3357000.3366140,
	abstract = {Road maps are important in our personal lives and are widely used in many different applications. Therefore, an up-to-date road map is essential. The huge amount of GPS data collected from moving objects provides an opportunity to generate an up-to-date road map. In this paper, we propose a novel method to generate road maps using GPS trajectories that is accurate with good coverage area, has a minimum number of vertices and edges, and several details of the road network. Our algorithm starts by identifying the locations of intersections using a line simplification algorithm with spatial-constraints and grid-based method. Then, it creates graph connectivity information to connect intersections and build road segments. In addition, our algorithm extracts road features such as turn restrictions, average speed, road length, road type, and the number of cars traveling in a specific portion of the road. To demonstrate the accuracy of our proposed algorithm, we conduct experiments using two real data sets and compare our results with two baseline methods. The comparisons indicate that our algorithm is able to achieve higher F-score in terms of accuracy and generates a detailed road map that is not overly complex.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Alsahfi, Tariq and Almotairi, Mousa and Elmasri, Ramez and Alshemaimri, Bader},
	booktitle = {Proceedings of the 12th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3357000.3366140},
	isbn = {9781450369671},
	keywords = {GPS trajectories, Map Generation, Road map features, Road segments},
	location = {Chicago, IL, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'19},
	title = {Road Map Generation and Feature Extraction from GPS Trajectories Data},
	url = {https://doi.org/10.1145/3357000.3366140},
	year = {2019},
}

@inproceedings{10.1145/3357000.3366142,
	abstract = {Traffic signs are basic and important elements in maps. They are related to traffic regulations, profoundly affecting/managing the travel mode of human beings and efficiency of vehicle running. Traffic sign mining technology is applied in many research fields such as traditional map update, high-precision map establishment and automatic driving. Image based traffic sign identification technology has the advantages of low cost and high efficiency over manual processing mode, and traffic sign detection has thus become a significant task with the pacing advancement of autonomous driving. However, many common object detection methods cannot be directly applied to this task, as the size of traffic signs are very small yet they vary considerably. Due to such characteristics, features of traffic signs are difficult to capture, and are harder to discriminate between classes. To address this problem, we proposed a selective feature fusion based Faster R-CNN with Arc-Softmax loss, which optimizes the detection performance from the two following ways: network structure and loss function. We discover that each Faster R-CNN layer is only capable of detecting targets within a certain size range. By carefully selecting and combining different layers' feature maps, we can extract features that effectively represent traffic signs of various sizes. Then, Arc-Softmax loss penalizes the angular distances between the feature vectors of different signs, and their corresponding weight vectors of the last fully connected layers, thereby encouraging intra-class compactness and inter-class separability between learned features. Extensive analysis and experiments on the challenging Tsinghua-Tencent 100K benchmark demonstrate the superiority and implementation simplicity of our proposed method. Code will be made publicly available.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Li, Site and Gu, Yang and Song, Zhichao and Xing, Tengfei and Meng, Yiping and Xu, Pengfei and Hu, Runbo and Zhang, Tiancheng and Yu, Ge and Chai, Hua},
	booktitle = {Proceedings of the 12th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3357000.3366142},
	isbn = {9781450369671},
	keywords = {map updates, object detection, traffic sign},
	location = {Chicago, IL, USA},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'19},
	title = {Small Traffic Sign Detection Through Selective Feature Fusion Based Faster R-CNN With Arc-Softmax Loss},
	url = {https://doi.org/10.1145/3357000.3366142},
	year = {2019},
}

@inproceedings{10.1145/3357000.3366143,
	abstract = {Given a sequence of possibly sparse and noisy GPS traces and a map of the road network, map matching algorithms can infer the most accurate trajectory on the road network. However, if the road network is wrong (for example due to missing or incorrectly mapped roads, missing parking lots, misdirected turn restrictions or misdirected one-way streets) standard map matching algorithms fail to reconstruct the correct trajectory.In this paper, an algorithm to tracking vehicles able to move both on and off the known road network is formulated. It efficiently unifies existing hidden Markov model (HMM) approaches for map matching and standard free-space tracking methods (e.g. Kalman smoothing) in a principled way. The algorithm is a form of interacting multiple model (IMM) filter subject to an additional assumption on the type of model interaction permitted, termed here as semi-interacting multiple model (sIMM) filter. A forward filter (suitable for real-time tracking) and backward MAP sampling step (suitable for MAP trajectory inference and map matching) are described. The framework set out here is agnostic to the specific tracking models used, and makes clear how to replace these components with others of a similar type. In addition to avoiding generating misleading map matching trajectories, this algorithm can be applied to learn map features by detecting unmapped or incorrectly mapped roads and parking lots, incorrectly mapped turn restrictions and road directions.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Murphy, James and Pao, Yuanyuan and Yuen, Albert},
	booktitle = {Proceedings of the 12th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3357000.3366143},
	isbn = {9781450369671},
	keywords = {Bayesian filtering, Map matching, map learning, object tracking, road networks},
	location = {Chicago, IL, USA},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS'19},
	title = {Map matching when the map is wrong: Efficient on/off road vehicle tracking and map learning},
	url = {https://doi.org/10.1145/3357000.3366143},
	year = {2019},
}

@inproceedings{10.1145/3357384.3357907,
	abstract = {Fastest Route Recommendation (FRR) aims to find the fastest path in response to user's queries in a large complex road network. Early studies cast the FRR task as a pathfinding problem on graphs and adopt heuristic algorithms as the major solution due to the efficiency and robustness. A major problem of heuristic algorithms is that the heuristic function is usually empirically set with simple methods, which is difficult to model other useful factors. In this paper, we extend the classic A* algorithm for the FRR task by modeling complex traffic information with neural networks. Specially, we identify an important factor that is important to improve the FRR task, i.e. the estimation of travel time. For this purpose, we first develop a module for predicting the time-varying traffic speed for a road segment, which is the foundation for estimating the travel time. Conditioned on this module, we further design another module to estimate the fastest travel time between two locations connected by routes. We adopt neural networks to implement both modules for enabling the capacity of modeling complex traffic characteristics and dynamics. In this way, the original two cost functions of A* algorithm have been set in a more principled way with neural networks. To our knowledge, we are the first to use neural networks for improving A* algorithm in the FRR task. It elegantly combines the merits of A* algorithm and the powerful modeling capacities of neural networks for the FRR task. Extensive results on the three real-world datasets have shown the effectiveness and robustness of the proposed model.},
	address = {New York, NY, USA},
	author = {Wu, Ning and Wang, Jingyuan and Zhao, Wayne Xin and Jin, Yang},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	doi = {10.1145/3357384.3357907},
	isbn = {9781450369763},
	keywords = {traffic speed prediction, heuristic search, fastest route planning},
	location = {Beijing, China},
	numpages = {10},
	pages = {1923–1932},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {Learning to Effectively Estimate the Travel Time for Fastest Route Recommendation},
	url = {https://doi.org/10.1145/3357384.3357907},
	year = {2019},
}

@inproceedings{10.1145/3357384.3357926,
	abstract = {Most social media messages are written in languages other than English, but commonly used text mining tools were designed only for English. This paper introduces the Unicode Convolutional Neural Network (UnicodeCNN) for analyzing text written in any language. The UnicodeCNN does not require the language to be known in advance, allows the language to change arbitrarily mid-sentence, and is robust to the misspellings and grammatical mistakes commonly found in social media. We demonstrate the UnicodeCNN's effectiveness on the challenging task of content-based tweet geolocation using a dataset with 900 million tweets written in more than 100 languages. Whereas previous work restricted itself to predicting a tweet's country or city of origin (and only worked on tweets written in certain languages from highly populated cities), we predict the exact GPS locations of tweets (and our method works on tweets written in any language sent from anywhere in the world). We predict GPS coordinates using the mixture of von Mises-Fisher (MvMF) distribution. The MvMF exploits the Earth's spherical geometry to improve predictions, a task that previous work considered too computationally difficult. On English tweets, our model's predictions average more than 300km closer to the true location than previous work, and in other languages our model's predictions are up to 1500km more accurate. Remarkably, the UnicodeCNN can learn geographic knowledge in one language and automatically transfer that knowledge to other languages.},
	address = {New York, NY, USA},
	author = {Izbicki, Mike and Papalexakis, Vagelis and Tsotras, Vassilis},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	doi = {10.1145/3357384.3357926},
	isbn = {9781450369763},
	keywords = {convolutional neural networks, geotagging, multilingual, twitter},
	location = {Beijing, China},
	numpages = {10},
	pages = {89–98},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {Geolocating Tweets in any Language at any Location},
	url = {https://doi.org/10.1145/3357384.3357926},
	year = {2019},
}

@inproceedings{10.1145/3357384.3358008,
	abstract = {A large number of spatial knowledge graphs (SKGs) are available from spatially enriched knowledge bases, e.g., DBpedia and YAGO2. This provides a great chance to understand valuable information about the regions surrounding us. However, it is hard to comprehend SKGs due to the explosively growing volume and the complication of the graph structures. Thus we study the problem of similar region search (SRS), which is an easy-to-use but effective way to explore spatial data. The effectiveness of SRS highly depends on how to measure the region similarity. However, existing approaches cannot make use of the rich information contained in SKGs thus may lead to incorrect results. In this paper, we propose a spatial knowledge representation learning method for region similarity, namely SKRL4RS. SKRL4RS firstly encodes the spatial entities of an SKG into a vector space to make it easier to extract useful features. Then regions are represented by 3-D tensors using the spatial entity embeddings together with geographical information. Finally, region tensors are fed into the conventional triplet network to learn the feature vectors of regions. The region similarity measure learned by SKRL4RS can capture the hierarchical types, semantic relatedness, and relative locations of spatial entities inside a region. Experimental results on two real-world datasets show that our SKRL4RS outperforms the state-of-the-art by a significant margin in terms of the accuracy of measuring region similarity.},
	address = {New York, NY, USA},
	author = {Jin, Xiongnan and Oh, Byungkook and Lee, Sanghak and Lee, Dongho and Lee, Kyong-Ho and Chen, Liang},
	booktitle = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},
	doi = {10.1145/3357384.3358008},
	isbn = {9781450369763},
	keywords = {deep learning, entity embedding, region similarity, spatial knowledge graph},
	location = {Beijing, China},
	numpages = {10},
	pages = {669–678},
	publisher = {Association for Computing Machinery},
	series = {CIKM '19},
	title = {Learning Region Similarity over Spatial Knowledge Graphs with Hierarchical Types and Semantic Relations},
	url = {https://doi.org/10.1145/3357384.3358008},
	year = {2019},
}

@inproceedings{10.1145/3357419.3357433,
	abstract = {Samut Songkhram is one province in Thailand which has coconut cultivation for domestic consumption and export abroad. However in the dry season, will be faced with the problem of using water due to the problem with sea water condition that raised, which the initial solution is to bring fresh water from the upper dams to push saltwater out of the area. Therefore, the data used of fresh water in the area, is important to water management in the upcoming crisis. Evalution of water requirement can assist the location of water demand in dry areas. Satellite data and ground data combined to estimate the volume of water needed. This research the assessment of water requirement for coconut cultivation is mainly aimed to determine the area of coconut growing through remote sensing technology with LANDSAT8 and assess the water demand in the coconut growing area. The result found, the area of coconut cultivation as 74876 Rai, where was analyzed by satellite data of Landsat8 with Supervised Classification method. The accuracy from interpreted data is 92 percent. The determination of water requirements for planting coconut were analyzed from the equation of Penman Monteith, average equal to 458,473,908 cubic meters. The value of Reference Crop Evapotranspiration average ETo is 3.80 mm / day and the coefficient of water use (Crop Coefficient) average Kc is 1.35. This research found the amount of coconut water needs in the early stages of growth with relatively little water demand and gradually increased, and there is no change in water usage.},
	address = {New York, NY, USA},
	author = {Phonphan, Walaiporn and Plangcharienpon, Supeerat and Thanakunwutthirot, Manatsanan},
	booktitle = {Proceedings of the 9th International Conference on Information Communication and Management},
	doi = {10.1145/3357419.3357433},
	isbn = {9781450371889},
	keywords = {Remote Sensing, Geographic Information Systems, Evaluation of water requirement},
	location = {Prague, Czech Republic},
	numpages = {4},
	pages = {48–51},
	publisher = {Association for Computing Machinery},
	series = {ICICM 2019},
	title = {Evaluation of water requirement for coconut growing with remote sensing technology},
	url = {https://doi.org/10.1145/3357419.3357433},
	year = {2019},
}

@inproceedings{10.1145/3358528.3358566,
	abstract = {High-resolution earth observation images have increased dramatically because of the increasing of remote sensing satellites. Researchers must do large-scale target annotations to meet the training needs of deep neural network based model. However, most existing datasets contain an insufficient number of annotated samples, due to the inefficient manual annotation process which reason lies in the large number of remote sensing images, huge size, numerous targets, and high accuracy requirements. This paper proposed an efficient annotation method for big data sets of high-resolution earth observation images, in which the annotation process is divided into two parallel sub-processes, fast panchromatic image labeling and multi-spectral image fusion. Automatic scale transform is utilized for annotation of fused imagery. Experimental results show that the proposed method could improve the accuracy and efficiency of target labeling. Mask-RCNN and Faster-RCNN based target detection results demonstrate the validity of the big dataset annotated via our method.},
	address = {New York, NY, USA},
	author = {Lu, Zeshan and Liu, Kun and Liu, Zhen and Wang, Cong and Shen, Maoxin and Xu, Tao},
	booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
	doi = {10.1145/3358528.3358566},
	isbn = {9781450371926},
	keywords = {Annotation method, Mask-RCNN, high-resolution images, target detection},
	location = {Jinan, China},
	numpages = {4},
	pages = {240–243},
	publisher = {Association for Computing Machinery},
	series = {ICBDT '19},
	title = {An Efficient Annotation Method for Big Data Sets of High-Resolution Earth Observation Images},
	url = {https://doi.org/10.1145/3358528.3358566},
	year = {2019},
}

@inproceedings{10.1145/3358528.3358582,
	abstract = {The Convolutional Neural Network Model (CNN) has shown excellent performance in many tasks in recent years, such as the application in image recognition and classification. In this paper we propose a method based on the deep convolutional neural network model VGG and radar reflectivity factor to quantitatively estimation the rainfall rate intensity, which improves the shortcomings of the traditional method Z-R relationship with high error. We selected the shallow convolutional neural network LeNet for comparison. Then, we studied the effect of selecting different sizes of radar reflectivity factor images on the quantitative estimation of rainfall rate intensity. The results show that the depth convolutional neural network VGG and the relatively large size of radar reflectivity factor image are better than the traditional Z-R relationship and shallow convolutional neural network LeNet.},
	address = {New York, NY, USA},
	author = {Yang, Haochen and Wang, Tieqiao and Zhou, Xuesong and Dong, Jiwen and Gao, Xizhan and Niu, Sijie},
	booktitle = {Proceedings of the 2nd International Conference on Big Data Technologies},
	doi = {10.1145/3358528.3358582},
	isbn = {9781450371926},
	keywords = {Z-R relationship, Rainfall rate intensity, Radar reflectivity factor, Convolutional neural network},
	location = {Jinan, China},
	numpages = {4},
	pages = {244–247},
	publisher = {Association for Computing Machinery},
	series = {ICBDT '19},
	title = {Quantitative Estimation of Rainfall Rate Intensity Based on Deep Convolutional Neural Network and Radar Reflectivity Factor},
	url = {https://doi.org/10.1145/3358528.3358582},
	year = {2019},
}

@article{10.1145/3362069,
	abstract = {Each year, an average of around 6 million car accidents occur in the United States. Road safety features (e.g., concrete barriers, metal crash barriers, rumble strips) play an important role in preventing or mitigating vehicle crashes. Accurate maps of road safety features is an important component of safety management systems for federal or state transportation agencies, helping traffic engineers identify locations to invest in safety infrastructure. In current practice, mapping road safety features is largely done manually (e.g., observations on the road or visual interpretation of streetview imagery), which is both expensive and time consuming. In this article, we propose a deep learning approach to automatically map road safety features from streetview imagery. Unlike existing convolutional neural networks that classify each image individually, we propose to further add a recurrent neural network (long short-term memory) to capture geographic context of images (spatial autocorrelation effect along linear road network paths). Evaluations on real-world streetview imagery show that our proposed model outperforms several baseline methods.},
	address = {New York, NY, USA},
	articleno = {15},
	author = {Sainju, Arpan Man and Jiang, Zhe},
	doi = {10.1145/3362069},
	issn = {2691-1922},
	issue_date = {August 2020},
	journal = {ACM/IMS Trans. Data Sci.},
	keywords = {Spatial classification, deep learning, road network classification, spatial dependency},
	month = {sep},
	number = {3},
	numpages = {20},
	publisher = {Association for Computing Machinery},
	title = {Mapping Road Safety Features from Streetview Imagery: A Deep Learning Approach},
	url = {https://doi.org/10.1145/3362069},
	volume = {1},
	year = {2020},
}

@inproceedings{10.1145/3362752.3362754,
	abstract = {In recent years, global warming is increasing due to the influence of greenhouse gas such as carbon dioxide. Therefore, in the automotive field, emission control has been conducted. Then, automakers are focusing on the EV development. This development still has some concern problems such as a short-distance driving range, long-period charging time, and small amount of charging infrastructure compared with the internal combustion engine vehicle. In addition, EV offers a special feature such as regenerative braking systems. Accordingly, our research team has used the Geographic Information System, road networks, elevation models, and EV driving data to derive the equations of estimated power consumption for each vehicle type individually. The driving support system that manages electric power of EV using this formula is constructed. This research standardized the derivation method of estimated power consumption formula based on previous research achievement. Furthermore, this research constructed a mechanism to collect driving data of the user, reviewing the estimated power consumption formula after the EV was sold. In the accuracy verification of the estimated power consumption, the estimated value calculated by the system was compared with the measured value and the WLTC value. As a result, it was confirmed that to reduce the error.},
	address = {New York, NY, USA},
	author = {Shiota, Atsushi and Nakayama, Shuntaro and Iwai, Hiroki and Tsuruhara, Keita and Kerdphol, Thongchart and Mitani, Yasunori},
	booktitle = {Proceedings of the 2019 2nd International Conference on Electronics and Electrical Engineering Technology},
	doi = {10.1145/3362752.3362754},
	isbn = {9781450372145},
	keywords = {Navigation System, Global Positioning System, Geographic Information System (GIS), Electric Vehicle, Digital Elevation Model},
	location = {Penang, Malaysia},
	numpages = {6},
	pages = {62–67},
	publisher = {Association for Computing Machinery},
	series = {EEET 2019},
	title = {Construction of Standardization Method of Estimated Power Consumption System by Driving of EV using Geographic Information System and User's Driving Data},
	url = {https://doi.org/10.1145/3362752.3362754},
	year = {2019},
}

@inproceedings{10.1145/3366030.3366094,
	abstract = {Researchers must crawl geo-social data to analyze and visualize geo-social data. A conventional method to exhaustively crawl geosocial data is based on a grid. The crawler divides a specified area into a grid and uses the center coordinates of each cell to query databases using APIs. However, there is a difficult problem when using the grid-based method. It is that researchers cannot estimate the optimized grid size to exhaustively crawl geo-social data in advance because the optimized grid size depends on data density owing to geographical characteristics of an area. We focus on the fact that geo-social data are dense along roads. Thus, we propose a method based on road maps to exhaustively crawl geo-social data. We demonstrated that our method can crawl geo-social data by using almost the same number of queries compared to the crawler with an optimized grid size.},
	address = {New York, NY, USA},
	author = {Ijima, Sou and Hirota, Masaharu and Yokoyama, Shohei},
	booktitle = {Proceedings of the 21st International Conference on Information Integration and Web-Based Applications \&amp; Services},
	doi = {10.1145/3366030.3366094},
	isbn = {9781450371797},
	keywords = {Road Maps, OpenStreetMap, Grid-based Method, Google Places API, Geo-Social Data, Data Crawling},
	location = {Munich, Germany},
	numpages = {5},
	pages = {250–254},
	publisher = {Association for Computing Machinery},
	series = {iiWAS2019},
	title = {A Crawling Method with No Parameters for Geo-social Data based on Road Maps},
	url = {https://doi.org/10.1145/3366030.3366094},
	year = {2020},
}

@inproceedings{10.1145/3366194.3366222,
	abstract = {In order to more fully make use of the feature information of remote sensing image and improve the detection accuracy of remote sensing image, after preprocessing, we propose a multi-featured remote sensing image change detection method. Firstly, two different methods based on spectral features and gradient features are used to calculate the change intensity image for different time-phase images. Then, the fusion algorithm is used to fuse the spectral feature method and the gradient feature respectively, and then the fusion results are merged. The difference fusion intensity map based on the two features is obtained. Finally, the image threshold is segmented by two levels using the Two-Level Clustering algorithm to obtain image change information. Compared with the traditional method, the method improves the comprehensiveness of the change information expression by fusing the multiple features of the optical image, and adopts the Two-Level Clustering to introduce the middle term, and the final threshold segmentation of the change intensity map can further become more realistic. It is verified by two sets of experimental data that the method can better integrate multi-feature information and express change information more comprehensively.},
	address = {New York, NY, USA},
	author = {Chen, ZhangHua and Leng, XiangGuang and Lei, Lin},
	booktitle = {Proceedings of the 2019 International Conference on Robotics, Intelligent Control and Artificial Intelligence},
	doi = {10.1145/3366194.3366222},
	isbn = {9781450372985},
	keywords = {Change Detection, Fusion, Multiple Features, Two-Level Clustering},
	location = {Shanghai, China},
	numpages = {7},
	pages = {159–165},
	publisher = {Association for Computing Machinery},
	series = {RICAI '19},
	title = {Multiple features fusion change detection method based on Two-Level Clustering},
	url = {https://doi.org/10.1145/3366194.3366222},
	year = {2019},
}

@inproceedings{10.1145/3366423.3380021,
	abstract = {The quality of a digital map is of utmost importance for geo-aware services. However, maintaining an accurate and up-to-date map is a highly challenging task that usually involves a substantial amount of manual work. To reduce the manual efforts, methods have been proposed to automatically derive road attributes by mining GPS traces. However, previous methods always modeled each road attribute separately based on intuitive hand-crafted features extracted from GPS traces. This observation motivates us to propose a machine learning based method to learn joint features not only from GPS traces but also from map data. To model the relations among the target road attributes, we extract low-level shared feature embeddings via multi-task learning, while still being able to generate task-specific fused representations by applying attention-based feature fusion. To model the relations between the target road attributes and other contextual information that is available from a digital map, we propose to leverage map tiles at road centers as visual features that capture the information of the surrounding geographic objects around the roads. We perform extensive experiments on the OpenStreetMap where state-of-the-art classification accuracy has been obtained compared to existing road attribute detection approaches.},
	address = {New York, NY, USA},
	author = {Yin, Yifang and Varadarajan, Jagannadan and Wang, Guanfeng and Wang, Xueou and Sahrawat, Dhruva and Zimmermann, Roger and Ng, See-Kiong},
	booktitle = {Proceedings of The Web Conference 2020},
	doi = {10.1145/3366423.3380021},
	isbn = {9781450370233},
	keywords = {multi-task learning, digital maps, Road attributes, GPS trajectories},
	location = {Taipei, Taiwan},
	numpages = {7},
	pages = {2662–2668},
	publisher = {Association for Computing Machinery},
	series = {WWW '20},
	title = {A Multi-task Learning Framework for Road Attribute Updating via Joint Analysis of Map Data and GPS Traces},
	url = {https://doi.org/10.1145/3366423.3380021},
	year = {2020},
}

@article{10.1145/3368268,
	abstract = {Consider a city’s road network and a worker who is traveling on a given path from a starting point s to a destination d (e.g., from school or work to home) in said network. Consider further that there is a set of tasks in the network available to be performed, where each such task takes a certain amount of time to be completed and yields a positive reward if completed, and, finally, that the worker is willing to deviate from his/her path as long as the travel time to the selected tasks plus the time taken for completing them does not exceed a given time budget. We call this problem the In-Route Task Selection (IRTS) problem and consider two variants thereof. In the first one, named IRTS-SP, we assume that the worker only specifies s and d and he/she wants to consider alternative paths that deviate (cost-wise) as little as possible from the cost of the shortest path connecting s and d. In the second variant, named IRTS-PP, we assume that the worker has a preferred path from s to d and wants to travel along that one path for as long as possible. The latter is practically relevant in cases where the worker has a path other than the shortest one that is more desirable for non-objective reasons, e.g., availability of public transit, bicycle-friendliness or perceived safety. Common to both variants though, we assume that the worker wants to maximize the rewards collected by completing tasks. Clearly, there are now two conflicting criteria for the worker to contemplate when considering which tasks to perform: minimizing path deviation and maximizing collected reward. In this context, we investigate both IRTS variants using the skyline paradigm in order to obtain the set of non-dominated solutions w.r.t. the tradeoffs between earned rewards and deviation from either the cost of the shortest path, in the case of IRTS-SP, or the actual preferred path, in the case of IRTS-PP. Returning the skyline set of solutions to workers is of practical interest as it empowers them, e.g., it allows them to decide, at query time, which tasks suit them better. We propose exact and heuristic approaches in order to solve both variants of the IRTS problem. Our experiments, using real city-scale datasets, show that while the exact approaches serve as benchmarks, they do not scale due to the NP-hardness of the problems. The overall best heuristic approach, on the other hand, can solve relatively large instances of the IRTS problems within practical query processing time, e.g., at par with less effective greedy heuristics, while still producing very good approximate skyline sets, e.g., often yielding less than 10\% relative error w.r.t. the exact solution.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Costa, Camila F. and Nascimento, Mario A.},
	doi = {10.1145/3368268},
	issn = {2374-0353},
	issue_date = {June 2020},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {In-route queries, road networks, skyline, spatial crowdsourcing},
	month = {dec},
	number = {2},
	numpages = {45},
	publisher = {Association for Computing Machinery},
	title = {In-Route Task Selection in Spatial Crowdsourcing},
	url = {https://doi.org/10.1145/3368268},
	volume = {6},
	year = {2019},
}

@inproceedings{10.1145/3377713.3377748,
	abstract = {After unconventional events occur, rapid emergency response is often needed in order to reduce losses. When there are more than one disaster points and more than one supply points, how to coordinate the local dispatch of emergency resources among these points is a problem worth studying. In this paper, we design a multi-objective, multi-point to multi-point and multi-resource emergency scheduling model that consider resources arrive time and transportation cost. Then a new emergency resource scheduling algorithm based on multi objective evolutionary algorithm based on decomposition (MOEA/D) is proposed, a new mutation operator was also designed to solve this problem. The experimental results show that this method can get a feasible scheduling scheme, which is faster than NSGA-II and has better scheduling results.},
	address = {New York, NY, USA},
	author = {Li, Yanrong and Li, Xiaoyong and Hou, Liyang and Kong, Wenping and Ma, Wenxue},
	booktitle = {Proceedings of the 2019 2nd International Conference on Algorithms, Computing and Artificial Intelligence},
	doi = {10.1145/3377713.3377748},
	isbn = {9781450372619},
	keywords = {multi-objective optimization, emergency resource scheduling, MOEA/D},
	location = {Sanya, China},
	numpages = {6},
	pages = {219–224},
	publisher = {Association for Computing Machinery},
	series = {ACAI '19},
	title = {A Multi-Objective Emergency Resource Scheduling Method Based on MOEA/D},
	url = {https://doi.org/10.1145/3377713.3377748},
	year = {2020},
}

@article{10.1145/3379562,
	abstract = {Given N geo-located point instances (e.g., crime or disease cases) in a spatial domain, we aim to detect sub-regions (i.e., hotspots) that have a higher probability density of generating such instances than the others. Hotspot detection has been widely used in a variety of important urban applications, including public safety, public health, urban planning, and equity, among others. The problem is challenging because its societal applications often have low tolerance for false positives and require significance testing that is computationally intensive. In related work, the spatial scan statistic introduced a likelihood ratio--based framework for hotspot evaluation and significance testing. However, it fails to consider the effect of spatial non-determinism, causing many missing detections. Our previous work introduced a non-deterministic normalization--based scan statistic to mitigate this issue. However, its robustness against false positives is not stably controlled. To address these limitations, we propose a unified framework that can improve the completeness of results without incurring more false positives. We also propose a reduction algorithm to improve the computational efficiency. Experiment results confirm that the unified framework can greatly improve the recall of hotspot detection without increasing the number of false positives, and the reduction algorithm can greatly reduce execution time.},
	address = {New York, NY, USA},
	articleno = {17},
	author = {Xie, Yiqun and Shekhar, Shashi},
	doi = {10.1145/3379562},
	issn = {2691-1922},
	issue_date = {August 2020},
	journal = {ACM/IMS Trans. Data Sci.},
	keywords = {Unified framework, hotspot, smart cities},
	month = {sep},
	number = {3},
	numpages = {29},
	publisher = {Association for Computing Machinery},
	title = {A Unified Framework for Robust and Efficient Hotspot Detection in Smart Cities},
	url = {https://doi.org/10.1145/3379562},
	volume = {1},
	year = {2020},
}

@article{10.1145/3380972,
	abstract = {The timely and accurate prediction of remote sensing data is of utmost importance especially in a situation where the predicted data is utilized to provide insights into emerging issues, like environmental nowcasting. Significant research progress can be found to date in devising variants of neural network (NN) models to fulfil this requirement by improving feature extraction and dynamic process representation power. Nevertheless, all these existing NN models are built upon rigid structures that often fail to maintain tradeoff between bias and variance, and consequently, need to spend a lot of time to empirically determine the most appropriate network configuration. This article proposes a self-adaptive recurrent deep incremental network model (SARDINE) which is a novel variant of the deep recurrent neural network with intrinsic capability of self-constructing the network structure in a dynamic and incremental fashion while learning from observed data samples. Moreover, the proposed SARDINE is able to model the spatial feature evolution while scanning the data in a single pass manner, and this further saves significant time when dealing with remote sensing imagery containing millions of pixels. Subsequently, we employ SARDINE in combination with a spatial influence mapping unit to accomplish the prediction. The effectiveness of the proposed model is evaluated in terms of predicting a time series of normalized difference vegetation index (NDVI) data derived from Landsat Thematic Mapper (TM)-5 and Moderate Resolution Imaging Spectroradiometer (MODIS) Terra satellite imagery. The experimental result demonstrates that the SARDINE-based prediction is able to achieve state-of-the-art accuracy with significantly reduced computational cost.},
	address = {New York, NY, USA},
	articleno = {16},
	author = {Das, Monidipa and Pratama, Mahardhika and Ghosh, Soumya K.},
	doi = {10.1145/3380972},
	issn = {2374-0353},
	issue_date = {September 2020},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {remote sensing, incremental model, evolving RNN, Spatio-temporal prediction},
	month = {apr},
	number = {3},
	numpages = {26},
	publisher = {Association for Computing Machinery},
	title = {SARDINE: A Self-Adaptive Recurrent Deep Incremental Network Model for Spatio-Temporal Prediction of Remote Sensing Data},
	url = {https://doi.org/10.1145/3380972},
	volume = {6},
	year = {2020},
}

@article{10.1145/3382080,
	abstract = {Spatial analysis and pattern recognition with vector spatial data is particularly useful to enrich raw data. In road networks, for instance, there are many patterns and structures that are implicit with only road line features, among which highway interchange appeared very complex to recognize with vector-based techniques. The goal is to find the roads that belong to an interchange, such as the slip roads and the highway roads connected to the slip roads. To go further than state-of-the-art vector-based techniques, this article proposes to use raster-based deep learning techniques to recognize highway interchanges. The contribution of this work is to study how to optimally convert vector data into small images suitable for state-of-the-art deep learning models. Image classification with a convolutional neural network (i.e., is there an interchange in this image or not?) and image segmentation with a u-net (i.e., find the pixels that cover the interchange) are experimented and give better results than existing vector-based techniques in this specific use case (99.5\% against 74\%).},
	address = {New York, NY, USA},
	articleno = {21},
	author = {Touya, Guillaume and Lokhat, Imran},
	doi = {10.1145/3382080},
	issn = {2374-0353},
	issue_date = {September 2020},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {map generalization, highway interchange, deep neural networks, Spatial data enrichment},
	month = {apr},
	number = {3},
	numpages = {21},
	publisher = {Association for Computing Machinery},
	title = {Deep Learning for Enrichment of Vector Spatial Databases: Application to Highway Interchange},
	url = {https://doi.org/10.1145/3382080},
	volume = {6},
	year = {2020},
}

@article{10.1145/3383653.3383655,
	abstract = {This study investigates human activity community in a city by conceptualizing it as a network embedding problem. In order to learn the latent representations of activity-travel patterns from individual daily trajectories, network embedding learns a vector space representation for each type of activity place as a node connected by movement links to preserve the structure of individual activities. The proposed approach is applied to mobile positioning data at the individual level obtained for a weekday from volunteers at Guangzhou City. Assessments are conducted to validate individual decision making for several types of activities by a field survey. This study contributes to a general framework for discovering individual activity-travel patterns from human movement trajectories.},
	address = {New York, NY, USA},
	author = {Huang, Tianyuan},
	doi = {10.1145/3383653.3383655},
	issue_date = {November 2019},
	journal = {SIGSPATIAL Special},
	keywords = {urban planning, network embedding, graph representation, geographic knowledge discovery},
	month = {feb},
	number = {3},
	numpages = {2},
	pages = {7–8},
	publisher = {Association for Computing Machinery},
	title = {SRC: discovering human activity community in a city},
	url = {https://doi.org/10.1145/3383653.3383655},
	volume = {11},
	year = {2020},
}

@inproceedings{10.1145/3383812.3383837,
	abstract = {Gene expression programming (GEP) algorithm, as an excellent artificial intelligence algorithm, is very suitable for the discovery of complex functional relationships. This paper proposes a recovery Remote-sensed Image based on the GEP algorithm. It uses a local reference image to recovery a large range of degraded satellite remote sensing images. Firstly, it uses the GEP algorithm to discover the mathematical function relationship between the reference image and the degraded image, and then uses the function relationship to recovery and reconstructs the degraded image. Thereby achieving the purpose of the using reference image is to improve and restore the large range degraded image. The experimental results show that the method can restore the degraded satellite image. In this paper, using the Formosat satellite as the degraded image, and using the drone image as the reference image to verify the paper algorithm. The experimental results show that the method can restore the degraded satellite image.},
	address = {New York, NY, USA},
	author = {Li, Shixiang and Wang, Yuli},
	booktitle = {Proceedings of the 2020 3rd International Conference on Image and Graphics Processing},
	doi = {10.1145/3383812.3383837},
	isbn = {9781450377201},
	keywords = {reference image, mathematical function relationship, image restoration, gene expression programming, degraded satellite image},
	location = {Singapore, Singapore},
	numpages = {4},
	pages = {172–175},
	publisher = {Association for Computing Machinery},
	series = {ICIGP '20},
	title = {Recovery Remote Sense Image Using the GEP Artificial Intelligence Algorithm},
	url = {https://doi.org/10.1145/3383812.3383837},
	year = {2020},
}

@inproceedings{10.1145/3384419.3430610,
	abstract = {As the COVID-19 outbreak evolves around the world, the World Health Organization (WHO) and its Member States have been heavily relying on staying at home and lock down measures to control the spread of the virus. In last months, various signs showed that the COVID-19 curve was flattening, but the premature lifting of some containment measures (e.g., school closures and telecommuting) are favouring a second wave of the disease. The accurate evaluation of possible countermeasures and their well-timed revocation are therefore crucial to avoid future waves or reduce their duration. In this paper, we analyze patient and route data collected by the Korea Centers for Disease Control \&amp; Prevention (KCDC). We extract information from real-world data sets and use them to parameterize simulations and evaluate different what-if scenarios.},
	address = {New York, NY, USA},
	author = {Yang, Lishan and Schmedding, Anna and Pinciroli, Riccardo and Smirni, Evgenia},
	booktitle = {Proceedings of the 18th Conference on Embedded Networked Sensor Systems},
	doi = {10.1145/3384419.3430610},
	isbn = {9781450375900},
	keywords = {simulation, geographic information system (GIS), data analysis, coronavirus, agent-based model (ABM), SARS-CoV-2, COVID-19},
	location = {Virtual Event, Japan},
	numpages = {2},
	pages = {782–783},
	publisher = {Association for Computing Machinery},
	series = {SenSys '20},
	title = {Simulating COVID-19 containment measures using the South Korean patient data: poster abstract},
	url = {https://doi.org/10.1145/3384419.3430610},
	year = {2020},
}

@article{10.1145/3385851,
	abstract = {We address the problem of performing efficient spatial and topological queries on large tetrahedral meshes with arbitrary topology and complex boundaries. Such meshes arise in several application domains, such as 3D Geographic Information Systems (GISs), scientific visualization, and finite element analysis. To this aim, we propose Tetrahedral trees, a family of spatial indexes based on a nested space subdivision (an octree or a kD-tree) and defined by several different subdivision criteria. We provide efficient algorithms for spatial and topological queries on Tetrahedral trees and compare to state-of-the-art approaches. Our results indicate that Tetrahedral trees are an improvement over R*-trees for querying tetrahedral meshes; they are more compact, faster in many queries, and stable at variations of construction thresholds. They also support spatial queries on more general domains than topological data structures, which explicitly encode adjacency information for efficient navigation but have difficulties with domains with a non-trivial geometric or topological shape.},
	address = {New York, NY, USA},
	articleno = {23},
	author = {Fellegara, Riccardo and Floriani, Leila De and Magillo, Paola and Weiss, Kenneth},
	doi = {10.1145/3385851},
	issn = {2374-0353},
	issue_date = {December 2020},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {topological queries, spatial queries, spatial indexes, octrees, kD-trees, Tetrahedral meshes},
	month = {jun},
	number = {4},
	numpages = {34},
	publisher = {Association for Computing Machinery},
	title = {Tetrahedral Trees: A Family of Hierarchical Spatial Indexes for Tetrahedral Meshes},
	url = {https://doi.org/10.1145/3385851},
	volume = {6},
	year = {2020},
}

@inproceedings{10.1145/3386392.3399279,
	abstract = {The richness of Cultural Heritage (CH) sites exposes tourists to an information overload which makes it difficult to efficiently select the items that they like and can practically visit within a tour.Faceted information exploration has been proposed as a solution to analyze large sets of data. However, most works focus on the inspection of a single type of information, e.g., hotels or music. In contrast, CH items are heterogeneous: they include natural and artificial monuments and different types of artworks which might be visited within a single tour. Moreover, CH sites are often visited in group, thus raising the expectation that all the involved people share information and decisions about what to do.In order to address this issue, we propose a map-based faceted exploration model that makes it possible to create custom, long-lasting maps representing a shared information space for user collaboration, and temporally project these maps on the basis of fine-grained filters which help users focus on items associated to short-term, specific interests. Our model supports the user in the organization and filtering of CH information on the basis of multiple perspectives related to the attributes of items. We propose graphical widgets to support interactive data visualization, faceted exploration, category-based information hiding and transparency of results at the same time. The widgets are based on the sunburst diagram, which compactly displays visualization criteria on data categories by showing facets and facet values in a circular structure.},
	address = {New York, NY, USA},
	author = {Mauro, Noemi and Izzi, Gianmarco and Pellegrino, Marco and Ardissono, Liliana and Grandi, Claudio and Lucenteforte, Maurizio and Segnan, Marino},
	booktitle = {Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
	doi = {10.1145/3386392.3399279},
	isbn = {9781450379502},
	keywords = {interactive user interfaces for ch applications, geographic information search, faceted information exploration, dynamic projection of geographic maps},
	location = {Genoa, Italy},
	numpages = {7},
	pages = {340–346},
	publisher = {Association for Computing Machinery},
	series = {UMAP '20 Adjunct},
	title = {Faceted Exploration of Cultural Heritage},
	url = {https://doi.org/10.1145/3386392.3399279},
	year = {2020},
}

@inproceedings{10.1145/3386392.3399280,
	abstract = {Cultural Heritage exploration is interesting for the development of inclusive tourist guides because it exposes visitors to different types of challenges, from steering content recommendation to visitors' interests and cognitive capabilities, to the suggestion of places that can be effectively reached and visited under different types of constraints: e.g., temporal and physical ones. In this work we are interested in the needs of people with Autism in order to support them in the exploration of a geographic area. Specifically, this paper presents a mobile tourist guide that we are developing to help people in visiting new places. The app is an evolution of PIUMA (Personalised Interactive Urban Maps for Autism), conceived to help autistic citizens in their everyday movements. It shows a map tailored to users with Autism Spectrum Disorder. In particular, it presents a personalized selection of safe Points of Interest, i.e., places that are, at the same time, interesting for the user and have "safe" characteristics from the sensory point of view, such as being quiet, scarcely crowded, or with smooth lights. In this paper, we present how we intend to extend PIUMA to support tourists.},
	address = {New York, NY, USA},
	author = {Cena, Federica and Mauro, Noemi and Ardissono, Liliana and Mattutino, Claudio and Rapp, Amon and Cocomazzi, Stefano and Brighenti, Stefania and Keller, Roberto},
	booktitle = {Adjunct Publication of the 28th ACM Conference on User Modeling, Adaptation and Personalization},
	doi = {10.1145/3386392.3399280},
	isbn = {9781450379502},
	keywords = {tourism, recommender systems, cultural heritage exploration, autism spectrum disorder, accessibility},
	location = {Genoa, Italy},
	numpages = {5},
	pages = {347–351},
	publisher = {Association for Computing Machinery},
	series = {UMAP '20 Adjunct},
	title = {Personalized Tourist Guide for People with Autism},
	url = {https://doi.org/10.1145/3386392.3399280},
	year = {2020},
}

@inproceedings{10.1145/3394486.3403246,
	abstract = {Many types of event sequence data exhibit triggering and clustering properties in space and time. Point processes are widely used in modeling such event data with applications such as predictive policing and disaster event forecasting. Although current algorithms can achieve significant event prediction accuracy, the historic data or the self-excitation property can introduce biased prediction. For example, hotspots ranked by event hazard rates can make the visibility of a disadvantaged group (e.g., racial minorities or the communities of lower social economic status) more apparent. Existing methods have explored ways to achieve parity between the groups by penalizing the objective function with several group fairness metrics. However, these metrics fail to measure the fairness on every prefix of the ranking. In this paper, we propose a novel list-wise fairness criterion for point processes, which can efficiently evaluate the ranking fairness in event prediction. We also present a strict definition of the unfairness consistency property of a fairness metric and prove that our list-wise fairness criterion satisfies this property. Experiments on several real-world spatial-temporal sequence datasets demonstrate the effectiveness of our list-wise fairness criterion.},
	address = {New York, NY, USA},
	author = {Shang, Jin and Sun, Mingxuan and Lam, Nina S.N.},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3394486.3403246},
	isbn = {9781450379984},
	keywords = {spatial-temporal point process, ranking, list-wise, fairness},
	location = {Virtual Event, CA, USA},
	numpages = {11},
	pages = {1948–1958},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {List-wise Fairness Criterion for Point Processes},
	url = {https://doi.org/10.1145/3394486.3403246},
	year = {2020},
}

@inproceedings{10.1145/3394486.3403327,
	abstract = {Given a set of locations in a city, on which ones should we place ads on so as to reach as many people as possible within a limited budget? Past research has addressed this question under the assumption that dense trajectory data are available to determine the reach of each ad. However, the data that are available in most industrial settings do not consist of dense, long-range trajectories; instead, they consist of statistics on people's short-range point-to-point movements. In this paper, we address the natural problem that arises such data: given a distribution of population and point-to-point movement statistics over a network, find a set of locations within a budget that achieves maximum expected reach. We call this problem geodemographic influence maximization (GIM). We show that the problem is NP-hard, but its objective function is monotone and submodular, thus admits a greedy algorithm with a 1 over 2 (1-1 over e) approximation ratio. Still, this algorithm is inapplicable on large-scale data for high-frequency digital signage ads. We develop an efficient deterministic algorithm, Lazy-Sower, exploiting a novel, tight double-bounding scheme of marginal influence gain as well as the locality proprieties of the problem; a learning-based variant, NN-Sower, utilizes randomization and deep learning to further improve efficiency, with a slight loss of quality. Our exhaustive experimental study on two real-world urban datasets demonstrates the efficacy and efficiency of our solutions compared to baselines.},
	address = {New York, NY, USA},
	author = {Zhang, Kaichen and Zhou, Jingbo and Tao, Donglai and Karras, Panagiotis and Li, Qing and Xiong, Hui},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3394486.3403327},
	isbn = {9781450379984},
	keywords = {submodular optimization, outdoor advertising, neural network, influence maximization, geodemographic influence, approximation algorithm},
	location = {Virtual Event, CA, USA},
	numpages = {11},
	pages = {2764–2774},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {Geodemographic Influence Maximization},
	url = {https://doi.org/10.1145/3394486.3403327},
	year = {2020},
}

@inproceedings{10.1145/3394486.3403386,
	abstract = {Computing estimated time of arrival (ETA) is one of the most important services for online ride-hailing platforms like DiDi and Uber. With billions of service queries per day on such platforms, a fast inference ETA module ensures the efficiency of the overall decision system to guarantee satisfied user experience, as well as saving significant operating cost. In this paper, we develop a novel ETA learning system named as CompactETA, which provides an accurate online travel time inference within 100 microseconds. In the proposed method, we encode high order spatial and temporal dependency into sophisticated representations by applying graph attention network on a spatiotemporal weighted road network graph. We further encode the sequential information of the travel route by positional encoding to avoid the recurrent network structure. The properly learnt representations enable us to apply a very simple multi-layer perceptron model for online real-time inference. Evaluation of both offline experiments and online A/B testing verifies that CompactETA reduces the inference latency by more than 100 times compared to a state-of-the-art system, while maintains competing prediction accuracy.},
	address = {New York, NY, USA},
	author = {Fu, Kun and Meng, Fanlin and Ye, Jieping and Wang, Zheng},
	booktitle = {Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3394486.3403386},
	isbn = {9781450379984},
	keywords = {estimated time of arrival, graph attention network, low-latency system, positional encoding},
	location = {Virtual Event, CA, USA},
	numpages = {9},
	pages = {3337–3345},
	publisher = {Association for Computing Machinery},
	series = {KDD '20},
	title = {CompactETA: A Fast Inference System for Travel Time Prediction},
	url = {https://doi.org/10.1145/3394486.3403386},
	year = {2020},
}

@inproceedings{10.1145/3397056.3397058,
	abstract = {Community business is the foundation of urban commerce, an important carrier to meet the comprehensive consumption of residents, and an important element of urban quality improvement. Based on the collection and processing of internet big data, combined with the understanding of the connotation of community business, this paper uses GIS spatial analysis technology and nuclear density analysis technology to analyze the spatial distribution characteristics and format changes of community business in the Chongqing. The results show that: (1) The community business in the Chongqing shows a state of "the south is more important than the north". (2) The community business in the Chongqing shows the form of "multi-core + multi-node". In addition to the obvious clustering of community business in the surrounding city business area, there are also many strong community businesses co-existing, and its nuclear density value is comparable to that of the community business in the surrounding city business area, which is in line with the multi-group spatial layout in the Chongqing. (3) There is a strong spatial correlation between the community business and the resident population as a whole, the correlation coefficient is 0.82. (4) From 2014 to 2019, the growth of community business space in the Chongqing is mainly within the inner ring, while showing the trend of diffusion from the inner ring to the outer ring. (5) From 2014 to 2019, the number of commercial shopping facilities in the Chongqing increased, while the number of catering facilities decreased.},
	address = {New York, NY, USA},
	author = {He, Zong and Ye, Sheng and Jia, Yahui and Liu, Jian},
	booktitle = {Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3397056.3397058},
	isbn = {9781450377416},
	keywords = {Spatial characteristics, POI, GIS, Community business, Business format},
	location = {Marseille, France},
	numpages = {5},
	pages = {19–23},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '20},
	title = {Analysis of Spatial Distribution Characteristics and Format Changes of Community Business Based on Internet Big Data},
	url = {https://doi.org/10.1145/3397056.3397058},
	year = {2020},
}

@inproceedings{10.1145/3397056.3397059,
	abstract = {Traffic congestion not only causes significant losses to city economy, but also seriously affects the happiness index of urban residents. Different from the specific and detailed research perspective of traditional traffic congestion, this paper attempts to study urban traffic congestion from a relatively macro perspective. The real-time traffic data, building survey data, Gaode POI data, bus stops data, urban road, population data, DEM data are collected. The spatial heterogeneity law of urban traffic congestion and its influencing factors are analyzed by GIS. The analysis found that:(1) Traffic congestion in Chongqing is "heavy in the north and light in the south", and the business district has a relatively obvious impact on traffic congestion. (2) Population density and commercial POI have a great impact on traffic congestion. For every 0.1 percentage point increase in the proportion of commercial POI, the average traffic jam time increased by 0.09 hours. (3) The number of bus stops can significantly alleviate traffic congestion. For every additional bus station, the traffic congestion time in this area can be reduced by 0.05 hours. (4) Increasing road density cannot effectively solve the problem of traffic congestion, which once again proves the correctness of "down Law". (5) Within the community, increasing the diversity of land use cannot effectively relieve traffic congestion.},
	address = {New York, NY, USA},
	author = {Ye, Sheng and He, Zong and Jia, Yahui and Luo, Yachen},
	booktitle = {Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3397056.3397059},
	isbn = {9781450377416},
	keywords = {Traffic congestion, Space differentiation, Influencing factors, GWR, GIS},
	location = {Marseille, France},
	numpages = {5},
	pages = {48–52},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '20},
	title = {Analysis of Spatial Heterogeneity and Influencing Factors of Urban Traffic Congestion Based on GIS},
	url = {https://doi.org/10.1145/3397056.3397059},
	year = {2020},
}

@inproceedings{10.1145/3397056.3397075,
	abstract = {In recent years, flood-risk analysis has played an important role in the evaluation of submergence and flooding simulation. In this paper, we study a flood point query problem: Given a terrain T and total volume of rainfall in the area, determine if a given point is flooded. Existing methods build a contour tree by exacting all data points from T, and then get a merge tree, which is complicated with increasing resolution of terrains. Given the volume of rain, this paper proposes a flood analysis algorithm based on a fast binary merge tree generation. By eliminating the invalid saddle vertices in the data, the algorithm directly establishes the merger tree according to the corresponding contour hierarchy. Besides, the area and volume are also attached to enrich the merge tree. We describe a suite of experimental results showing the performance of our algorithm in practice and the running time of the preprocessing step is greatly reduced.},
	address = {New York, NY, USA},
	author = {Wu, Ye and Wu, Xuqiao and Chen, Luo},
	booktitle = {Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3397056.3397075},
	isbn = {9781450377416},
	keywords = {terrain, point flood query, fast binary merge tree},
	location = {Marseille, France},
	numpages = {5},
	pages = {38–42},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '20},
	title = {Point Flood Query Based on Fast Binary Merge Tree},
	url = {https://doi.org/10.1145/3397056.3397075},
	year = {2020},
}

@inproceedings{10.1145/3397056.3397079,
	abstract = {The evaluation of the environmental sensitivity to desertification of the rapidly changing Mediterranean landscape under the effects of climate change is evidently necessary for planning a sustainable method of management policies. This paper aim is to evaluate the current sensitivity to desertification of Basilicata region in southern Italy, where this phenomenon has been increasing over the past years, without control. Losses are strongly impacting on the potential regional progress, both economic and social, leading also to a huge ecological damage. The analysis was carried out based on the MEDALUS (MEditerrean Desertification and Land Use) model, developed within a dedicated European project. This method identifies the areas of the region that are more likely to be sensible to desertification thought the Environmentally Sensitive Areas (ESAs) index. Starting with this methodology, the model parameters were implemented and then processed with a GIS-based approach, in order to evaluate soil, climate, management and vegetation quality factors, that represent the necessary input for assessing ESAs. The results were useful to indicate that the region is effectively lacking of management policies in those highly sensitive areas, due to a proved scarcity of previous studies on the topic. This study may open new opportunities for a further more conscious planning to prevent landscape, land and soil degradation.},
	address = {New York, NY, USA},
	author = {Gabriele, Marzia and Previtali, Mattia},
	booktitle = {Proceedings of the 2020 3rd International Conference on Geoinformatics and Data Analysis},
	doi = {10.1145/3397056.3397079},
	isbn = {9781450377416},
	keywords = {sensitivity, remote sensing, landscape management planning, land degradation, desertification, climate change, UNCCD, NDVI, MEDALUS, ESAs indicators},
	location = {Marseille, France},
	numpages = {5},
	pages = {62–66},
	publisher = {Association for Computing Machinery},
	series = {ICGDA '20},
	title = {A GIS and Remote Sensing Approach for Desertification Sensitivity Assessment in Basilicata Region (Italy)},
	url = {https://doi.org/10.1145/3397056.3397079},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422198,
	abstract = {Given a spatial graph and a set of node attributes, the Node-attributed Spatial Graph Partitioning (NSGP) problem partitions a node-attributed spatial graph into k homogeneous sub-graphs that minimize both the total RMSErank1 and edge-cuts while meeting a size constraint on the sub-graphs. RMSErank1 is the Root Mean Square Error between a matrix and its rank-one decomposition. The NSGP problem is important for many societal applications such as identifying homogeneous communities in a spatial graph and detecting interrelated patterns in traffic accidents. This problem is NP-hard; it is computationally challenging because of the large size of spatial graphs and the constraint that the sub-graphs must be homogeneous, i.e. similar in terms of node attributes. This paper proposes a novel approach for finding a set of homogeneous sub-graphs that can minimize both the total RMSErank1 and edge-cuts while meeting the size constraint. Experiments and a case study using U.S. Census datasets and HP#6 watershed network datasets demonstrate that the proposed approach partitions a spatial graph into a set of homogeneous sub-graphs and reduces the computational cost.},
	address = {New York, NY, USA},
	author = {Bereznyi, Daniel and Qutbuddin, Ahmad and Her, YoungGu and Yang, KwangSoo},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422198},
	isbn = {9781450380195},
	keywords = {spatial graph partitioning, node-attributed spatial graph, matrix rank-one decomposition},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {58–67},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Node-attributed Spatial Graph Partitioning},
	url = {https://doi.org/10.1145/3397536.3422198},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422210,
	abstract = {Massive datasets of spatial trajectories representing the mobility of a diversity of moving objects are ubiquitous in research and industry. Similarity search of a large collection of trajectories is indispensable for turning these datasets into knowledge. Locality sensitive hashing (LSH) is a powerful technique for fast similarity searches. Recent methods employ LSH and attempt to realize an efficient similarity search of trajectories; however, those methods are inefficient in terms of search time and memory when applied to massive datasets. To address this problem, we present the trajectory-indexing succinct trit-array trie (tSTAT), which is a scalable method leveraging LSH for trajectory similarity searches. tSTAT quickly performs the search on a tree data structure called trie. We also present two novel techniques that enable to dramatically enhance the memory efficiency of tSTAT. One is a node reduction technique that substantially omits redundant trie nodes while maintaining the time performance. The other is a space-efficient representation that leverages the idea behind succinct data structures (i.e., a compressed data structure supporting fast data operations). We experimentally test tSTAT on its ability to retrieve similar trajectories for a query from large collections of trajectories and show that tSTAT performs superiorly in comparison to state-of-the-art similarity search methods.},
	address = {New York, NY, USA},
	author = {Kanda, Shunsuke and Takeuchi, Koh and Fujii, Keisuke and Tabei, Yasuo},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422210},
	isbn = {9781450380195},
	keywords = {succinct data structures, scalable similarity search, Trajectory data mining, Fr\'{e}chet distance},
	location = {Seattle, WA, USA},
	numpages = {12},
	pages = {518–529},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Succinct Trit-array Trie for Scalable Trajectory Similarity Search},
	url = {https://doi.org/10.1145/3397536.3422210},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422211,
	abstract = {We study the problem of tracking the movement of groups using sparse trajectory data extracted from Location Based Social Networks (LBSNs). Tracking group movement using LBSN data is challenging because the data may contain a large amount of noise due to the lack of stability in group entity, spatial extent and posting time. We propose a first-of-its-kind solution, Group Kalman Filter (GKF), which aims to improve the effectiveness of group tracking by predicting the spatial properties of groups with a group movement model. Our experiments with real LBSN data and synthetic LBSN data show that GKF can detect groups and predict group movement with a high level of accuracy and efficiency.},
	address = {New York, NY, USA},
	author = {Kannangara, Sameera and Xie, Hairuo and Tanin, Egemen and Harwood, Aaron and Karunasekera, Shanika},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422211},
	isbn = {9781450380195},
	keywords = {Tracking, Moving Objects, LBSN, Kalman Filter},
	location = {Seattle, WA, USA},
	numpages = {12},
	pages = {251–262},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Tracking Group Movement in Location Based Social Networks},
	url = {https://doi.org/10.1145/3397536.3422211},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422213,
	abstract = {A geo-marketplace allows users to be paid for their location data. Users concerned about privacy may want to charge more for data that pinpoints their location accurately, but may charge less for data that is more vague. A buyer would prefer to minimize data costs, but may have to spend more to get the necessary level of accuracy. We call this interplay between privacy, utility, and price spatial privacy pricing. We formalize the issues mathematically with an example problem of a buyer deciding whether or not to open a restaurant by purchasing location data to determine if the potential number of customers is sufficient to open. The problem is expressed as a sequential decision making problem, where the buyer first makes a series of decisions about which data to buy and concludes with a decision about opening the restaurant or not. We present two algorithms to solve this problem, including experiments that show they perform better than baselines.},
	address = {New York, NY, USA},
	author = {Nguyen, Kien and Krumm, John and Shahabi, Cyrus},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422213},
	isbn = {9781450380195},
	keywords = {privacy pricing, location privacy, geo-marketplace},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {263–272},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Spatial Privacy Pricing: The Interplay between Privacy, Utility and Price in Geo-Marketplaces},
	url = {https://doi.org/10.1145/3397536.3422213},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422215,
	abstract = {Consider the following scenario: (a) a worker traveling on the shortest path between two locations in a city's road network, (b) he/she is willing to deviate from such path in order to complete tasks in the network, (c) tasks are associated with rewards and appear and disappear dynamically, i.e., they are not known in advance, and (d) the worker specifies a time budget which limits the total time he/she is willing to spend on his/her trip. Now assume the worker wants to minimize the detour from the original path while, at the same time, maximizing the rewards collected by completing tasks; clearly two competing criteria. We call this problem the Online In-Route Task Selection (Online-IRTS) query, and we investigate it using the paradigm of skyline queries in order to systematically explore different trade-offs between earned rewards and path deviation. Because of the online nature of the problem, i.e., irrevocable decisions about which task to perform have to be made without knowledge of future tasks, it is not possible to guarantee optimal solutions for the Online-IRTS query. Therefore, we propose two heuristic approaches, one is based on local optimizations, and the other one is based on incremental solutions, along with a method to evaluate the quality of their solutions w.r.t. the optimal offline solution. Our experiments using city-scale realistic datasets show that the first approach is more effective whereas the second is more efficient, allowing one to choose which approach to use according to his/her priorities.},
	address = {New York, NY, USA},
	author = {Costa, Camila F. and Nascimento, Mario A.},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422215},
	isbn = {9781450380195},
	keywords = {Spatial Crowdsourcing, Skyline, Road Networks, In-Route Queries},
	location = {Seattle, WA, USA},
	numpages = {12},
	pages = {239–250},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Online In-Route Task Selection in Spatial Crowdsourcing},
	url = {https://doi.org/10.1145/3397536.3422215},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422218,
	abstract = {Map matching has long been a fundamental yet challenging problem. However, there are currently only a few public small-scale map matching benchmark datasets. Both the GPS trajectories and the road network in the existing map matching datasets are represented by location only, which cannot support the development of data-driven and semantic-enriched map matching algorithms that have increasingly emerged in recent years. To bridge the gap, we present the first large-scale attribute-rich map matching benchmark dataset covering two cities in Southeast Asia (i.e., Singapore and Jakarta). Our GPS trajectories contain rich contextual information including the accuracy level, bearing, speed, and transport mode in addition to the latitude and longitude geo-coordinates. The underlying road network is a snapshot of the OpenStreetMap where roads are associated with rich attributes such as road type, speed limit, etc. To ensure the quality of our dataset, the annotation of the map-matched routes has been conducted by a team of professional map operators. Analysis on our dataset provides new insights into the challenges and opportunities in map matching algorithms.},
	address = {New York, NY, USA},
	author = {Xu, Zhengmin and Yin, Yifang and Dai, Chengcheng and Huang, Xiaocheng and Kudali, Robinson and Foflia, Jinal and Wang, Guanfeng and Zimmermann, Roger},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422218},
	isbn = {9781450380195},
	keywords = {map matching, digital maps, datasets, GPS trajectories, GIS},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {171–174},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Grab-Posisi-L: A Labelled GPS Trajectory Dataset for Map Matching in Southeast Asia},
	url = {https://doi.org/10.1145/3397536.3422218},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422220,
	abstract = {The wide spread of GPS-enabled devices and the Internet of Things (IoT) has increased the amount of spatial data being generated every second. The current scale of spatial data cannot be handled using centralized systems. This has led to the development of distributed spatial data streaming systems that scale to process in real-time large amounts of streamed spatial data. The performance of distributed streaming systems relies on how even the workload is distributed among their machines. However, it is challenging to estimate the workload of each machine because spatial data and query streams are skewed and rapidly change with time and users' interests. Moreover, a distributed spatial streaming system often does not maintain a global system workload state because it requires high network and processing overheads to be collected from the machines in the system.This paper introduces TrioStat; an online workload estimation technique that relies on a probabilistic model for estimating the workload of partitions and machines in a distributed spatial data streaming system. It is infeasible to collect and exchange statistics with a centralized unit because it requires high network overhead. Instead, TrioStat uses a decentralised technique to collect and maintain the required statistics in real-time locally in each machine. TrioStat enables distributed spatial data streaming systems to compare the workloads of machines as well as the workloads of data partitions. TrioStat requires minimal network and storage overhead. Moreover, the required storage is distributed across the system's machines.},
	address = {New York, NY, USA},
	author = {Daghistani, Anas and Aref, Walid G. and Ghafoor, Arif},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422220},
	isbn = {9781450380195},
	keywords = {spatial stream processing, load balancing, distributed streaming systems, collecting statistics, Workload estimation},
	location = {Seattle, WA, USA},
	numpages = {9},
	pages = {78–86},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {TrioStat: Online Workload Estimation in Distributed Spatial Data Streaming Systems},
	url = {https://doi.org/10.1145/3397536.3422220},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422231,
	abstract = {Light Detection and Ranging (LiDAR) sensors generate dense point clouds that can be used to map forest structures at a high spatial resolution level. In this work, we consider the problem of identifying individual trees in a LiDAR point cloud. Existing techniques generally require intense parameter tuning and user interactions. Our goal is defining an automatic approach capable of providing robust results with minimal user interactions.To this end, we define a segmentation algorithm based on the watershed transform and persistence-based simplification. The proposed algorithm uses a divide-and-conquer technique to split a LiDAR point cloud into regions with uniform density. Within each region, single trees are identified by applying a segmentation approach based on watershed by simulated immersion. Experiments show that our approach performs better than state-of-the-art algorithms on most of the study areas in the benchmark provided by the NEW technologies for a better mountain FORest timber mobilization (NEWFOR) project. Moreover, our approach requires a single (Boolean) parameter. This makes our approach well suited for a wide range of forest analysis applications, including biomass estimation, or field inventory surveys.},
	address = {New York, NY, USA},
	author = {Xu, Xin and Iuricich, Federico and De Floriani, Leila},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422231},
	isbn = {9781450380195},
	keywords = {Watershed transform, Tree segmentation, Topological persistence, LiDAR},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {191–194},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {A Persistence-Based Approach for Individual Tree Mapping},
	url = {https://doi.org/10.1145/3397536.3422231},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422232,
	abstract = {Geospatial data providers have adopted a variety of science gateways as the primary method for accessing remote geospatial data. Early systems provided little more than a simple file transfer mechanism but over the past decade, advanced features were incorporated to allow users to retrieve data seamlessly without concern for native file formats, data resolution, or even spatial projections. However, the recent growth in Deep Learning models in the geospatial domains has exposed additional requirements for accessing geospatial repositories. In this paper we discussed the major data accessibility challenges faced by the Deep Learning community namely: (1) reproducibility of data preprocessing workflows, (2) optimizing data transfer between gateways and computational environments, and (3) minimizing local storage requirements using on-the-fly augmentation. In this paper, we present our vision of spatial data generators to act as middleware between geospatial data gateways and Deep Learning models. We propose advanced features for spatial data generators and describe how they could satisfy the data accessibility requirements of the geospatial Deep Learning community. Lastly, we argue that satisfying these data accessibility requirements will not only enhance the reproducibility of Deep Learning workflows and speed their development but will also improve the quality of training and prediction of operational Deep Learning models.},
	address = {New York, NY, USA},
	author = {Soliman, Aiman and Terstriep, Jeffrey},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422232},
	isbn = {9781450380195},
	keywords = {Scientific Reproducibility, Remote Sensing, Image Preprocessing, Geospatial Data Gateway, Geospatial Big Data, Deep Learning},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {593–596},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Leveraging Geospatial Data Gateways to Support the Operational Application of Deep Learning Models: Vision Paper},
	url = {https://doi.org/10.1145/3397536.3422232},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422243,
	abstract = {Location fingerprinting is a technique for determining the location of a device by measuring ambient signals such as radio signal strength, temperature, or any signal that varies with location. The accuracy of the technique is compromised by signal noise, quantization, and limited calibration resources. We develop generic, probabilistic models of location fingerprinting to find accuracy estimates. In one case, we look at predeployment modeling to predict accuracy before any signals have been measured using a new concept of noisy reverse geocoding. In another case, we model a previously deployed system to predict its accuracy. The models allow us to explore the accuracy implications of signal noise, calibration effort, and quantization of signals and space.},
	address = {New York, NY, USA},
	author = {Krumm, John},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422243},
	isbn = {9781450380195},
	keywords = {signal fingerprinting, noisy reverse geocoding, location, indoor location, geocoding, WiFi location},
	location = {Seattle, WA, USA},
	numpages = {8},
	pages = {560–567},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Location Accuracy Estimates for Signal Fingerprinting},
	url = {https://doi.org/10.1145/3397536.3422243},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422245,
	abstract = {Due to the massively increasing amount of available geospatial data and the need to present it in an understandable way, clustering this data is more important than ever. As clusters might contain a large number of objects, having a representative for each cluster significantly facilitates understanding a clustering. Clustering methods relying on such representatives are called center-based. In this work we consider the problem of center-based clustering of trajectories.In this setting, the representative of a cluster is again a trajectory. To obtain a compact representation of the clusters and to avoid overfitting, we restrict the complexity of the representative trajectories by a parameter l. This restriction, however, makes discrete distance measures like dynamic time warping (DTW) less suited.There is recent work on center-based clustering of trajectories with a continuous distance measure, namely, the Fr\'{e}chet distance. While the Fr\'{e}chet distance allows for restriction of the center complexity, it can also be sensitive to outliers, whereas averaging-type distance measures, like DTW, are less so. To obtain a trajectory clustering algorithm that allows restricting center complexity and is more robust to outliers, we propose the usage of a continuous version of DTW as distance measure, which we call continuous dynamic time warping (CDTW). Our contribution is twofold:(1) To combat the lack of practical algorithms for CDTW, we develop an approximation algorithm that computes it.(2) We develop the first clustering algorithm under this distance measure and show a practical way to compute a center from a set of trajectories and subsequently iteratively improve it.To obtain insights into the results of clustering under CDTW on practical data, we conduct extensive experiments.},
	address = {New York, NY, USA},
	author = {Brankovic, Milutin and Buchin, Kevin and Klaren, Koen and Nusser, Andr\'{e} and Popov, Aleksandr and Wong, Sampson},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422245},
	isbn = {9781450380195},
	keywords = {Trajectory Similarity, Trajectory Clustering, Continuous Dynamic Time Warping},
	location = {Seattle, WA, USA},
	numpages = {12},
	pages = {99–110},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {(k, l)-Medians Clustering of Trajectories Using Continuous Dynamic Time Warping},
	url = {https://doi.org/10.1145/3397536.3422245},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422248,
	abstract = {Human mobility literature is limited in their ability to capture the novelty-seeking or the exploratory tendency of individuals. Mainly, the vast majority of mobility prediction models rely uniquely on the history of visited locations (as captured in the input dataset) to predict future visits. This hinders the prediction of new unseen places and reduces prediction accuracy. In this paper, we show that a two-dimensional modeling of human mobility, which explicitly captures both regular and exploratory behaviors, yields a powerful characterization of users. Using such model, we identify the existence of three distinct mobility profiles with regard to the exploration phenomenon - Scouters (i.e., extreme explorers), Routiners (i.e., extreme returners), and Regulars (i.e., without extreme behavior). Further, we extract and analyze the mobility traits specific to each profile. We then investigate temporal and spatial patterns in each mobility profile and show the presence of recurrent visiting behavior of individuals even in their novelty-seeking moments. Our results unveil important novelty preferences of people, which are ignored by literature prediction models. Finally, we show that prediction accuracy is dramatically affected by exploration moments of individuals. We then discuss how our profiling methodology could be leveraged to improve prediction.},
	address = {New York, NY, USA},
	author = {Amichi, Licia and Viana, Aline Carneiro and Crovella, Mark and Loureiro, Antonio A.F.},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422248},
	isbn = {9781450380195},
	keywords = {Mobility Profiling, Individual Mobility, Exploration},
	location = {Seattle, WA, USA},
	numpages = {11},
	pages = {314–324},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Understanding individuals' proclivity for novelty seeking},
	url = {https://doi.org/10.1145/3397536.3422248},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422255,
	abstract = {In this paper, we investigate the suitability of state-of-the-art representation learning methods to the analysis of behavioral similarity of moving individuals, based on CDR trajectories. The core of the contribution is a novel methodological framework, mob2vec, centered on the combined use of a recent symbolic trajectory segmentation method for the removal of noise, a novel trajectory generalization method incorporating behavioral information, and an unsupervised technique for the learning of vector representations from sequential data. mob2vec is the result of an empirical study conducted on real CDR data through an extensive experimentation. As a result, it is shown that mob2vec generates vector representations of CDR trajectories in low dimensional spaces which preserve the similarity of the mobility behavior of individuals.},
	address = {New York, NY, USA},
	author = {Damiani, Maria Luisa and Acquaviva, Andrea and Hachem, Fatima and Rossini, Matteo},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422255},
	isbn = {9781450380195},
	keywords = {symbolic trajectories, representation learning, human mobility},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {367–376},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Learning Behavioral Representations of Human Mobility},
	url = {https://doi.org/10.1145/3397536.3422255},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422264,
	abstract = {Geometric intersection algorithms are fundamental in spatial analysis in Geographic Information System (GIS). Applying high performance computing to perform geometric intersection on huge amount of spatial data to get real-time results is necessary. Given two input geometries (polygon or polyline) of a candidate pair, we introduce a new two-step geospatial filter that first creates sketches of the geometries and uses it to detect workload and then refines the sketches by the common areas of sketches to decrease the overall computations in the refine phase. We call this filter PolySketch-based CMBR (PSCMBR) filter. We show the application of this filter in speeding-up line segment intersections (LSI) reporting task that is a basic computation in a variety of geospatial applications like polygon overlay and spatial join.We also developed a parallel PolySketch-based PNP filter to perform PNP tests on GPU which reduces computational workload in PNP tests. Finally, we integrated these new filters to the hierarchical filter and refinement (HiFiRe) system to solve geometric intersection problem. We have implemented the new filter and refine system on GPU using CUDA. The new filters introduced in this paper reduce more computational workload when compared to existing filters. As a result, we get on average 7.96X speedup compared to our prior version of HiFiRe system.},
	address = {New York, NY, USA},
	author = {Liu, Yiming and Puri, Satish},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422264},
	isbn = {9781450380195},
	keywords = {Spatial Operations, Parallel Algorithms, HPC, CUDA},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {487–496},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Efficient Filters for Geometric Intersection Computations using GPU},
	url = {https://doi.org/10.1145/3397536.3422264},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422269,
	abstract = {An important problem in terrain analysis is modeling how water flows across a terrain creating floods by forming channels and filling depressions. In this paper we study a number of flow-query related problems: given a terrain Σ represented as a triangulated xy-monotone surface with n vertices, and a rain distribution R which may vary over time, determine how much water is flowing over a given edge as a function of time. We develop internal-memory as well as I/O-efficient algorithms for flow queries. This paper contains four main results:(i) An internal-memory algorithm for answering terrain-flow queries: preprocess Σ into a linear-size data structure so that given a rain distribution R the flow-rate functions of all edges of Σ can be reported quickly.(ii) I/O-efficient algorithms for answering terrain-flow queries.(iii) An internal memory algorithm for answering edge-flow queries: preprocess Σ into a linear-size data structure so that given a rain distribution R, the flow-rate function of an edge under the single-flow direction (SFD) model can be computed quickly.(iv) We present an efficient algorithm that given a path in Σ computes the two-dimensional channel along which water flows.},
	address = {New York, NY, USA},
	author = {Lowe, Aaron and Svendsen, Svend C. and Agarwal, Pankaj K. and Arge, Lars},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422269},
	isbn = {9781450380195},
	keywords = {river-network extraction, hydrological modeling, flood-risk analysis, Terrains},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {5–14},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {1D and 2D Flow Routing on a Terrain},
	url = {https://doi.org/10.1145/3397536.3422269},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422271,
	abstract = {Geospatial data integration combines two or more data layers to facilitate advanced querying, analysis, reasoning, and visualization. In general, different layers (e.g., ZIP codes, census blocks, school districts, and land use parcels) have different spatial partitions and different types of associated semantic descriptors. In addition, geospatial data may contain errors (e.g., due to imprecision in the measurements or to representation constraints) causing uncertainty that needs to be incorporated and quantified in the query answers. In this paper, we leverage semantic descriptors in heterogeneous information layers to build a data structure that enables efficient processing of geospatial range queries by returning an estimate of the answer together with an error bound. We present the processing algorithms and evaluate our approach by means of experiments that encompass large datasets, demonstrating the benefits of our approach.},
	address = {New York, NY, USA},
	author = {Trajcevski, Goce and Balasubramani, Booma Sowkarthiga and Cruz, Isabel F. and Tamassia, Roberto and Teng, Xu},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422271},
	isbn = {9781450380195},
	keywords = {Uncertainty, Semantics, Range Queries, Geospatial Data integration},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {68–77},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Semantically Augmented Range Queries over Heterogeneous Geospatial Data},
	url = {https://doi.org/10.1145/3397536.3422271},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422272,
	abstract = {Each year, millions of people either die or get injured due to road incidents. Thus, integrating safety optimization techniques into future traffic systems is of utmost importance. Emerging connected vehicle technologies have enabled ways to manage traffic networks with optimization goals such as travel time efficiency, fuel efficiency. However, these existing studies have focused less on maximizing traffic safety. Increasing space between vehicles in the road network with an acceptable travel time increase will help to improve the safety of the system. We propose the Platooning Graph, which is capable of modelling the inter-vehicular spacing optimization problem and we provide a fast and readily deployable algorithm to find a good approximate solution. Using microscopic traffic simulations, we demonstrate how the proposed method can improve safety, with minimal impact on travel time.},
	address = {New York, NY, USA},
	author = {Muthugama, Lakmal and Karunasekera, Shanika and Tanin, Egemen},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422272},
	isbn = {9781450380195},
	keywords = {vehicle platooning, vehicle following systems, traffic safety, traffic management systems, temporal graphs},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {453–456},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {Platooning Graph for Safer Traffic Management},
	url = {https://doi.org/10.1145/3397536.3422272},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422342,
	abstract = {We present staty, a browser-based tool for quality assurance of public transit station tagging in OpenStreetMap (OSM). Building on the results of a similarity classifier for these stations, our tool visualizes name tag errors as well as incorrect and/or missing station group relations. Detailed edit suggestions are provided for individual objects. This is done intrinsically without an external ground truth. Instead, the underlying classifier is trained on the OSM data itself. We describe how our tool derives errors and suggestions from station tag similarities and provide experimental results on the OSM data of the United Kingdom, the United States, and a dataset consisting of Germany, Switzerland, and Austria. Our tool can be accessed under https://staty.cs.uni-freiburg.de.},
	address = {New York, NY, USA},
	author = {Bast, Hannah and Brosi, Patrick and N\"{a}ther, Markus},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422342},
	isbn = {9781450380195},
	keywords = {Quality Assurance, Public Transit Data, OpenStreetMap Data},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {207–210},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {staty: Quality Assurance for Public Transit Stations in OpenStreetMap},
	url = {https://doi.org/10.1145/3397536.3422342},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422343,
	abstract = {With the development of Light Detection and Ranging (LiDAR) technology, point cloud data is a valuable resource to build three-dimensional (3D) models of digital twins. The geospatial 3D model is the principal element to abstract a geographic feature with geometric and semantic properties. The 3D model data provides more efficiency to handle, retrieve, exchange, and visualize geographic features compared to point clouds. However, the construction of 3D models, especially indoor space where various objects exist, usually necessitates expensive time and manual labor resources to organize and extract the geometry information by authoring tools.This demonstration introduces Point-in Space-out (PinSout), a new framework to automatically generate 3D space models from raw 3D point cloud data by leveraging three open-source software: PointNet, Point Cloud Library (PCL), and 3D City Database (3DCityDB). The framework performs the semantic segmentation by PointNet, a deep learning algorithm for the point cloud, to assign a target label to each point from a point cloud, such as walls, floors, and ceilings. It then divides the point cloud into each label cluster and computes surface elements by PCL. Each surface is stored into a 3DCityDB database to export an OGC CityGML data. Finally, we evaluate the accuracy with two datasets: a synthetic point-cloud set of a 3D model and a real dataset taken from the exhibition halls.},
	address = {New York, NY, USA},
	author = {Kim, Taehoon and Cho, Wijae and Matono, Akiyoshi and Kim, Kyoung-Sook},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422343},
	isbn = {9781450380195},
	keywords = {point cloud, open source, deep learning, citygml, 3d model},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {211–214},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {PinSout: Automatic 3D Indoor Space Construction from Point Clouds with Deep Learning},
	url = {https://doi.org/10.1145/3397536.3422343},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422347,
	abstract = {As an open license project, Open Street Map (OSM) aims to make the collectively produced geographic data freely available to be used for various purposes. Routing engines frequently take advantage of this data set. Nonetheless, providing routing services on top of OSM requires the full connectivity of the OSM road network graph in the interest area. This connectivity needs to be achieved individually at every level of the road network graph: the motorway, trunk, primary, secondary, tertiary, and residential roads. However, due to its open-editing nature, the OSM data often contains faults attributed to issues like missing road network connections or mistakenly attributed road segments.In this paper, we demonstrate a system we have developed that helps the end-user (i.e., cartographer) discover and fix the connectivity errors in an OSM road network graph. More specifically, the system aims to achieve full connectivity in the overall road network graph, which in turn requires full connectivity at each road level. The system automatically detects the connectivity errors that would otherwise remain undetected or need a lengthy manual process to discover. It can accept hints from the editor through its easy to use graphical user interface to investigate errors further, improve the detection process, and subsequently fix them. Based on our pilot runs in New Zealand with the supervision of professional cartographers and a team from Microsoft Geospatial, we were able to detect more than 300 incorrect connections and to achieve connectivity across different road levels.},
	address = {New York, NY, USA},
	author = {Tabet, Fares and Patel, Birva H. and Dincer, Kivanc and Govind, Harsh and Cao, Peiwei and Song, Ashley and Ali, Mohamed},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422347},
	isbn = {9781450380195},
	keywords = {road network graph, road network disconnections, open street map, network error detection},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {421–424},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {A Semi-Automated System for Exploring and Fixing OSM Connectivity},
	url = {https://doi.org/10.1145/3397536.3422347},
	year = {2020},
}

@inproceedings{10.1145/3397536.3422348,
	abstract = {All online map service providers are working hard to maintain high-quality maps to provide high-quality services. Example inaccuracies that can be encountered in the provided maps may include missing road segments, shifted road segments, missing road connections, missing or incorrect turn restrictions, and mislabeling road attributes like marking a directional road as one-way. Maps may also be rapidly changing in some areas due to new constructions. While the accuracy of various mapping systems, as given by service providers, is known to be high, even the minor discrepancies in the underlying maps may lead to unsatisfactory user experience in routing and location-based services.In this paper, we present a system that compares the routes returned by the public APIs of some major routing engines, namely Bing Maps, Google Maps, and OpenStreetMap. The system highlights the differences in the proposed routes between these routing engines, given the same start/end points for a planned trip. The route differences are examined based on travel distance, travel duration, and route geometry. The system can also enforce a routing engine to take the same route as another routing engine to identify the possible discrepancies in the underlying mapping system of each routing engine. The system identifies and categorizes the discovered discrepancies, across various engines, in (1) the geometry of the road segments, (2) the connectivity and turn restrictions of the Road Network Graph (RNG), and (3) the attributes of the road segments. The presented system is currently in pilot use by a group of professional editors to support their daily work of identifying, visually inspecting, and interactively trying alternative corrections to the underlying RNGs in various parts of the globe. This helps us develop the system's capabilities even further based on their continuous feedback in real usage scenarios.},
	address = {New York, NY, USA},
	author = {Bandil, Ayush and Girdhar, Vaishali and Dincer, Kivanc and Govind, Harsh and Cao, Peiwei and Song, Ashley and Ali, Mohamed},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3422348},
	isbn = {9781450380195},
	keywords = {service, routing service, road network graph (RNG), real time spatial web, maps analysis, geospatial databases comparison, geographic information system, discrepancy analysis},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {425–428},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {An Interactive System to Compare, Explore and Identify Discrepancies across Map Providers},
	url = {https://doi.org/10.1145/3397536.3422348},
	year = {2020},
}

@inproceedings{10.1145/3397536.3428351,
	abstract = {advertisement (ad) recommendation services for mobile users is rapidly increasing. The conventional ways of recommending ads are based on the analysis of users' explicit behavior such as search keywords and keyword matching based on browsing history. However, it might not be effective enough for latent buyers. We have been working on an analysis of the user's latent interest on web browsing history which categorized positive and negative behaviors. In this paper, we adapt the method of the linked pages to real world locations using geo-tagged tweets. By several evaluations, we discuss the possibility to recommend ads according to the user's current location.},
	address = {New York, NY, USA},
	author = {Omura, Takanobu and Kawai, Yukiko and Nakajima, Shinsuke and Suzuki, Kenta and Kawai, Yukiko and Nakajima, Shinsuke},
	booktitle = {Proceedings of the 28th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3397536.3428351},
	isbn = {9781450380195},
	keywords = {regression analysis, geo-tagged tweets, Social networking sites},
	location = {Seattle, WA, USA},
	numpages = {2},
	pages = {665–666},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '20},
	title = {A Proposal of Latent Interest Analysis by Geo-tagged SNS for Advertisement Recommendation},
	url = {https://doi.org/10.1145/3397536.3428351},
	year = {2020},
}

@article{10.1145/3397579,
	abstract = {Road networks are major influential factors in the development of any nation. Due to different factors involved in their evolution, road networks exhibit complex structures, which result in problems such as inefficient traffic patterns, congestion, and environmental pollution. The existing metrics of road network analysis are found to be inadequate to reveal the complexity of networks that contributes to such problems. Therefore, we propose a new dimension for the road network analysis using road-types to understand such problems and analyze 57 cities from eight regions around the world: (i) India, (ii) North America, (iii) South America, (iv) Europe, (v) Asia, (vi) China, (vii) Japan, and (viii) Oceania. We define four new metrics: (i) Link Type Distribution, (ii) Link Type Demand, (iii) Preference Cost, and (iv) Type Closeness, for measuring the influence of road-types in the cities concerned. Link type distribution computes the share of each road-type in shortest paths while link type demand measures the type distribution considering the users’ preference of road-types. The metric preference cost is defined to measure the additional cost users incur on choosing higher-quality roads and type closeness computes the minimum distance a user needs to travel to reach the preferred road-type. Our analysis shows that, in cities of India, Europe, Asia, China, and Oceania, the primary, secondary, and tertiary roads play a crucial role in the shortest paths in terms of link type distribution as well as link type demand. Also, the involvement of secondary road-type is an important feature among cities from all regions except Japan, where the distribution and demand concentrated in tertiary as well as unclassified types. Cities in North and South American regions provide more importance to residential roads besides the secondary type. In addition to distribution and demand, our analysis shows that the primary, secondary, and tertiary road-types with higher demands suffer from link type deficits while the least demanded types are offered with link type surpluses, which explains one of the reasons for inefficient traffic patterns in congested cities. As far as the additional cost incurred for higher-quality roads is concerned, cities in India and USA provide least preference cost while Chinese and South American cities incur high costs. Among the 57 road networks concerned, Japan shows uniqueness in terms of the proposed metrics due to their inclusion of tertiary and unclassified roads.},
	address = {New York, NY, USA},
	articleno = {28},
	author = {Babu, Sarath and Manoj, B. S.},
	doi = {10.1145/3397579},
	issn = {2374-0353},
	issue_date = {December 2020},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {type dominance, type demand, type closeness, transportation systems, road networks, complex networks, Road-types},
	month = {aug},
	number = {4},
	numpages = {45},
	publisher = {Association for Computing Machinery},
	title = {Toward a Type-based Analysis of Road Networks},
	url = {https://doi.org/10.1145/3397579},
	volume = {6},
	year = {2020},
}

@article{10.1145/3402125,
	abstract = {SmarterROUTES contributes to personalised routing and navigation by data-driven route ranking and an environmentally aware road scene complexity-estimation mechanism. Traditional routing algorithms provide the fastest, shortest, or most ecological route by calculating the lowest-cost path from A to B using an underlying network of weighted connections. Our implementation goes a step further and ranks a set of such routes. The ranking is the result of additional weighing based on governmental data sets, extracts of the OpenStreetMap (OSM) database, or periodically adapted extracts of web services (e.g., Representational state transfer APIs). The selected data sets and their relative contributions to the overall ranking mechanism can be dynamically adapted by the end-user. Another major contribution toward a fully personalised navigation experience is the implementation of a road scene complexity scoring mechanism. Road complexity is estimated based on the combined input from geospatial data sources, traffic data, sensor analysis, and Street View–based complexity analysis. The latter input source uses Street View images as input for a Densenet Convolutional Neural Network (CNN), pre-trained on buildings using Bag-of-Words and Structure-from-motion features, outputting an image descriptor. The current version uses an adapted version of this network to predict road complexity scores. The predicted values correspond with the subjective complexity judgements of the end-users.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Bock, Jelle De and Verstockt, Steven},
	doi = {10.1145/3402125},
	issn = {2374-0353},
	issue_date = {March 2021},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {web and real-time applications, trajectory analysis (dealing with quality and uncertainty), scalable routing engines, machine learning, dynamic routing algorithms, algorithms, Data mining},
	month = {aug},
	number = {1},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {SmarterROUTES—A Data-driven Context-aware Solution for Personalized Dynamic Routing and Navigation},
	url = {https://doi.org/10.1145/3402125},
	volume = {7},
	year = {2020},
}

@inproceedings{10.1145/3403746.3403909,
	abstract = {In order to solve the problems of poor real-time performance and single display method in traditional unmanned aerial vehicle(UAV) orthographic image mosaic, this paper proposes a real-time updating technology of local area orthoimages based on 3D digital earth, which can be widely used in the fields of emergency rescue, environmental monitoring and military reconnaissance.The technology is that in the process of UAV aerial photography, the server receives sequence aerial image in real time and synchronously completes orthographic image splicing, then automatically cuts them into regular image tiles and transmits the regular image tiles to the display terminal in an incremental manner. The display terminal software constructs a 3D digital earth based on quadtree LOD algorithm, and updates the terrain texture of the target range with the received high-definition orthographic image tiles, so as to realize real-time coverage monitoring of key areas. In this paper, the above work is verified by real aerial images, and the results prove the feasibility and effectiveness of the work.},
	address = {New York, NY, USA},
	author = {Song, Li and Niu, Yugang and Deng, Baosong and Li, Jing and Xia, Xing and Gong, Yihang},
	booktitle = {Proceedings of the 3rd International Conference on Computer Science and Software Engineering},
	doi = {10.1145/3403746.3403909},
	isbn = {9781450375528},
	keywords = {Real-time update, Orthographic image mosaic, 3D digital earth},
	location = {Beijing, China},
	numpages = {6},
	pages = {83–88},
	publisher = {Association for Computing Machinery},
	series = {CSSE '20},
	title = {Real-time Updating Technology of Local Area Orthoimages Based on 3D Digital Earth},
	url = {https://doi.org/10.1145/3403746.3403909},
	year = {2020},
}

@inproceedings{10.1145/3403896.3403968,
	abstract = {The breach of users' location privacy can be catastrophic. To prevent privacy breaches, numerous location privacy methods have been developed in the last two decades. However, they have not been widely adopted in location-based applications. As a result, users' true location data is directly shared with untrusted service providers or researchers, raising concerns about location privacy. In this paper, we describe our effort to develop an open source repository, named Geopriv4j, in order to facilitate the adoption of location privacy methods in location-based services and research studies. Geopriv4j emphasizes on the practicality of location privacy, by identifying local, on-the-fly privacy methods under multiple categories. To facilitate adoption, Geopriv4j unifies the implementation of location privacy in Java, and provides usage examples as well as a sample Android app. To validate our implementation, we evaluate the location privacy methods in Geopriv4j with CPU, memory, and run time measures, using synthetically generated location traces.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Fan, Liyue and Gunja, Sriram Yechan},
	booktitle = {Proceedings of the Sixth International ACM SIGMOD Workshop on Managing and Mining Enriched Geo-Spatial Data},
	doi = {10.1145/3403896.3403968},
	isbn = {9781450380355},
	keywords = {open source, location-based applications, location privacy},
	location = {Portland, Oregon},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {GeoRich '20},
	title = {Geopriv4j: an open source repository for practical location privacy},
	url = {https://doi.org/10.1145/3403896.3403968},
	year = {2020},
}

@inproceedings{10.1145/3405962.3405987,
	abstract = {The paper focuses on calculating suitable place names and descriptive tags for large photo collections of visually interesting sights. The core dataset analyzed contains 45 million crowd-sourced geotagged pictures of the Panoramio database. We present several methods for analysis along with machine learning experiments for tag recommendation and suggest a manually built taxonomy of tag categories, based on the analysis of most widely used taglike words in the photo titles, along with their popularities. The methods, selected tags and the taxonomy can be used for building different tourism applications for visually interesting sights.},
	address = {New York, NY, USA},
	author = {Luberg, Ago and Pindis, Jakob and Tammet, Tanel},
	booktitle = {Proceedings of the 10th International Conference on Web Intelligence, Mining and Semantics},
	doi = {10.1145/3405962.3405987},
	isbn = {9781450375429},
	keywords = {POI categorization, crowd-sourced mapping, photo tagging, popularity analysis},
	location = {Biarritz, France},
	numpages = {10},
	pages = {149–158},
	publisher = {Association for Computing Machinery},
	series = {WIMS 2020},
	title = {Sights, titles and tags: mining a worldwide photo database for sightseeing},
	url = {https://doi.org/10.1145/3405962.3405987},
	year = {2020},
}

@article{10.1145/3406096,
	abstract = {The increasing availability of vehicular trajectory data is at the core of smart mobility solutions. Such data offer us unprecedented information for the development of trajectory data mining-based applications. An essential task of trajectory analysis is the employment of efficient and accurate methods to compare trajectories. This work presents a systematic survey of vehicular trajectory similarity measures and provides a panorama of the research field. First, we show an overview of vehicle trajectory data, including the models and some preprocessing techniques. Then, we give a comprehensive review of methods to compare trajectories and their intrinsic properties. We classify the methods according to the trajectory representation and features such as metricity, computational complexity, and robustness to noise and local time shift. Last, we discuss the applications of vehicular trajectory similarity measures and some open research problems.},
	address = {New York, NY, USA},
	articleno = {94},
	author = {Sousa, Roniel S. De and Boukerche, Azzedine and Loureiro, Antonio A. F.},
	doi = {10.1145/3406096},
	issn = {0360-0300},
	issue_date = {September 2021},
	journal = {ACM Comput. Surv.},
	keywords = {Datasets, mobility, trajectory, vehicle},
	month = {sep},
	number = {5},
	numpages = {32},
	publisher = {Association for Computing Machinery},
	title = {Vehicle Trajectory Similarity: Models, Methods, and Applications},
	url = {https://doi.org/10.1145/3406096},
	volume = {53},
	year = {2020},
}

@article{10.1145/3406596,
	abstract = {Given a spatial graph, an origin and a destination, and on-board diagnostics (OBD) data, the energy-efficient path selection problem aims to find the path with the least expected energy consumption (EEC). Two main objectives of smart cities are sustainability and prosperity, both of which benefit from reducing the energy consumption of transportation. The challenges of the problem include the dependence of EEC on the physical parameters of vehicles, the autocorrelation of the EEC on segments of paths, the high computational cost of EEC estimation, and potential negative EEC. However, the current cost estimation models for the path selection problem do not consider vehicles’ physical parameters. Moreover, the current path selection algorithms follow the “path + edge” pattern when exploring candidate paths, resulting in redundant computation. Our preliminary work introduced a physics-guided energy consumption model and proposed a maximal-frequented-path-graph shortest-path algorithm using the model. In this work, we propose an informed algorithm using an admissible heuristic and propose an algorithm to handle negative EEC. We analyze the proposed algorithms theoretically and evaluate the proposed algorithms via experiments with real-world and synthetic data. We also conduct two case studies using real-world data and a road test to validate the proposed method.},
	address = {New York, NY, USA},
	articleno = {22},
	author = {Li, Yan and Kotwal, Pratik and Wang, Pengyue and Xie, Yiqun and Shekhar, Shashi and Northrop, William},
	doi = {10.1145/3406596},
	issn = {2691-1922},
	issue_date = {August 2020},
	journal = {ACM/IMS Trans. Data Sci.},
	keywords = {On-board diagnostics data, eco-routing, energy-efficient path, physics-guided, shortest path},
	month = {sep},
	number = {3},
	numpages = {28},
	publisher = {Association for Computing Machinery},
	title = {Physics-guided Energy-efficient Path Selection Using On-board Diagnostics Data},
	url = {https://doi.org/10.1145/3406596},
	volume = {1},
	year = {2020},
}

@inproceedings{10.1145/3406971.3406991,
	abstract = {Sampling is a popular approach in big data visualization, however, current sampling approaches don't work well when visualization type is scatter plot, and are even worse in supporting keyword search queries. In this paper, we present an approach of density-aware stratified sampling, it first probing the density of record in different areas of the visualization, then taking the density data to guide the stratified sampling. We conducted an extensively user study to show the efficiency and efficacy of our approach, the experiment shows that our approach can provide very close scatter plots of keyword search queries of a 200 million record dataset within 0.2 second, and the construction time is only 1/4 of an alternative method.},
	address = {New York, NY, USA},
	author = {Dong, Liming and Feng, Bin and Liu, Weidong},
	booktitle = {Proceedings of the 4th International Conference on Graphics and Signal Processing},
	doi = {10.1145/3406971.3406991},
	isbn = {9781450377812},
	keywords = {scatter plot, sampling, keyword search, Big data visualization},
	location = {Nagoya, Japan},
	numpages = {5},
	pages = {107–111},
	publisher = {Association for Computing Machinery},
	series = {ICGSP '20},
	title = {Density-aware Stratified Sampling for Visualizing Large Volume Geo-Spatial Data},
	url = {https://doi.org/10.1145/3406971.3406991},
	year = {2020},
}

@inproceedings{10.1145/3407703.3407706,
	abstract = {With the rapid development of information technologies such as the Internet, the Internet of Things and cloud computing, network big data is widely used in various fields of society, and making good use of network big data is great significance to the sustainable development of cities. Based on the network big data as the data source, this paper analyzes the spatial distribution and spatial-temporal pattern of house prices in the main city zone of Kunming through the Getis-Ord Gi* index and Kriging method, and quantifies the spatial-temporal changes of house prices through statistical analysis and spatial overlay analysis. The result shows that in hot and cold spot analysis, hot spot (high house price agglomeration area) is surrounded by cold spot (low house price agglomeration area), and Chenggong District is the cold spot dominant area. The house price in the main city zone of Kunming forms two high-value areas in the spatial distribution, and presents a pile-shaped ring structure with a central high price and low price in circumference. During the study period, the overall house price fluctuations were small, showing a pattern of north high and south low. The results of statistical analysis and spatial overlay analysis show that there is a significant difference in house prices among various administrative districts. The high-value clusters and low-value clusters in house prices are randomly distributed in various administrative regions. The price changes in Chenggong District are the most significant, and Panlong District has the smallest change.},
	address = {New York, NY, USA},
	author = {Zhao, Zhengxian and Xu, Quanli and Peng, Shuangyun and Hong, Liang},
	booktitle = {Proceedings of the 2020 Artificial Intelligence and Complex Systems Conference},
	doi = {10.1145/3407703.3407706},
	isbn = {9781450377270},
	keywords = {Getis-Ord Gi* Index, House price, Kriging, Network big data, Spatial-temporal patterns},
	location = {Wuhan, China},
	numpages = {6},
	pages = {5–10},
	publisher = {Association for Computing Machinery},
	series = {AICSconf '20},
	title = {Analyzing Spatial-Temporal Patterns of House Price Based on Network Big Data in the Main City Zone of Kunming},
	url = {https://doi.org/10.1145/3407703.3407706},
	year = {2020},
}

@inproceedings{10.1145/3407703.3407729,
	abstract = {To explore temporal and spatial evolution of NDVI and its response relation with Climate in Kunming, Yunnan Province, it is is of great significance to the ecological environmental protection and ecological safety construction. Based on the Landsat data of Kunming City from 2010 to 2018 and the meteorological data of Kunming City during the same period, this paper uses the method of pixel dichotomy, correlation analysis and other methods to analyze the temporal and spatial evolution of vegetation coverage and its response to climate change in the past 9 years. The results show that: (1) From 2010 to 2018, Kunming's average annual NDVI showed an upward trend, with an increase of 0.017/year. In 2018, the area covered by high vegetation was as high as 54\%, and the ecological environment of Kunming City was significantly improved. (2) The change of NDVI in Kunming is insignificantly positively correlated with annual precipitation and insignificantly negatively correlated with annual average temperature, but has a higher correlation with annual average temperature and is more sensitive to temperature factors. (3) The correlation between NDVI change and elevation differentiation is significant within the elevation zone of 619 ~ 2642m in Kunming city, NDVI increases with the increase of elevation.},
	address = {New York, NY, USA},
	author = {Zhong, Xincheng and Xu, Quanli},
	booktitle = {Proceedings of the 2020 Artificial Intelligence and Complex Systems Conference},
	doi = {10.1145/3407703.3407729},
	isbn = {9781450377270},
	keywords = {Kunming, NDVI, Precipitation, elevation, temperature, temporal and spatial evolution},
	location = {Wuhan, China},
	numpages = {6},
	pages = {138–143},
	publisher = {Association for Computing Machinery},
	series = {AICSconf '20},
	title = {Temporal and Spatial Evolution of NDVI and Its Response Analysis with Climate and DEM Based on Landsat image: Taking Kunming as an example in 2010-2018},
	url = {https://doi.org/10.1145/3407703.3407729},
	year = {2020},
}

@inproceedings{10.1145/3407703.3407730,
	abstract = {Using remote sensing software ENVI5.1, combined with Landsat 8 data, the land cover information of the Dianchi Lake Basin is classified and extracted by maximum likelihood classification of supervised classification, ISODATA algorithm of unsupervised classification, and decision tree classification.The classification results and classification accuracy were obtained. Accuracy evaluation and comparative analysis of each classification method.The results show that the overall accuracy of supervised classification in the land cover classification in the study area is 93.90\%, the overall accuracy of unsupervised classification is 85.72\%, and the overall accuracy of decision tree classification is 75.59\%.The supervised classification accuracy is higher than that unsupervised classification and decision tree classification.The categories extracted by supervised classification are continuous and the boundaries are clear, and supervised classification effect is basically consistent with the actual situation. Among them, the accuracy of the producers of forest land, agricultural land, build land, unused land and water area has reached more than 90\%.},
	address = {New York, NY, USA},
	author = {Jin, Lijuan and Xu, Quanli},
	booktitle = {Proceedings of the 2020 Artificial Intelligence and Complex Systems Conference},
	doi = {10.1145/3407703.3407730},
	isbn = {9781450377270},
	keywords = {Dianchi basin, classification accuracy, classification methods, comparative analysis, land cover informations},
	location = {Wuhan, China},
	numpages = {6},
	pages = {144–149},
	publisher = {Association for Computing Machinery},
	series = {AICSconf '20},
	title = {A Comparative Study on Methods of Extracting Land Cover Informations Based on Landsat 8 In Dianchi Basin},
	url = {https://doi.org/10.1145/3407703.3407730},
	year = {2020},
}

@inproceedings{10.1145/3407982.3408007,
	abstract = {Traffic congestion data can be used to provide the necessary information for public transportation optimizations like schedule improvements and fleet size estimations. This data can also be published for general use by the public. It also might be useful for creating governmental and municipal policies regarding infrastructure and the distribution of resources. Having a public decision-making body providing the information can be great for open data communities. In this paper we propose a model for calculating the traffic flow index for a whole city as well as for a road segment, using public transportation vehicle data. We also provide a mechanism for map visualization of the state of the city traffic both historically and in real-time.},
	address = {New York, NY, USA},
	author = {Yosifov, Georgi and Petrov, Milen},
	booktitle = {Proceedings of the 21st International Conference on Computer Systems and Technologies},
	doi = {10.1145/3407982.3408007},
	isbn = {9781450377683},
	keywords = {Global Positioning System, bus, city flow index, public transport, traffic congestion},
	location = {Ruse, Bulgaria},
	numpages = {7},
	pages = {201–207},
	publisher = {Association for Computing Machinery},
	series = {CompSysTech '20},
	title = {Traffic flow city index based on public transportation vehicles data},
	url = {https://doi.org/10.1145/3407982.3408007},
	year = {2020},
}

@inproceedings{10.1145/3408308.3427610,
	abstract = {Developing accurate solar performance models, which infer solar output from widely available external data sources, is increasingly important as the grid's solar capacity rises. These models are important for a wide range of solar analytics, including solar forecasting, resource estimation, and fault detection. The most significant error in existing models is inaccurate estimates of clouds' effect on solar output, as cloud formations and their impact on solar radiation are highly complex. In 2018 and 2019, respectively, the National Oceanic and Atmospheric Administration (NOAA) in the U.S. began releasing multispectral data comprising 16 different light wavelengths (or channels) from the GOES-16 and GOES-17 satellites every 5 minutes. Enough channel data is now available to learn solar performance models using machine learning (ML). In this paper, we show how to develop both local and global solar performance models using ML on multispectral data, and compare their accuracy to existing physical models based on ground-level weather readings and on NOAA's estimates of downward shortwave radiation (DSR), which also derive from multispectral data but using a physical model. We show that ML-based solar performance models based on multispectral data are much more accurate than weather- or DSR-based models, improving the average MAPE across 29 solar sites by over 50\% for local models and 25\% for global models.},
	address = {New York, NY, USA},
	author = {Singh Bansal, Akansha and Irwin, David},
	booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
	doi = {10.1145/3408308.3427610},
	isbn = {9781450380614},
	keywords = {Solar Modeling and Analysis, Satellite Data, Black-Box Models},
	location = {Virtual Event, Japan},
	numpages = {10},
	pages = {1–10},
	publisher = {Association for Computing Machinery},
	series = {BuildSys '20},
	title = {See the Light: Modeling Solar Performance using Multispectral Satellite Data},
	url = {https://doi.org/10.1145/3408308.3427610},
	year = {2020},
}

@inproceedings{10.1145/3408308.3431117,
	abstract = {Auto-Encoder has been widely applied to anomaly detection areas. In this paper, we present a geo-distributed driving maneuver anomaly detection system based on auto-encoder. The auto-encoder is trained by using the normal driving data, so it memorizes the feature of normal driving pattern. The well trained auto-encoder is able to work as a classifier during the detection phase, it will tell whether the input data is normal or abnormal. To further improve the detection accuracy, we divide a city into a set of sub-regions by maximizing the spatial contrast within the same sub-region and minimizing the spatial contrast among different sub-regions. To examine performance of the proposed system, we evaluate it using a large dataset of GPS trajectories. The experiment results show our system achieves high detection accuracy.},
	address = {New York, NY, USA},
	author = {Liu, Miaomiao and Du, Wan},
	booktitle = {Proceedings of the 7th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
	doi = {10.1145/3408308.3431117},
	isbn = {9781450380614},
	keywords = {region partitioning, geo-distributed, anomaly detection},
	location = {Virtual Event, Japan},
	numpages = {2},
	pages = {310–311},
	publisher = {Association for Computing Machinery},
	series = {BuildSys '20},
	title = {Geo-Distributed Driving Maneuver Anomaly Detection},
	url = {https://doi.org/10.1145/3408308.3431117},
	year = {2020},
}

@inproceedings{10.1145/3409334.3452044,
	abstract = {Due to its deep penetration in people's daily life, smartphone has been proposed as a practical platform for indoor localization. Yet one major challenge is how to handle the non-negligible sensor errors that can become problematic when accumulated over time. To this end, a series of approaches such as fingerprint and pedestrian dead reckoning have been proposed, which, however, either need WiFi infrastructure, pre-installed beacons or can only support certain movement patterns or scenarios. In this paper, we take one step further towards tackle this challenge by carefully developing a testbed that can enable deep investigation on the smartphone-based indoor localization problem and the potential for promising practical solution design. In particular, our testbed only accesses the raw inertial measurement unit and orientation data from the smartphone, making it infrastructure-free and require no pre-installation, and providing an in-depth view of sensor errors and their impacts on the localization accuracy. Our testbed also provides built-in functionalities for localization and supports real-time data processing and visualization, which can be extremely valuable for solution development and practical usefulness. We have conducted extensive experiments to evaluate our testbed, and obtained interesting observations that not only validate the effectiveness of our testbed design, but also opens a future direction to develop more advanced mechanisms such as deep learning based approaches to better compensate sensor errors and achieve high accuracy in practice.},
	address = {New York, NY, USA},
	author = {Wang, Yunshu and Easson, Lee and Wang, Feng},
	booktitle = {Proceedings of the 2021 ACM Southeast Conference},
	doi = {10.1145/3409334.3452044},
	isbn = {9781450380683},
	keywords = {testbed development, smartphone, indoor localization, dead-reckoning},
	location = {Virtual Event, USA},
	numpages = {8},
	pages = {79–86},
	publisher = {Association for Computing Machinery},
	series = {ACM SE '21},
	title = {Testbed development for a novel approach towards high accuracy indoor localization with smartphones},
	url = {https://doi.org/10.1145/3409334.3452044},
	year = {2021},
}

@article{10.1145/3410410,
	address = {New York, NY, USA},
	author = {Shekhar, Shashi},
	doi = {10.1145/3410410},
	issn = {0001-0782},
	issue_date = {September 2020},
	journal = {Commun. ACM},
	month = {aug},
	number = {9},
	numpages = {1},
	pages = {93},
	publisher = {Association for Computing Machinery},
	title = {Technical perspective: Progress in spatial computing for flood prediction},
	url = {https://doi.org/10.1145/3410410},
	volume = {63},
	year = {2020},
}

@article{10.1145/3410413,
	abstract = {An important problem in terrain analysis is modeling how water flows across a terrain and creates floods by filling up depressions. In this paper, we study a number of flood-risk related problems: given a terrain Σ, represented as a triangulated xy-monotone surface with n vertices, a rain distribution R, and a volume of rain Ψ, determine which portions of Σ are flooded. We give an overview of efficient algorithms for these problems as well as explore the efficacy and efficiency of these algorithms on real terrains.},
	address = {New York, NY, USA},
	author = {Lowe, Aaron and Agarwal, Pankaj K. and Rav, Mathias},
	doi = {10.1145/3410413},
	issn = {0001-0782},
	issue_date = {September 2020},
	journal = {Commun. ACM},
	month = {aug},
	number = {9},
	numpages = {9},
	pages = {94–102},
	publisher = {Association for Computing Machinery},
	title = {Flood-risk analysis on terrains},
	url = {https://doi.org/10.1145/3410413},
	volume = {63},
	year = {2020},
}

@inproceedings{10.1145/3411170.3411269,
	abstract = {Road traffic injuries are the leading cause of death for people aged 5-29 years old worldwide. They are the third largest cause of death (nearly 38,000 people annually) in the United States and the tenth leading cause of death (approximately 1.35 million people per year) worldwide, which translates to 3,700 deaths on average per day worldwide. Furthermore, 4.4 million Americans are injured seriously enough to require medical attention, which results in $380 million in direct medical costs annually. Yet, given all of that, the various navigation software and GPS products used by motorists on a daily basis do not account for the safety of the routes they suggest and navigate users through. This paper is to report on Safe Routes, a navigation software under research and development at Santa Clara University's Ethical, Pragmatic, and Intelligent Computing (EPIC) Laboratory for the safety scoring of routes in an effort to help reduce the number of road traffic injuries and fatalities. Safe Routes operates on historical accident data as well as real time road and weather conditions for every road segment in all the potential routes between a user designated source and destination in order to calculate the safety rating of each and to rank them for the user with the hope of persuading the user to choose the safest route for navigation.},
	address = {New York, NY, USA},
	author = {Shaghaghi, Navid and Mackey, Aidan and Mistele, Stephen and Rooney, Kevin and Tallis, Nicholas},
	booktitle = {Proceedings of the 6th EAI International Conference on Smart Objects and Technologies for Social Good},
	doi = {10.1145/3411170.3411269},
	isbn = {9781450375597},
	keywords = {Navigation Software, Road Segments, Route Safety Calculation and Ranking, Smart Cities, Smart Transportation},
	location = {Antwerp, Belgium},
	numpages = {4},
	pages = {240–243},
	publisher = {Association for Computing Machinery},
	series = {GoodTechs '20},
	title = {Safe Routes},
	url = {https://doi.org/10.1145/3411170.3411269},
	year = {2020},
}

@inproceedings{10.1145/3412382.3458786,
	abstract = {Human activity in the room causes signal changes on the power-lines. Therefore, the state of a person can be inferred by measuring the signal changes on the powerlines. Previous powerline-based indoor human sensing systems suffer from expensive hardware costs and thus cannot be deployed on a large scale. In this work, we propose a more cost-effective version of the sensing system and achieve sufficiently good results in a real deployment.},
	address = {New York, NY, USA},
	author = {Zhou, Tian and Zhang, Lin},
	booktitle = {Proceedings of the 20th International Conference on Information Processing in Sensor Networks (Co-Located with CPS-IoT Week 2021)},
	doi = {10.1145/3412382.3458786},
	isbn = {9781450380980},
	keywords = {sensor design, powerline system, indoor human sensing},
	location = {Nashville, TN, USA},
	numpages = {2},
	pages = {406–407},
	publisher = {Association for Computing Machinery},
	series = {IPSN '21},
	title = {Detecting Human-Induced Changes in Powerline Signals: A Human Sensing System Design: Poster Abstract},
	url = {https://doi.org/10.1145/3412382.3458786},
	year = {2021},
}

@inproceedings{10.1145/3412841.3441933,
	abstract = {Predictive business process monitoring aims to accurately predict a variable of interest (e.g. remaining time) or the future state of the process instance (e.g. outcome or next step). It is an important topic both from a research and practitioner perspective. For example, existing research suggests that even when problems occur with service provision, providing accurate estimates around process completion time is positively correlated with increasing customer satisfaction. The quest for models with higher predictive power has led to the development of a variety of novel techniques. However, though the location of events is a crucial explanatory variable in many business processes, as yet there have been no studies which have incorporated spatial context into the predictive process monitoring framework. This paper seeks to address this problem by introducing the concept of a spatial event log which records location details at a trace or event level.The predictive utility of spatial contextual features is evaluated vis-\`{a}-vis other contextual features. An approach is proposed to predict the remaining time of an in-flight process instance by calculating the buffer distances between the location of events in a spatial event log to capture spatial proximity and connectedness. These distances are subsequently utilised to construct a regression model which is then used to predict the remaining time for events in the test dataset. The proposed approach is benchmarked against existing approaches using five real-life event logs and demonstrates that spatial features improve the predictive power of business process monitoring models.},
	address = {New York, NY, USA},
	author = {Ogunbiyi, Niyi and Basukoski, Artie and Chaussalet, Thierry},
	booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3412841.3441933},
	isbn = {9781450381048},
	keywords = {spatial context, remaining time predictive modelling, process monitoring, operational business process management, distributed processes},
	location = {Virtual Event, Republic of Korea},
	numpages = {8},
	pages = {535–542},
	publisher = {Association for Computing Machinery},
	series = {SAC '21},
	title = {Incorporating spatial context into remaining-time predictive process monitoring},
	url = {https://doi.org/10.1145/3412841.3441933},
	year = {2021},
}

@inproceedings{10.1145/3412841.3441934,
	abstract = {Region interpolation methods impose restrictions that have an influence on how concavities are transformed, potentially, causing their transformation to be unnatural, e.g., a concavity unexpectedly appears (disappears) from (to) a point. In this work we present an algorithm to transform a line segment to a concavity, and a line segment to a simple non-closed linestring with possibly several concavities. The algorithm is deterministic, it does not assume that an element in the source is transformed directly to an element (or set of elements) in the target, works in stages (steps), i.e., several different transformations can occur while an element in the source is transformed to an element in the target, and its output is a set of moving segments representing the transformation. The complexity of a non-optimized implementation of the transformation of a segment to a concavity using the algorithm is O (kn2), where k is the number of steps in the transformation (the number of intermediate transformations in the transformation) and n is the number of points in the target geometry. The algorithm is primarily devised to be integrated with the region interpolation methods proposed in the spatiotemporal databases literature.},
	address = {New York, NY, USA},
	author = {Duarte, Jos\'{e} and McKenney, Mark},
	booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3412841.3441934},
	isbn = {9781450381048},
	keywords = {spatiotemporal data, moving regions, interpolating polylines, interpolating concavities},
	location = {Virtual Event, Republic of Korea},
	numpages = {8},
	pages = {543–550},
	publisher = {Association for Computing Machinery},
	series = {SAC '21},
	title = {An algorithm to interpolate concavities},
	url = {https://doi.org/10.1145/3412841.3441934},
	year = {2021},
}

@inproceedings{10.1145/3412841.3441935,
	abstract = {Multiple aspect trajectories (MATs) is an emerging concept in the domain of Geographical Information Systems, where the basic view of semantic trajectories is enhanced with the notion of multiple heterogeneous aspects, characterizing different semantic dimensions related to the pure movement data. Many applications benefit from the analysis of multiple aspects trajectories, ranging from the analysis of people trajectories and the extraction of daily habits to the monitoring of vessel trajectories and the detection of outlying behaviors. This work proposes a novel MAT similarity measure as the core component in a hierarchical clustering algorithm. Despite the many clustering methods in the literature and the recent works on MAT similarity, there are still no works that dig deeper into the MAT clustering task. The current article copes with this issue by introducing TraFoS, a new similarity measure that defines a novel method for comparing MATs. TraFos includes a multi-vector representation of MATs that improves their similarity comparison. TraFos allows us to compare MATs across each aspect and then combine similarities in a single measure. We compared TraFos with other state of the art similarity metrics in Agglomerative clustering. The experimental results show that TraFos outperforms other similarities metrics in terms of internal, external clustering metrics and training time.},
	address = {New York, NY, USA},
	author = {Varlamis, Iraklis and Sardianos, Christos and Bogorny, Vania and Alvares, Luis Otavio and Carvalho, J\^{o}nata Tyska and Renso, Chiara and Perego, Raffaele and Violos, John},
	booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
	doi = {10.1145/3412841.3441935},
	isbn = {9781450381048},
	keywords = {trajectory similarity, trajectory clustering, semantic trajectories, multi-aspect trajectories},
	location = {Virtual Event, Republic of Korea},
	numpages = {8},
	pages = {551–558},
	publisher = {Association for Computing Machinery},
	series = {SAC '21},
	title = {A novel similarity measure for multiple aspect trajectory clustering},
	url = {https://doi.org/10.1145/3412841.3441935},
	year = {2021},
}

@inproceedings{10.1145/3415088.3415095,
	abstract = {COVID-19 has spread around the world. In efforts to protect their citizens, countries around the world have implemented various levels of rules and regulations in attempts to stem the spread of the disease. Some countries merely implemented behavioural changes such as asking their citizens to demonstrate social distancing, to use of masks, and to implement good hand hygiene. Other countries implemented severe lockdowns where citizens could not leave their homes. And yet other countries implemented a flexible lockdown system where people were to stay at home except for certain types of activities such as shopping for food, obtaining medical care, and exercising. This paper looks at the situation in South Africa which implemented various levels of lockdown with each level having its own rules and regulations. Lockdown Level 5 was the most severe lockdown in South Africa and, at the time of writing this paper, the country was under a less severe Lockdown Level 3. Although at the time of writing this paper, lockdown levels have not increased in severity, it is a possibility that the country could revert to a more severe lockdown level. In addition, the lockdown levels could vary between the different provinces within the country and the lockdown levels could also vary between different districts within one province.This could create a situation where, for example, two major cities such as Pretoria and Johannesburg which are only approximately seventy kilometers apart and in the same province could, in fact, be at different lockdown levels. This lockdown affects more people than the COVID-19 disease itself. This paper looks at how Geographical Information Systems can be combined with various domain specific data in order to assist in identifying entities which are adversely affected differently by the different lockdown levels. For example, at some lockdown levels, schools were allowed to operate whereas at more strict lockdown levels, schools were not allowed to operate. In such cases, schools would need to know which pupils lived in which districts. At some lockdown levels, alcohol could be sold whereas at more strict lockdown levels, alcohol could not be sold. In such cases, alcohol manufacturers such as breweries and distilleries would need to know how their outlets would be affected and adjust production accordingly.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Butgereit, Laurie},
	booktitle = {Proceedings of the 2nd International Conference on Intelligent and Innovative Computing Applications},
	doi = {10.1145/3415088.3415095},
	isbn = {9781450375580},
	keywords = {COVID-19, geographical information systems},
	location = {Plaine Magnien, Mauritius},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {ICONIC '20},
	title = {A model for using geographical data combined with domain specific data to identify entities adversely affected by COVID-19 lockdowns: a south african perspective},
	url = {https://doi.org/10.1145/3415088.3415095},
	year = {2020},
}

@inproceedings{10.1145/3415958.3433038,
	abstract = {Spatial information retrieval is a common task of digital ecosystems due to the popularity of collecting and storing spatial information and phenomena in the world of the Internet of Things (IoT). Spatial relationships play an important role in this context by specifying how two or more spatial objects are related or connected. Examples of spatial relationships include topological relationships (e.g., intersect, overlap, contains), metric relationships (e.g., nearest neighbors), and direction relationships (e.g., cardinal directions like north and south). Many works in the literature have proposed definitions and implementations of spatial queries based on specific types of spatial relationships. Hence, a holistic view of these works is important to understand their applicability and relations. This paper advances in the literature by providing a comprehensive survey of the implementations and types of spatial queries that can be used by digital ecosystems. We present a novel characterization based on spatial relationships to define topological-based, metric-based, and direction-based spatial queries. For each type of spatial query, we present its intuitive and formal definitions together with possible strategies of implementation. Further, we identify hybrid spatial queries as combinations of two or more spatial relationships, and spatial joins as generalization cases. In addition, we present some equivalences between some types of queries. As a result, we point out future research topics in spatial information retrieval.},
	address = {New York, NY, USA},
	author = {Carniel, Anderson Chaves},
	booktitle = {Proceedings of the 12th International Conference on Management of Digital EcoSystems},
	doi = {10.1145/3415958.3433038},
	isbn = {9781450381154},
	keywords = {Topological relationship, Spatial relationship, Spatial join, Spatial information retrieval, Metric relationship, IoT, Hybrid spatial queries, Direction relationship},
	location = {Virtual Event, United Arab Emirates},
	numpages = {8},
	pages = {10–17},
	publisher = {Association for Computing Machinery},
	series = {MEDES '20},
	title = {Spatial Information Retrieval in Digital Ecosystems: A Comprehensive Survey},
	url = {https://doi.org/10.1145/3415958.3433038},
	year = {2020},
}

@article{10.1145/3416914,
	abstract = {Rapidly developing location acquisition technologies provide a powerful tool for understanding and predicting human mobility in cities, which is very significant for urban planning, traffic regulation, and emergency management. However, with the existing methodologies, it is still difficult to accurately predict millions of peoples’ mobility in a large urban area such as Tokyo, Shanghai, and Hong Kong, especially when collected data used for model training are often limited to a small portion of the total population. Obviously, human activities in city are closely linked with point-of-interest (POI) information, which can reflect the semantic meaning of human mobility. This motivates us to fuse human mobility data and city POI data to improve the prediction performance with limited training data, but current fusion technologies can hardly handle these two heterogeneous data. Therefore, we propose a unique POI-embedding mechanism, that aggregates the regional POIs by categories to generate an artificial POI-image for each urban grid and enriches each trajectory snippet to a four-dimensional tensor in an analogous manner to a short video. Then, we design a deep learning architecture combining CNN with LSTM to simultaneously capture both the spatiotemporal and geographical information from the enriched trajectories. Furthermore, transfer learning is employed to transfer mobility knowledge from one city to another, so that we can fully utilize other cities’ data to train a stronger model for the target city with only limited data available. Finally, we achieve satisfactory performance of human mobility prediction at the citywide level using a limited amount of trajectories as training data, which has been validated over five urban areas of different types and scales.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Jiang, Renhe and Song, Xuan and Fan, Zipei and Xia, Tianqi and Wang, Zhaonan and Chen, Quanjun and Cai, Zekun and Shibasaki, Ryosuke},
	doi = {10.1145/3416914},
	issn = {2691-1922},
	issue_date = {February 2021},
	journal = {ACM/IMS Trans. Data Sci.},
	keywords = {urban computing, transfer learning, human mobility, deep learning, Big data},
	month = {jan},
	number = {1},
	numpages = {26},
	publisher = {Association for Computing Machinery},
	title = {Transfer Urban Human Mobility via POI Embedding over Multiple Cities},
	url = {https://doi.org/10.1145/3416914},
	volume = {2},
	year = {2021},
}

@inproceedings{10.1145/3418094.3418133,
	abstract = {Mosquitoes are responsible for transfer of many vector-borne diseases including Malaria, Zika and Dengue. These amount to 17\% of the total infectious diseases across the globe, leading to a death toll approximately 700,000 annually.Dengue is a preventable viral infection transmitted by Aedes mosquitoes. However, over the past 50 years, the number of dengue cases has increased by a whopping 30-fold. Every year an approximately 500,000 people are admitted with severe dengue, with an estimated 40,000 deaths. In several countries in south American continent and Asia, dengue is one of the leading causes of death. It is mainly found in tropical and sub-tropical regions, particularly surrounding urban and semi-urban areas. Historically, there has been an intensive increase in the number of dengue cases from 2000-2010 and, if adequately explored, essential information can be retrieved.Our work involves the development of the Dengue Spread Information System (DSIS), a geographic-health information system designed to highlight the spread of dengue cases in Iquitos, Peru, and San Juan, Puerto Rico from 1990 to 2013. The application is aimed at citizens, travelers, policymakers and researchers to analyze and interpret the change in risk factors leading to dengue outbreaks and develop essential early warning applications and policies to counter future dengue outbreaks.},
	address = {New York, NY, USA},
	author = {Bhanot, Karan and Schroeder, Dominic and Llewellyn, Isaac and Luczak, Nicholas and Munasinghe, Thilanka},
	booktitle = {Proceedings of the 4th International Conference on Medical and Health Informatics},
	doi = {10.1145/3418094.3418133},
	isbn = {9781450377768},
	keywords = {Information system, dengue, informatics, public health, risk factors, vector-borne diseases, web application},
	location = {Kamakura City, Japan},
	numpages = {10},
	pages = {150–159},
	publisher = {Association for Computing Machinery},
	series = {ICMHI '20},
	title = {Dengue Spread Information System (DSIS)},
	url = {https://doi.org/10.1145/3418094.3418133},
	year = {2020},
}

@inproceedings{10.1145/3423333.3431789,
	abstract = {As a strategic resource, personal location information has played a key role in responding to the impact of COVID-19. In China, while LBS integrating GIS technology and personal information is widely used, it also faces the challenge of how to develop reliable LBS application. This research summarizes the "city-community" big data framework followed by LBS, and combs applying patterns of personal location information at city-level and community-level respectively. On this basis, the problems such as privacy security and data credibility are analyzed. Then according to the problem-oriented principle, the layered structure about GIS and LBS is proposed. This structure is an innovative conceptual model that not only defines location data for GIS-calculations and personal information for LBS applications, but also emphasizes the connection between the two. The results of this paper may provide suggestions for the construction of LBS related systems in response to public health emergencies.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Zhang, Yudong and Liu, Yi},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on Emergency Management Using GIS},
	doi = {10.1145/3423333.3431789},
	isbn = {9781450381598},
	keywords = {public health emergency, privacy security, LBS application, GIS technology, COVID-19},
	location = {Seattle, Washington},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {EM-GIS '20},
	title = {Research on location-based services in COVID-19: integrating GIS technology and personal information},
	url = {https://doi.org/10.1145/3423333.3431789},
	year = {2020},
}

@inproceedings{10.1145/3423333.3431793,
	abstract = {This paper proposed a method to perform smart traffic management for scenic areas with cyber-physical system (CPS) by sharing information among tourists (the mobile phone application), the traffic information platform, and parking lots. Smart traffic management is suggested to be performed based on the interaction of information sharing and physical driving and parking. Taking Wuyuan scenic spots in Jiangxi province as an example, the AnyLogic simulation was used to simulate the cyber-physical system of traffic control and information sharing applying the Markov model. Simulation results show that the information flow can significantly shorten the waiting time of vehicles and improve the utilization rate of scenic areas. The method proposed in this paper may benefit to reduce traffic congestion and perform smart traffic management for scenic areas in busy seasons.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Wang, Shuyi and Zhang, Yudong and Liu, Yi},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on Emergency Management Using GIS},
	doi = {10.1145/3423333.3431793},
	isbn = {9781450381598},
	keywords = {traffic flow, traffic congestion around scenic spots, information flow, CPS, AnyLogic},
	location = {Seattle, Washington},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {EM-GIS '20},
	title = {Smart traffic management with cyber-physical systems for scenic areas},
	url = {https://doi.org/10.1145/3423333.3431793},
	year = {2020},
}

@inproceedings{10.1145/3423334.3431448,
	abstract = {Many Natural Language Processing (NLP) tasks, like question answering or analyzing verbatim comments, have started to use word embeddings due to their ability to capture semantic relations between words. Recently, embeddings have been also applied in the geospatial context to represent geospatial ontologies, thanks to their ability to capture semantic similarity. In this paper, we present an analysis of a promising embedding technique particularly suitable for representing hierarchical structures. We conduct a deep technical evaluation of many parameters and their impact on the quality of the representation.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Dassereto, Federico and Rocco, Laura Di and Shaw, Shanley and Guerrini, Giovanna and Bertolotto, Michela},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks, and Geoadvertising},
	doi = {10.1145/3423334.3431448},
	isbn = {9781450381604},
	keywords = {Knowledge Bases, Geotagging, Geographic Information Retrieval, Embeddings},
	location = {Seattle, WA, USA},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {LocalRec'20},
	title = {How to Tune Parameters in Geographical Ontologies Embedding},
	url = {https://doi.org/10.1145/3423334.3431448},
	year = {2020},
}

@inproceedings{10.1145/3423334.3431451,
	abstract = {Map construction is the problem of reconstructing a travel network based on trajectory data of entities travelling on the network. While many map construction algorithms reconstruct the global structure of a network well, local features such as location of crossings and turns are generally harder to reconstruct correctly, in particular for noisy and irregularly sampled data. We demonstrate how subtrajectory clustering can be used to construct maps that capture both the global structure and local features well.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Buchin, Kevin and Buchin, Maike and Gudmundsson, Joachim and Hendriks, Jorren and Sereshgi, Erfan Hosseini and Sacrist\'{a}n, Vera and Silveira, Rodrigo I. and Sleijster, Jorrick and Staals, Frank and Wenk, Carola},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks, and Geoadvertising},
	doi = {10.1145/3423334.3431451},
	isbn = {9781450381604},
	keywords = {map inference, map construction, clustering, Trajectory data},
	location = {Seattle, WA, USA},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {LocalRec'20},
	title = {Improved Map Construction using Subtrajectory Clustering},
	url = {https://doi.org/10.1145/3423334.3431451},
	year = {2020},
}

@inproceedings{10.1145/3423335.3428169,
	abstract = {Fast-moving wildfires pose difficult modeling challenges, including accounting for heterogeneity in individual evacuee behavior and capturing a complex changing system across time dimensions. Here, we employ a NetLogo agent-based model that enables the development of behavioral models for nearest shelter evacuations using origin information. We use GIS shape files (i.e., road network, building blocks etc.) and the spatiotemporal wildfire dynamics (wind speed, direction and possibility of spread) to support our analysis. Our framework is capable of generating various wildfire scenarios that capture the overall evacuation processes. We can use the simulations to demonstrate the feasibility of agent-based models and to compare them under different fire evacuation scenarios.},
	address = {New York, NY, USA},
	author = {Grajdura, Sarah A. and Borjigin, Sachraa G. and Niemeier, Deb A.},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
	doi = {10.1145/3423335.3428169},
	isbn = {9781450381611},
	keywords = {GIS shapefiles, agent-based simulation, trajectory data, wildfire evacuation},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {56–59},
	publisher = {Association for Computing Machinery},
	series = {GeoSim '20},
	title = {Agent-based wildfire evacuation with spatial simulation: a case study},
	url = {https://doi.org/10.1145/3423335.3428169},
	year = {2020},
}

@inproceedings{10.1145/3423336.3429349,
	abstract = {Remote sensing imagery and sensor data comes in a variety of spatial and spectral characteristics, including very high resolution images from spaceborne and airborne sensors, as well as high, medium, and low resolution sensors from space. In image information mining, the resolution of the imagery affects the characteristics of the features, information, and knowledge that can be extracted or harvested from the imagery pixels. For instance, low resolution imagery has salient visual features on the scale of urban areas, rivers, coastlines, and other geographical land forms. These large-scale features are great for environmental monitoring and understanding large-scale geomorphic processes. However, high resolution imagery allows application tasks such as the extraction of building footprints or counting vehicles in a parking lot. For this reason, different resolutions of imagery and different image processing techniques are often necessary. To develop a truly flexible and robust modeling capability, it is necessary to mine image features and information across a variety of scales and using a variety of image processing and computer vision techniques, each appropriately attuned to the type of information and the spatial-spectral characteristics of the imagery. In this research, we develop a multi-resolution, multi-sensor framework for processing a variety of remote sensing imagery, including MODIS, Landsat, and high-resolution satellite imagery. Deep neural networks are used to extract image features from high-resolution images; while classical image feature extraction techniques are applied to medium and low resolution sensor data. The various features are agglomerated into a geospatial data cube using PostGIS, with the aim of facilitating advanced geospatial modeling of natural and anthropogenic processes. We demonstrate how this multi-resolution data cube of remote sensing visual features facilitates analysis of natural and anthropogenic phenomena, as well as discuss some potential future applications.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Cao, Yulin and Dahu, Butros M. and Scott, Grant J.},
	booktitle = {Proceedings of the 9th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3423336.3429349},
	isbn = {9781450381628},
	keywords = {geospatial visual feature database, image information mining, multi-resolution feature analytics, remote sensing},
	location = {Seattle, Washington},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '20},
	title = {A geographic computational visual feature database for natural and anthropogenic phenomena analysis from multi-resolution remote sensing imagery},
	url = {https://doi.org/10.1145/3423336.3429349},
	year = {2020},
}

@inproceedings{10.1145/3423337.3429433,
	abstract = {Pausanias's second-century CE Periegesis Hellados presents a ten-volume grand tour of the Greek mainland. After the post-enlightenment rediscovery of ancient Greek literature, his Description of Greece proved highly influential as a guidebook to Greece's antiquities, directing travellers and archaeologists alike to uncovering and interpreting major sites, notably at Athens, Corinth and Olympia. Recent studies focusing on his Description as a narrative, however, have drawn attention to the textual construction of space, and the different ways in which space and place are conceptualised and related to each other. This paper outlines the initial work of the Digital Periegesis project, which is using semantic geo-annotation to capture and analyse the forms of space within and the spatial form of this narrative. In particular, it discusses the challenges and affordances of using geo-parsing, spatio-temporal analysis, network analysis, and Linked Open Data (LOD) for rethinking the geographies of a non-modern literary text as based more on topological connections than topographic proximity.},
	address = {New York, NY, USA},
	author = {Foka, Anna and Barker, Elton and Konstantinidou, Kyriaki and Mostofian, Nasrin and Demiroglu, O. Cenk and Kiesling, Brady and Talatas, Linda},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL Workshop on Geospatial Humanities},
	doi = {10.1145/3423337.3429433},
	isbn = {9781450381635},
	keywords = {spatio-temporal analysis, spatial humanities research, semantic technologies, semantic geo-annotation, ontologies and linked open data, network analysis, modeling geohistorical data, literary narrative, gazetteer development, digital tools, digital methods, digital humanities, ancient geography, Pausanias},
	location = {Seattle, WA, USA},
	numpages = {9},
	pages = {1–9},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '20},
	title = {Semantically geo-annotating an ancient Greek "travel guide" Itineraries, Chronotopes, Networks, and Linked Data},
	url = {https://doi.org/10.1145/3423337.3429433},
	year = {2020},
}

@inproceedings{10.1145/3423455.3430303,
	abstract = {Mobility is an indicator of human movement through space and time. With the increasing availability of geolocated data (from GPS, accelerometers, etc.), it is now possible to examine individual as well as group human mobility patterns. Human mobility is influenced by both intrinsic (i.e. personal motivations) and extrinsic (i.e., events like natural hazards or a pandemic like the COVID-19) factors. However, the intricate relationships between human mobility patterns and sociodemographic characteristics in the context of a pandemic are yet to be fully explored. Our goal is to overcome this gap by using human mobility data at the census block group level from mobile phones and combining those with social vulnerability indicators to examine the overall spread of COVID-19 at local spatial scales. We used 585,878 weekly visits to 37,871 points of interests (POIs) from Safegraph to quantify mobility indices and social distancing metrics in 2,820 census block groups in the city of Los Angeles (LA) - before and during lockdown as well as during the phase1 and phase 2 reopening. Finally, using supervised machine learning algorithms, we classified the census block groups in LA into High, Medium and Low categories that represented the vulnerability of these block groups based on the cumulative number of occurrences of COVID-19 cases till July 24, 2020. Our results indicate that the tree-based classifiers performed well in comparison to the Support Vector Machines and Multinomial Logit models. Gradient Boosting had the highest classification accuracy of 97.4\% COVID-19 with an AUC score of 0.987. The block groups with high COVID-19 cases also had a high concentration of socially vulnerable populations, high human mobility index and a low social distancing index.},
	address = {New York, NY, USA},
	author = {Roy, Avipsa and Kar, Bandana},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
	doi = {10.1145/3423455.3430303},
	isbn = {9781450381659},
	keywords = {COVID-19, human mobility, social vulnerability, spatio-temporal analysis, supervised learning},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {39–48},
	publisher = {Association for Computing Machinery},
	series = {ARIC '20},
	title = {Characterizing the spread of COVID-19 from human mobility patterns and SocioDemographic indicators},
	url = {https://doi.org/10.1145/3423455.3430303},
	year = {2020},
}

@inproceedings{10.1145/3423455.3430305,
	abstract = {Agent-based models (ABM) play a prominent role in guiding critical decision-making and supporting the development of effective policies for better urban resilience and response to the COVID-19 pandemic. However, many ABMs lack realistic representations of human mobility, a key process that leads to physical interaction and subsequent spread of disease. Therefore, we propose the application of Latent Dirichlet Allocation (LDA), a topic modeling technique, to foot-traffic data to develop a realistic model of human mobility in an ABM that simulates the spread of COVID-19. In our novel approach, LDA treats POIs as "words" and agent home census block groups (CBGs) as "documents" to extract "topics" of POIs that frequently appear together in CBG visits. These topics allow us to simulate agent mobility based on the LDA topic distribution of their home CBG. We compare the LDA based mobility model with competitor approaches including a naive mobility model that assumes visits to POIs are random. We find that the naive mobility model is unable to facilitate the spread of COVID-19 at all. Using the LDA informed mobility model, we simulate the spread of COVID-19 and test the effect of changes to the number of topics, various parameters, and public health interventions. By examining the simulated number of cases over time, we find that the number of topics does indeed impact disease spread dynamics, but only in terms of the outbreak's timing. Further analysis of simulation results is needed to better understand the impact of topics on simulated COVID-19 spread. This study contributes to strengthening human mobility representations in ABMs of disease spread.},
	address = {New York, NY, USA},
	author = {Pesavento, John and Chen, Andy and Yu, Rayan and Kim, Joon-Seok and Kavak, Hamdi and Anderson, Taylor and Z\"{u}fle, Andreas},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
	doi = {10.1145/3423455.3430305},
	isbn = {9781450381659},
	keywords = {simulation, policy interventions, mobility modeling, latent dirichlet allocation topic modeling, agent-based modeling, COVID-19},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {29–38},
	publisher = {Association for Computing Machinery},
	series = {ARIC '20},
	title = {Data-driven mobility models for COVID-19 simulation},
	url = {https://doi.org/10.1145/3423455.3430305},
	year = {2020},
}

@inproceedings{10.1145/3423455.3430319,
	abstract = {High crime rates have become a public health problem in many important cities, according to World Health Organization. Many researchers have been developing algorithms to predict crime occurrences to tackle this problem. The smart cities' environment can provide us enough ubiquitous data, e.g., traffic flow, human mobility, and Points of Interest (POI) information, to feed those predictive policing algorithms and reflect city dynamics. POIs data provide essential information such as geographical location, category, customer reviews, and busy hours. Recent studies have shown that POI geographical locations are useful for predictive policing. In this paper, we aim at predicting crimes in a delimited region around the POIs of a city with new environmental features. We investigate the relevance of POIs location and the semantic and the temporal features from POIs data in our problem. We also propose and analyze different machine learning approaches to train prediction functions based on these features and conduct experiments on real crime data over multiple years. The experiments demonstrate that the popular time feature is more relevant than the historical information about the number of crimes around a POI, but both information is much less critical than the spatio-temporal information. This work is the first that studies the popular time feature extracted from POIs data and historical criminal information for predictive policing from the authors' knowledge.},
	address = {New York, NY, USA},
	author = {do R\^{e}go, Lu\'{\i}s Gustavo Coutinho and da Silva, Ticiana Linhares Coelho and Magalh\~{a}es, Regis Pires and de Mac\^{e}do, Jos\'{e} Ant\^{o}nio Fernandes and Silva, Wellington Clay Porcino},
	booktitle = {Proceedings of the 3rd ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
	doi = {10.1145/3423455.3430319},
	isbn = {9781450381659},
	keywords = {spatial-temporal systems, predictive policing, point of interest},
	location = {Seattle, Washington},
	numpages = {9},
	pages = {20–28},
	publisher = {Association for Computing Machinery},
	series = {ARIC '20},
	title = {Exploiting points of interest for predictive policing},
	url = {https://doi.org/10.1145/3423455.3430319},
	year = {2020},
}

@inproceedings{10.1145/3423457.3429368,
	abstract = {Given a collection of multi-attribute trajectories, an event definition, and a spatial network, the Significant Lagrangian Linear Hotspot Discovery (SLLHD) problem finds the paths where records in the trajectories tend to be events in the Lagrangian perspective. The SLLHD problem is of significant societal importance because of its applications in transportation planning, vehicle design, and environmental protection. Its main challenges include the potentially large number of candidate hotspots caused by the tremendous volume of trajectories as well as the non-monotonicity of the statistic measuring event concentration. The related work on the linear hotspot discovery problem is designed in the Eulerian perspective and focuses on point datasets, which ignores the dependence of event occurrence on trajectories and the paths where trajectories are. To solve this problem, we introduce an algorithm in the Lagrangian perspective, as well as five refinements that improve its computational scalability. Two case studies on real-world datasets and experiments on synthetic data show that the proposed approach finds hotspots which are not detectable by existing techniques. Cost analysis and experimental results on synthetic data show that the proposed approach yields substantial computational savings.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Li, Yan and Xie, Yiqun and Wang, Pengyue and Shekhar, Shashi and Northrop, William},
	booktitle = {Proceedings of the 13th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3423457.3429368},
	isbn = {9781450381666},
	keywords = {statistical significance, multi-attribute trajectories, linear hotspot, lagrangian, hotspot detection},
	location = {Seattle, Washington},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '20},
	title = {Significant lagrangian linear hotspot discovery},
	url = {https://doi.org/10.1145/3423457.3429368},
	year = {2020},
}

@inproceedings{10.1145/3423457.3429369,
	abstract = {The cost of traffic congestions has been significantly high in many countries. Traffic congestion can be minimized by coordinated route allocation to maximize the traffic efficiency in the whole road network. Unfortunately, the existing traffic management systems cannot achieve this type of optimization as vehicles tend to follow the shortest/fastest routes to their destinations. Such individually optimized routes may cause significant congestions in a road network. In the coming era of connected autonomous vehicles, traffic management systems can have access to a huge volume of prior temporal traffic data that depicts the historical traffic conditions collected at regular time intervals. This type of data provides great opportunities for traffic optimization at the network level. We propose a new route assignment algorithm for the era of connected autonomous vehicles. Our algorithm optimizes traffic based on real-time traffic conditions and prior temporal traffic data. Our experiments show that the proposed algorithm can improve traffic efficiency by up to 10\% over the state-of-the-art algorithm.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Motallebi, Sadegh and Xie, Hairuo and Tanin, Egemen and Ramamohanarao, Kotagiri},
	booktitle = {Proceedings of the 13th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3423457.3429369},
	isbn = {9781450381666},
	keywords = {traffic management systems, temporal traffic data, streaming traffic data, shortest path, route assignment, road networks},
	location = {Seattle, Washington},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '20},
	title = {Streaming route assignment with prior temporal traffic data},
	url = {https://doi.org/10.1145/3423457.3429369},
	year = {2020},
}

@inproceedings{10.1145/3423459.3430755,
	abstract = {Spatial sciences and geography have been integral to the modeling of and communicating information pertaining to the COVID-19 pandemic. Epidemiological models are being used within a geographic context to map the spread of the novel SARS-CoV-2 virus and to make decisions regarding state-wide interventions and allocating hospital resources. Data required for epidemiological models are often incomplete, biased, and available for a spatial unit more extensive than the one needed for decision-making. In this paper, we present results on a global sensitivity analysis of epidemiological model parameters on an important design variable, time to peak number of cases, within a geographic context. We design experiments for quantifying the impact of uncertainty of epidemiological model parameters on distribution of peak times for the state of California. We conduct our analysis at the county-level and perform a non-parametric, global sensitivity analysis to quantify interplay between the uncertainty of epidemiological parameters and design variables.},
	address = {New York, NY, USA},
	author = {Wang, Zhongying and Aydin, Orhun},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Modeling and Understanding the Spread of COVID-19},
	doi = {10.1145/3423459.3430755},
	isbn = {9781450381680},
	keywords = {Uncertainty, Spatial-temporal Analysis, Sensitivity Analysis, Epidemiological Model, COVID-19},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {11–14},
	publisher = {Association for Computing Machinery},
	series = {COVID-19},
	title = {Sensitivity Analysis for COVID-19 Epidemiological Models within a Geographic Framework},
	url = {https://doi.org/10.1145/3423459.3430755},
	year = {2020},
}

@inproceedings{10.1145/3423459.3430761,
	abstract = {CoronaViz (http://coronaviz.umiacs.io) is a research prototype developed by us to enable the dynamic map visualization of COVID-19 related variables including the number of confirmed cases, active cases, recoveries, and deaths all on a daily basis from the Johns Hopkins University web site at ter.ps/coronajhu, by allowing the underlying spatial region and the spanned time interval to vary. Any combination of the variables can be viewed. subject to a possibility of clutter which is avoided by the use of concentric circles (termed geo-circles) whose radius values correspond to the variable values. The variable values are provided both on cumulative and day-by-day bases. The visualization enables spatial and temporal variation.},
	address = {New York, NY, USA},
	author = {Samet, Hanan and Han, Yunheng and Kastner, John and Wei, Hong},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Modeling and Understanding the Spread of COVID-19},
	doi = {10.1145/3423459.3430761},
	isbn = {9781450381680},
	keywords = {Geographic Information Systems, Coronavirus, COVID-19},
	location = {Seattle, WA, USA},
	numpages = {10},
	pages = {53–62},
	publisher = {Association for Computing Machinery},
	series = {COVID-19},
	title = {Using Animation to Visualize Spatio-Temporal Varying COVID-19 Data},
	url = {https://doi.org/10.1145/3423459.3430761},
	year = {2020},
}

@inproceedings{10.1145/3427423.3427432,
	abstract = {Land Surface Temperature (LST) is the temperature found in the outermost layer of the soil surface. Information about LST is very important because LST is a factor that can influence global climate change. There are several ways that can be used to obtain LST data, one of which is to use data obtained from satellites using the help of a satellite image data processing application such as raster calculator within QGIS. There are various plugins provided by QGIS to help its users. Plugins are additional tools designed to deal with various problems encountered. However, there is currently no plugin that automatically calculates the LST algorithm. LST algorithm calculations performed on the QGIS application still use manual methods so to get LST data requires a complex step to produce LST data. To facilitate the process of getting LST data, a plugin is needed that helps users to automatically calculate LST. In this case, the plugin QGIS to calculate the surface temperature is built using the Python (PyQT for designing the UI and PyQGIS the API for QGIS) with the hope that it can simplify and speed up the process of calculating LST data},
	address = {New York, NY, USA},
	author = {Rahmahalim, Muhammad and Ramdani, Fatwa and Rusydi, Alfi Nur},
	booktitle = {Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology},
	doi = {10.1145/3427423.3427432},
	isbn = {9781450376051},
	keywords = {plugin, land surface temperature (LST), QGIS, Indonesia},
	location = {Malang, Indonesia},
	numpages = {7},
	pages = {206–212},
	publisher = {Association for Computing Machinery},
	series = {SIET '20},
	title = {Design and development of land surface temperature calculation plugin of QGIS},
	url = {https://doi.org/10.1145/3427423.3427432},
	year = {2021},
}

@inproceedings{10.1145/3427423.3427448,
	abstract = {The presence of Google Earth (GE) as a digital map has an impact on society. Where the community makes it a reference for the location of the object to navigation to a certain location. But the problem is whether the data presented by GE has good accuracy. In addition, Indonesia has a legal digital map released by the official state agency, the Geospatial Information Agency (BIG). This study compares the extent to which the GE accuracy of the X coordinate, Y coordinate and elevation. GE data is compared with the official BIG map for coordinates and DEMNAS for elevation. Coordinate comparison uses decimal format while DEMNAS is in meters. The comparison results for the X and Y coordinates have a very small error, which is close to 0 (zero) with the RMSE formula. While the high error rate is found in the elevation ratio of 4.5597.},
	address = {New York, NY, USA},
	author = {Supriyanto, Budi Fajar and Ramdani, Fatwa and Supianto, Ahmad Afif},
	booktitle = {Proceedings of the 5th International Conference on Sustainable Information Engineering and Technology},
	doi = {10.1145/3427423.3427448},
	isbn = {9781450376051},
	keywords = {elevation, digital maps, coordinate, accuracy, Google earth},
	location = {Malang, Indonesia},
	numpages = {7},
	pages = {220–226},
	publisher = {Association for Computing Machinery},
	series = {SIET '20},
	title = {Measuring the accuracy of coordinates and elevation of Google earth: how Google earth provide accuracy in location points and elevation},
	url = {https://doi.org/10.1145/3427423.3427448},
	year = {2021},
}

@inproceedings{10.1145/3429789.3429840,
	abstract = {Based on data on the New Student Selection (NSS) at Telkom University in the last three years, there are international study programs that have experienced a decrease in the ratio of the number of participants for the NSS compared to the capacity. A limited number of employees can also be an obstacle in marketing Telkom University to schools on-site in various cities. Based on enrollment data in 2019, there were more than 500 schools from which participants participated in international class study programs. Because there are quite a lot of cities, it would be better if displayed in the form of a map that can display the number of participants who took part in the selection of new students from each city. In this research, a case study was conducted in an international class aimed at designing a geographical information system to map the Telkom University roadshow locations. The system will be built using the Rapid Application Development (RAD) model. This research produces a GIS that maps alternatives that provide information related to registrants from various cities by mapping the data of each alternative city into a map so that users can be helped in considering the cities that need to be visited for the roadshow and also the resources needed for the visit. Based on black-box testing and user acceptance tests, it is concluded that the system is successfully carrying out its functions according to the design.},
	address = {New York, NY, USA},
	articleno = {50},
	author = {Kurniawan, Ramadhani and Kurniawati, Amelia and Rizana, Afrin Fauzya},
	booktitle = {Proceedings of the 2020 International Conference on Engineering and Information Technology for Sustainable Industry},
	doi = {10.1145/3429789.3429840},
	isbn = {9781450387712},
	keywords = {Roadshow Location, Rapid Application Development, Geographic Information System},
	location = {Tangerang, Indonesia},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {ICONETSI '20},
	title = {Mapping of Telkom University Roadshow Location Using Geographic Information System},
	url = {https://doi.org/10.1145/3429789.3429840},
	year = {2020},
}

@article{10.1145/3431785,
	abstract = {The amount of GPS data that can be collected is increasing tremendously, thanks to the increased popularity of Global Position System (GPS) devices (e.g., smartphones). This article aims to develop novel methods of converting crowd-sourced GPS traces into road topology maps. We explore map inference using a three-stage approach, which incorporates a novel Multi-source Variable Rate (MSVR) signal reconstruction mechanism. Unlike conventional map inference methods based on map graph theory, our approach, to the best of our knowledge, is the first use of estimation theory for map inference. In particular, our approach addresses the unique challenges of vehicular GPS data. This data is plentiful but suffers from noise in location and variable coverage of regions. This makes it difficult to differentiate between noise and sparsely covered regions when increasing coverage and reducing noise. Due to the asynchronous, variable sampling rate, and often under-sampled nature of the data, our MSVR approach can better handle inherent GPS errors, reconstruct road shapes more accurately, and better deal with variable GPS data density in empirical environments.We evaluated our method for map inference by comparing to Open Street Map maps as ground truth. We use the F-Measure, Precision, and Recall metrics to evaluate our method on Tsinghua University’s Beijing Taxi Dataset and Shanghai Jiao Tong University’s SUVnet Dataset. On these datasets, we obtained a mean&lt;?brk?&gt; F-Measure, Precision, and Recall of 0.7212, 0.9165, and 0.6021, respectively, outperforming a well-known method based on Kernel Density Estimation in terms of these evaluation metrics.},
	address = {New York, NY, USA},
	articleno = {9},
	author = {He, Eric and Bai, Fan and Hay, Curtis and Chen, Jinzhu and Bhagavatula, Vijayakumar},
	doi = {10.1145/3431785},
	issn = {2374-0353},
	issue_date = {June 2021},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {signal reconstruction, map inference, image processing, GPS, GNSS},
	month = {jan},
	number = {2},
	numpages = {23},
	publisher = {Association for Computing Machinery},
	title = {A Map Inference Approach Using Signal Processing from Crowd-sourced GPS Data},
	url = {https://doi.org/10.1145/3431785},
	volume = {7},
	year = {2021},
}

@inproceedings{10.1145/3443467.3443775,
	abstract = {The landslide disaster caused by the deformation and instability of the high slope of the expressway in mountainous area seriously threatens the safety of the expressway transportation. Therefore, it is of great significance to carry out long-term deformation monitoring on high slope of expressways. Compared with the traditional deformation monitoring technology, such as leveling and GPS, InSAR (Interferometric Synthetic Aperture Radar) technology has the advantage of high precision and weather free, which can obtain more effective surface deformation data. In this paper, taking the high slope at the exit of Luoziqing tunnel of Yuxi-Yuanjiang expressway in Yunnan Province as the research area, we use 45 scenes of Sentinel-1A SAR data in ascending orbit from January 2017 to December 2019 to obtain the surface deformation of the high slope by SBAS (Small baseline subset) technology. The results show that the slope is uplifted as a whole, and the uplift of the middle part of the slope is larger than that of the two sides. The deformation velocity and cumulative deformation is relatively low in winter and spring, while the deformation velocity and cumulative deformation is relatively large in summer and autumn. Different positions of the slope show different deformation characteristics, which shows that the SBAS InSAR technology is a promising method for high slope deformation monitoring in mountainous area.},
	address = {New York, NY, USA},
	author = {Liu, Yong and Zhao, Changyou},
	booktitle = {Proceedings of the 2020 4th International Conference on Electronic Information Technology and Computer Engineering},
	doi = {10.1145/3443467.3443775},
	isbn = {9781450387811},
	keywords = {surface deformation, high slop, Time-series InSAR, Luoziqing tunnel},
	location = {Xiamen, China},
	numpages = {5},
	pages = {318–322},
	publisher = {Association for Computing Machinery},
	series = {EITCE '20},
	title = {Surface deformation of a high slope of Yuxi-Yuanjiang Expressway based on time-series InSAR technology},
	url = {https://doi.org/10.1145/3443467.3443775},
	year = {2021},
}

@inproceedings{10.1145/3444757.3485109,
	abstract = {The adoption and deployment of the Internet of Things (IoT) technologies in agroecology raise a challenging research agenda. Agroecology IoT projects feature complex requirements involving: heterogeneous hardware and heterogeneous software systems; data collection architectures, stream and queuing systems as well as data management systems for real-time and batch processing with different data models. On top of that, agroecology IoT applications are characterized by complex spatio-temporal data and low quality communication networks.Developing conceptual models of such complex systems is mandatory for successful projects, but it is much more challenging than for traditional systems. To the best of our knowledge, a comprehensive (end-to-end) data modeling method applicable to such systems has not been provided yet. It motivated us to propose and assess a new UML profile for data modeling across an IoT ecosystem for agroecology applications. The modeling approach allows to represent the following components of a system: data producers, data integration and storage as well as data analytics. The profile has been validated in a real project on monitoring autonomous agricultural robots.},
	address = {New York, NY, USA},
	author = {Belhassena, Amina and Bimonte, Sandro and Battistoni, Pietro and Cariou, Christophe and Chalhoub, Gerard and Corrales, Juan Carlos and Laneurit, Jean and Moussa, Rim and Plazas, Julian Eduardo and Wrembel, Robert and Sebillo, Monica},
	booktitle = {Proceedings of the 13th International Conference on Management of Digital EcoSystems},
	doi = {10.1145/3444757.3485109},
	isbn = {9781450383141},
	keywords = {UML Profile, Internet of Things, Data Analytics, Conceptual Modelling},
	location = {Virtual Event, Tunisia},
	numpages = {9},
	pages = {120–128},
	publisher = {Association for Computing Machinery},
	series = {MEDES '21},
	title = {On Modeling Data for IoT Agroecology Applications by means of a UML Profile},
	url = {https://doi.org/10.1145/3444757.3485109},
	year = {2021},
}

@article{10.1145/3446936,
	abstract = {Integration of sensor and cloud technologies enable distributed sensing and data collection. We consider a scenario when sensing requests are originated from sensor aware applications that are hosted inside sensor-cloud infrastructures. These requests need to be satisfied using geographically distributed sensors. However, if the sensing resources are mobile, then sensing territory is not limited to a fixed region, rather spatially diverse. In this work, we present a generic scheme for integrating spatio-temporal information of mobile sensors for Internet of Things– (IoT) based environment monitoring system. A set of algorithms are proposed in this work to model spatial and temporal features of mobile resources and exploit resource mobility. We also propose probabilistic models to measure feasibility of a resource to sense a specific spatio-temporal phenomenon. We rank the resources based on their feasibility of satisfying the sensing requests and later use the information for efficient resource allocation and scheduling.},
	address = {New York, NY, USA},
	articleno = {11},
	author = {Bose, Sunanda and Paul, Sumit Kumar and Mukherjee, Nandini},
	doi = {10.1145/3446936},
	issn = {2374-0353},
	issue_date = {September 2021},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {spatial merge, route similarity, sensor cloud, GPS routes},
	month = {jun},
	number = {3},
	numpages = {38},
	publisher = {Association for Computing Machinery},
	title = {Predicting Spatio-Temporal Phenomena of Mobile Resources in Sensor Cloud Infrastructure},
	url = {https://doi.org/10.1145/3446936},
	volume = {7},
	year = {2021},
}

@inproceedings{10.1145/3447548.3467301,
	abstract = {This paper studies weakly supervised learning on spatial raster data based on imperfect vector training labels. Given raster feature imagery and imperfect (weak) vector labels with location registration errors, our goal is to learn a deep learning model for pixel classification and refine vector labels simultaneously. The problem is important in many geoscience applications such as streamline delineation and road mapping from earth imagery, where annotating imperfect coarse vector labels is far more efficient than drawing precise labels. But the problem is challenging due to the misalignment of vector labels with raster feature pixels and the need to infer true vector label location while learning neural network parameters. Existing works on weakly supervised learning often focus on noise and errors in label semantics, assuming label locations to be either correct or irrelevant (e.g., identical and independently distributed). A few works exist on label registration errors, but these methods often focus on label misalignment on object segment boundaries at the pixel level without guaranteeing vector continuity. To fill the gap, this paper proposes a spatial learning framework based on Expectation-Maximization that iteratively updates deep neural network parameters while inferring true vector label locations. Specifically, inference of true vector locations is based on both the current pixel class predictions and the geometric properties of vectors. Evaluations on real-world high-resolution remote sensing datasets in National Hydrography Dataset (NHD) refinement show that the proposed framework outperforms baseline methods in classification accuracy and refined vector quality.},
	address = {New York, NY, USA},
	author = {Jiang, Zhe and He, Wenchong and Kirby, Marcus and Asiri, Sultan and Yan, Da},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3447548.3467301},
	isbn = {9781450383325},
	keywords = {weakly supervised spatial deep learning, remote sensing, registration errors, limited training labels, imperfect vector labels},
	location = {Virtual Event, Singapore},
	numpages = {9},
	pages = {767–775},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {Weakly Supervised Spatial Deep Learning based on Imperfect Vector Labels with Registration Errors},
	url = {https://doi.org/10.1145/3447548.3467301},
	year = {2021},
}

@inproceedings{10.1145/3447548.3467406,
	abstract = {Learning to route has received significant research momentum as anew approach for the route planning problem in intelligent transportation systems. By exploring global knowledge of geographical areas and topological structures of road networks to facilitate route planning, in this work, we propose a novel Generative Adversarial Network (GAN) framework, namely Progressive Route Planning GAN (ProgRPGAN), for route planning in road networks. The novelty of ProgRPGAN lies in the following aspects: 1) we propose to plan a route with levels of increasing map resolution, starting on a low-resolution grid map, gradually refining it on higher-resolution grid maps, and eventually on the road network in order to progressively generate various realistic paths; 2) we propose to transfer parameters of the previous-level generator and discriminator to the subsequent generator and discriminator for parameter initialization in order to improve the efficiency and stability in model learning; and 3) we propose to pre-train embeddings of grid cells in grid maps and intersections in the road network by capturing the network topology and external factors to facilitate effective model learn-ing. Empirical result shows that ProgRPGAN soundly outperforms the state-of-the-art learning to route methods, especially for long routes, by 9.46\% to 13.02\% in F1-measure on multiple large-scale real-world datasets. ProgRPGAN, moreover, effectively generates various realistic routes for the same query.},
	address = {New York, NY, USA},
	author = {Fu, Tao-yang and Lee, Wang-Chien},
	booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \&amp; Data Mining},
	doi = {10.1145/3447548.3467406},
	isbn = {9781450383325},
	keywords = {route planning, neural networks, generative adversarial networks},
	location = {Virtual Event, Singapore},
	numpages = {11},
	pages = {393–403},
	publisher = {Association for Computing Machinery},
	series = {KDD '21},
	title = {ProgRPGAN: Progressive GAN for Route Planning},
	url = {https://doi.org/10.1145/3447548.3467406},
	year = {2021},
}

@inproceedings{10.1145/3447993.3483273,
	abstract = {Analytics with three-dimensional imagery from drones are driving the next generation of remote monitoring applications. Today, there is an unmet need in providing such analytics in an interactive manner, especially over weak Internet connections, to quickly diagnose and solve problems in the commercial industry space of monitoring assets using drones in remote parts of the world. Existing mechanisms either compromise on the quality of insights by not building 3D images and analyze individual 2D images in isolation, or spend tens of minutes building a 3D image before obtaining and uploading insights. We present Visage, a system that accelerates 3D image analytics by identifying smaller parts of the data that can actually benefit from 3D analytics and prioritizing building, and uploading the localized 3D images for those parts. To achieve this, Visage uses a graph to represent raw 2D images and their relative content overlap, and then identifies the various subgraphs using application knowledge that are good candidates for localized 3D image based insights. We evaluate Visage using data from multiple real deployments and show that it can reduce analytics-latency by up to four orders of magnitude.},
	address = {New York, NY, USA},
	author = {Jha, Sagar and Li, Youjie and Noghabi, Shadi and Ranganathan, Vaishnavi and Kumar, Peeyush and Nelson, Andrew and Toelle, Michael and Sinha, Sudipta and Chandra, Ranveer and Badam, Anirudh},
	booktitle = {Proceedings of the 27th Annual International Conference on Mobile Computing and Networking},
	doi = {10.1145/3447993.3483273},
	isbn = {9781450383424},
	keywords = {real-time, geospatial, drones, analytics, 3D imagery},
	location = {New Orleans, Louisiana},
	numpages = {15},
	pages = {789–803},
	publisher = {Association for Computing Machinery},
	series = {MobiCom '21},
	title = {Visage: enabling timely analytics for drone imagery},
	url = {https://doi.org/10.1145/3447993.3483273},
	year = {2021},
}

@inproceedings{10.1145/3448016.3458457,
	abstract = {Array DBMSs manage big N-d arrays, are not yet widely known, but are experiencing an R&amp;D surge due to the rapid growth of array volumes. Cellular automata (CA) operate on a discrete lattice of cells that can be modeled by an N-d array. CA are successfully applied to model fire spread, land cover change, road traffic, and other processes. We made traffic CA simulations possible by array DBMS due to novel components: native UDF language, proactive exec plans, convolution operator, retiling strategy, array versioning, locks, virtual axes, etc. A database approach to CA brings powerful parallelization, data fusion, array processing, and interoperability to name a few. To our best knowledge, our work is the first to run end-to-end CA simulations completely inside array DBMS: we enable array DBMS to simulate the physical world for the first time. Paper homepage: http://sigmod2021.gis.gg/},
	address = {New York, NY, USA},
	author = {Rodriges Zalipynis, Ramon Antonio},
	booktitle = {Proceedings of the 2021 International Conference on Management of Data},
	doi = {10.1145/3448016.3458457},
	isbn = {9781450383431},
	keywords = {urban traffic, physical simulation, cellular automata, array dbms},
	location = {Virtual Event, China},
	numpages = {5},
	pages = {2399–2403},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '21},
	title = {Convergence of Array DBMS and Cellular Automata: A Road Traffic Simulation Case},
	url = {https://doi.org/10.1145/3448016.3458457},
	year = {2021},
}

@inproceedings{10.1145/3448823.3448854,
	abstract = {The management of traffic information is an essential component of Intelligent Transport Systems (ITS). This traffic is no longer confined to roads but is increasingly emerging into the airspace. Recent developments in the field of unmanned aerial systems (UAS) enable new ways of transportation. Furthermore, the sensor systems of these traffic participants, such as cars, drones, and also intelligent infrastructure, are becoming increasingly prominent and powerful. Thus, future ITS will be confronted with the challenge of ever-increasing amounts of data from a wide variety of different traffic participants. Based on this, a novel concept is presented, how data from multi-modal traffic users can be accumulated. To accomplish this, a parametrized geographic data-centric model is presented that can map traffic routes of arbitrary shape in 2- and 3D environments. By this, traffic management of multimodal vehicles (cars, UAS, etc.) can be represented by one unique multimodal model. Furthermore, the parametrized data model allows situational data processing (e.g. congestion, weather condition, etc.) on a local and global basis. The geographic model extends Geo JSON as its foundation in order to rely on well-established standards.},
	address = {New York, NY, USA},
	articleno = {51},
	author = {Raich, Krispin and Kathrein, Robert and Erharter, Michael and D\"{o}ller, Mario},
	booktitle = {Proceedings of the 2020 4th International Conference on Vision, Image and Signal Processing},
	doi = {10.1145/3448823.3448854},
	isbn = {9781450389532},
	keywords = {intelligent transportation systems, drone corridor, air traffic management, UAS paths, 3D corridor},
	location = {Bangkok, Thailand},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {ICVISP 2020},
	title = {Spatial Extension Model for Multimodal Traffic Management},
	url = {https://doi.org/10.1145/3448823.3448854},
	year = {2021},
}

@article{10.1145/3449166,
	abstract = {The rise of geotargeted online advertising has disrupted the business model of local journalism, but it remains ambiguous whether online advertising platforms can effectively reach local audiences. To address this ambiguity, we present a focused study auditing the positional accuracy of geotargeted display advertisements on Google. We measure the frequency and severity of geotargeting errors by targeting display ads to random ZIP codes across the United States, collecting self-reported location information from users who click on the advertisement. We find evidence that geotargeting errors are common, but minor in terms of advertising goals. While 41\% of respondents lived outside the target ZIP code, only 11\% lived outside the target county, and only 2\% lived outside the target state. We also present details regarding a high volume of suspicious clicks in our data, which made the cost per sample extremely expensive. The paper concludes by discussing implications for advertisers, the business of local journalism, and future research.},
	address = {New York, NY, USA},
	articleno = {92},
	author = {Bandy, Jack and Hecht, Brent},
	doi = {10.1145/3449166},
	issue_date = {April 2021},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	keywords = {google, geopositioning, algorithm auditing, advertising},
	month = {apr},
	number = {CSCW1},
	numpages = {19},
	publisher = {Association for Computing Machinery},
	title = {Errors in Geotargeted Display Advertising: Good News for Local Journalism?},
	url = {https://doi.org/10.1145/3449166},
	volume = {5},
	year = {2021},
}

@article{10.1145/3449257,
	abstract = {The presence of people in an urban area throughout the day -- often called 'urban vitality' -- is one of the qualities world-class cities aspire to the most, yet it is one of the hardest to achieve. Back in the 1970s, Jane Jacobs theorized urban vitality and found that there are four conditions required for the promotion of life in cities: diversity of land use, small block sizes, the mix of economic activities, and concentration of people. To build proxies for those four conditions and ultimately test Jane Jacobs's theory at scale, researchers have had to collect both private and public data from a variety of sources, and that took decades. Here we propose the use of one single source of data, which happens to be publicly available: Sentinel-2 satellite imagery. In particular, since the first two conditions (diversity of land use and small block sizes) are visible to the naked eye from satellite imagery, we tested whether we could automatically extract them with a state-of-the-art deep-learning framework and whether, in the end, the extracted features could predict vitality. In six Italian cities for which we had call data records, we found that our framework is able to explain on average 55\% of the variance in urban vitality extracted from those records.},
	address = {New York, NY, USA},
	articleno = {48},
	author = {Scepanovic, Sanja and Joglekar, Sagar and Law, Stephen and Quercia, Daniele},
	doi = {10.1145/3449257},
	issue_date = {April 2021},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	keywords = {urban vitality, sentinel, satellite imagery, computer vision, ai},
	month = {apr},
	number = {CSCW1},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {Jane Jacobs in the Sky: Predicting Urban Vitality with Open Satellite Data},
	url = {https://doi.org/10.1145/3449257},
	volume = {5},
	year = {2021},
}

@article{10.1145/3450945,
	abstract = {With the ubiquity of spatial data, vertexes or edges in graphs can possess spatial location attributes side by side with other non-spatial attributes. For instance, as of June 2018, the Wikidata knowledge graph contains 48,547,142 data items (i.e., vertexes) and 13\% of them have spatial location attributes. The article proposes Riso-Tree, a generic efficient and scalable indexing framework for spatial entities in graph database management systems. Riso-Tree enables the fast execution of graph queries that involve different types of spatial predicates (GraSp queries). The proposed framework augments the classic R-Tree structure with pre-materialized sub-graph entries. The pruning power of R-Tree is enhanced with the sub-graph information. Riso-Tree partitions the graph into sub-graphs based on their connectivity to the spatial sub-regions. The proposed index allows for the fast execution of GraSp queries by efficiently pruning the traversed vertexes/edges based upon the materialized sub-graph information. The experiments show that the proposed Riso-Tree achieves up to two orders of magnitude faster execution time than its counterparts when executing GraSp queries on real graphs (e.g., Wikidata). The strategy of limiting the size of each sub-graph entry (PNmax) is proposed to reduce the storage overhead of Riso-Tree. The strategy can save up to around 70\% storage without harming the query performance according to the experiments. Another strategy is proposed to ensure the performance of the index maintenance (Irrelevant Vertexes Skipping). The experiments show that the strategy can improve performance, especially for slow updates. It proves that Riso-Tree is useful for applications that need to support frequent updates.},
	address = {New York, NY, USA},
	articleno = {12},
	author = {Sun, Yuhan and Sarwat, Mohamed},
	doi = {10.1145/3450945},
	issn = {2374-0353},
	issue_date = {September 2021},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {database index, spatial data, Graph database},
	month = {jun},
	number = {3},
	numpages = {39},
	publisher = {Association for Computing Machinery},
	title = {Riso-Tree: An Efficient and Scalable Index for Spatial Entities in Graph Database Management Systems},
	url = {https://doi.org/10.1145/3450945},
	volume = {7},
	year = {2021},
}

@article{10.1145/3453184,
	abstract = {In the last decade, more and more spatial data has been acquired on a global scale due to satellite missions, social media, and coordinated governmental activities. This observational data suffers from huge storage footprints and makes global analysis challenging. Therefore, many information products have been designed in which observations are turned into global maps showing features such as land cover or land use, often with only a few discrete values and sparse spatial coverage like only within cities. Traditional coding of such data as a raster image becomes challenging due to the sizes of the datasets and spatially non-local access patterns, for example, when labeling social media streams. This article proposes GloBiMap, a randomized data structure, based on Bloom filters, for modeling low-cardinality sparse raster images of excessive sizes in a configurable amount of memory with pure random access operations avoiding costly intermediate decompression. In addition, the data structure is designed to correct the inevitable errors of the randomized layer in order to have a fully exact representation. We show the feasibility of the approach on several real-world datasets including the Global Urban Footprint in which each pixel denotes whether a particular location contains a building at a resolution of roughly 10m globally as well as on a global Twitter sample of more than 220 million precisely geolocated tweets. In addition, we propose the integration of a denoiser engine based on artificial intelligence in order to reduce the amount of error correction information for extremely compressive GloBiMaps.},
	address = {New York, NY, USA},
	articleno = {18},
	author = {Werner, Martin},
	doi = {10.1145/3453184},
	issn = {2374-0353},
	issue_date = {December 2021},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {machine learning, geographic information systems, randomized data structures, data sparsity and compression, Image representation},
	month = {jun},
	number = {4},
	numpages = {24},
	publisher = {Association for Computing Machinery},
	title = {GloBiMapsAI: An AI-Enhanced Probabilistic Data Structure for Global Raster Datasets},
	url = {https://doi.org/10.1145/3453184},
	volume = {7},
	year = {2021},
}

@article{10.1145/3457184,
	abstract = {As people on average only spent 20 seconds(s) observing an artwork, they mostly miss a lot of informative details that are contained within it. As an example, the 75 different plants that can be found in the Ghent Altarpiece is something not a lot of people are aware of. Within this article, we present a methodology, based on cross-collection linking, to create awareness about the botanical imagery in Van Eyck’s masterpiece and to inform people about their region’s plant richness and diversity over time. As such, this article is a nice example of how the interdisciplinary fields of cultural heritage and botany can go hand in hand to facilitate its dissemination to the general public. The plants in the painting can be queried by their name or by a picture taken with a mobile device—a plant recognition app is used to evaluate the pictures taken from the plants. A study has also been performed to evaluate these apps and to select the most appropriate one for the collection of plants in the Ghent Alterpiece. Currently, we link the detected plants to herbaria, observation data, Global Biodiversity Information Facility plantinfo, and recent wikimedia commons pictures, but other links can also be easily integrated with the platform. Finally, we also studied nowadays plant observations (volunteered geographic information) in more detail and reveal which region currently has most of Van Eyck’s plants/flowers.},
	address = {New York, NY, USA},
	articleno = {40},
	author = {Chandrasekar, Krishna Kumar Thirukokaranam and Deman, Emile and Verstockt, Steven},
	doi = {10.1145/3457184},
	issn = {1556-4673},
	issue_date = {July 2021},
	journal = {J. Comput. Cult. Herit.},
	keywords = {visual graphics interface (VGI), geographing, cross collection linking, Plant identification},
	month = {jul},
	number = {3},
	numpages = {14},
	publisher = {Association for Computing Machinery},
	title = {Cross-collection Linking of Botanical Imagery in Ghent Altarpiece to Learn More about Van Eyck’s Masterpiece and to Explore a Region’s Plant Richness and Diversity over Time},
	url = {https://doi.org/10.1145/3457184},
	volume = {14},
	year = {2021},
}

@inproceedings{10.1145/3459637.3482000,
	abstract = {Nowadays, with the rapid development of IoT (Internet of Things) and CPS (Cyber-Physical Systems) technologies, big spatiotemporal data are being generated from mobile phones, car navigation systems, and traffic sensors. By leveraging state-of-the-art deep learning technologies on such data, urban traffic prediction has drawn a lot of attention in AI and Intelligent Transportation System community. The problem can be uniformly modeled with a 3D tensor (T, N, C), where T denotes the total time steps, N denotes the size of the spatial domain (i.e., mesh-grids or graph-nodes), and C denotes the channels of information. According to the specific modeling strategy, the state-of-the-art deep learning models can be divided into three categories: grid-based, graph-based, and multivariate time-series models. In this study, we first synthetically review the deep traffic models as well as the widely used datasets, then build a standard benchmark to comprehensively evaluate their performances with the same settings and metrics. Our study named DL-Traff is implemented with two most popular deep learning frameworks, i.e., TensorFlow and PyTorch, which is already publicly available as two GitHub repositories https://github.com/deepkashiwa20/DL-Traff-Grid and https://github.com/deepkashiwa20/DL-Traff-Graph. With DL-Traff, we hope to deliver a useful resource to researchers who are interested in spatiotemporal data analysis.},
	address = {New York, NY, USA},
	author = {Jiang, Renhe and Yin, Du and Wang, Zhaonan and Wang, Yizhuo and Deng, Jiewen and Liu, Hangchen and Cai, Zekun and Deng, Jinliang and Song, Xuan and Shibasaki, Ryosuke},
	booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
	doi = {10.1145/3459637.3482000},
	isbn = {9781450384469},
	keywords = {ubiquitous and mobile computing, traffic prediction, survey and benchmark, multivariate time-series, deep learning},
	location = {Virtual Event, Queensland, Australia},
	numpages = {11},
	pages = {4515–4525},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {DL-Traff: Survey and Benchmark of Deep Learning Models for Urban Traffic Prediction},
	url = {https://doi.org/10.1145/3459637.3482000},
	year = {2021},
}

@inproceedings{10.1145/3459637.3482004,
	abstract = {OpenStreetMap (OSM) is currently the richest publicly available information source on geographic entities (e.g., buildings and roads) worldwide. However, using OSM entities in machine learning models and other applications is challenging due to the large scale of OSM, the extreme heterogeneity of entity annotations, and a lack of a well-defined ontology to describe entity semantics and properties. This paper presents GeoVectors - a unique, comprehensive world-scale linked open corpus of OSM entity embeddings covering the entire OSM dataset and providing latent representations of over 980 million geographic entities in 180 countries. The GeoVectors corpus captures semantic and geographic dimensions of OSM entities and makes these entities directly accessible to machine learning algorithms and semantic applications. We create a semantic description of the GeoVectors corpus, including identity links to the Wikidata and DBpedia knowledge graphs to supply context information. Furthermore, we provide a SPARQL endpoint - a semantic interface that offers direct access to the semantic and latent representations of geographic entities in OSM.},
	address = {New York, NY, USA},
	author = {Tempelmeier, Nicolas and Gottschalk, Simon and Demidova, Elena},
	booktitle = {Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management},
	doi = {10.1145/3459637.3482004},
	isbn = {9781450384469},
	keywords = {semantic geographic data, openstreetmap, OSM embeddings},
	location = {Virtual Event, Queensland, Australia},
	numpages = {9},
	pages = {4604–4612},
	publisher = {Association for Computing Machinery},
	series = {CIKM '21},
	title = {GeoVectors: A Linked Open Corpus of OpenStreetMap Embeddings on World Scale},
	url = {https://doi.org/10.1145/3459637.3482004},
	year = {2021},
}

@article{10.1145/3460013,
	abstract = {The proliferation of GPS-enabled devices has led to the development of numerous location-based services. These services need to process massive amounts of streamed spatial data in real-time. The current scale of spatial data cannot be handled using centralized systems. This has led to the development of distributed spatial streaming systems. Existing systems are using static spatial partitioning to distribute the workload. In contrast, the real-time streamed spatial data follows non-uniform spatial distributions that are continuously changing over time. Distributed spatial streaming systems need to react to the changes in the distribution of spatial data and queries. This article introduces SWARM, a lightweight adaptivity protocol that continuously monitors the data and query workloads across the distributed processes of the spatial data streaming system and redistributes and rebalances the workloads as soon as performance bottlenecks get detected. SWARM is able to handle multiple query-execution and data-persistence models. A distributed streaming system can directly use SWARM to adaptively rebalance the system’s workload among its machines with minimal changes to the original code of the underlying spatial application. Extensive experimental evaluation using real and synthetic datasets illustrate that, on average, SWARM achieves 2 improvement in throughput over a static grid partitioning that is determined based on observing a limited history of the data and query workloads. Moreover, SWARM reduces execution latency on average 4 compared with the other technique.},
	address = {New York, NY, USA},
	articleno = {14},
	author = {Daghistani, Anas and Aref, Walid G. and Ghafoor, Arif and Mahmood, Ahmed R.},
	doi = {10.1145/3460013},
	issn = {2374-0353},
	issue_date = {September 2021},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {spatial continuous queries, cluster utilization, spatial stream processing, distributed streaming systems, Load balancing},
	month = {jun},
	number = {3},
	numpages = {43},
	publisher = {Association for Computing Machinery},
	title = {SWARM: Adaptive Load Balancing in Distributed Streaming Systems for Big Spatial Data},
	url = {https://doi.org/10.1145/3460013},
	volume = {7},
	year = {2021},
}

@article{10.1145/3460121,
	abstract = {We present a scalable approach for range and k nearest neighbor queries under computationally expensive metrics, like the continuous Fr\'{e}chet distance on trajectory data. Based on clustering for metric indexes, we obtain a dynamic tree structure whose size is linear in the number of trajectories, regardless of the trajectory’s individual sizes or the spatial dimension, which allows one to exploit low “intrinsic dimensionality” of datasets for effective search space pruning. Since the distance computation is expensive, generic metric indexing methods are rendered impractical. We present strategies that (i) improve on known upper and lower bound computations, (ii) build cluster trees without any or very few distance calls, and (iii) search using bounds for metric pruning, interval orderings for reduction, and randomized pivoting for reporting the final results. We analyze the efficiency and effectiveness of our methods with extensive experiments on diverse synthetic and real-world datasets. The results show improvement over state-of-the-art methods for exact queries, and even further speedups are achieved for queries that may return approximate results. Surprisingly, the majority of exact nearest-neighbor queries on real datasets are answered without any distance computations.},
	address = {New York, NY, USA},
	articleno = {15},
	author = {Gudmundsson, Joachim and Horton, Michael and Pfeifer, John and Seybold, Martin P.},
	doi = {10.1145/3460121},
	issn = {2374-0353},
	issue_date = {September 2021},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {range search, nearest neighbor, cover tree, cluster tree, clustering, dynamic metric index, Fr\'{e}chet Distance},
	month = {jun},
	number = {3},
	numpages = {33},
	publisher = {Association for Computing Machinery},
	title = {A Practical Index Structure Supporting Fr\'{e}chet Proximity Queries among Trajectories},
	url = {https://doi.org/10.1145/3460121},
	volume = {7},
	year = {2021},
}

@inproceedings{10.1145/3460426.3463644,
	abstract = {In this paper, we address the problem of global-scale image geolocation, proposing a mixed classification-retrieval scheme. Unlike other methods that strictly tackle the problem as a classification or retrieval task, we combine the two practices in a unified solution leveraging the advantages of each approach with two different modules. The first leverages the EfficientNet architecture to assign images to a specific geographic cell in a robust way. The second introduces a new residual architecture that is trained with contrastive learning to map input images to an embedding space that minimizes the pairwise geodesic distance of same-location images. For the final location estimation, the two modules are combined with a search-within-cell scheme, where the locations of most similar images from the predicted geographic cell are aggregated based on a spatial clustering scheme. Our approach demonstrates very competitive performance on four public datasets, achieving new state-of-the-art performance in fine granularity scales, i.e., 15.0\% at 1km range on Im2GPS3k.},
	address = {New York, NY, USA},
	author = {Kordopatis-Zilos, Giorgos and Galopoulos, Panagiotis and Papadopoulos, Symeon and Kompatsiaris, Ioannis},
	booktitle = {Proceedings of the 2021 International Conference on Multimedia Retrieval},
	doi = {10.1145/3460426.3463644},
	isbn = {9781450384636},
	keywords = {spacial clustering, location estimation, global-scale location estimation, geolocation, contrastive learning},
	location = {Taipei, Taiwan},
	numpages = {9},
	pages = {155–163},
	publisher = {Association for Computing Machinery},
	series = {ICMR '21},
	title = {Leveraging EfficientNet and Contrastive Learning for Accurate Global-scale Location Estimation},
	url = {https://doi.org/10.1145/3460426.3463644},
	year = {2021},
}

@article{10.1145/3465058,
	abstract = {Traffic bottlenecks are a set of road segments that have an unacceptable level of traffic caused by a poor balance between road capacity and traffic volume. A huge volume of trajectory data which captures realtime traffic conditions in road networks provides promising new opportunities to identify the traffic bottlenecks. In this paper, we define this problem as trajectory-driven traffic bottleneck identification: Given a road network R, a trajectory database T, find a representative set of seed edges of size K of traffic bottlenecks that influence the highest number of road segments not in the seed set. We show that this problem is NP-hard and propose a framework to find the traffic bottlenecks as follows. First, a traffic spread model is defined which represents changes in traffic volume for each road segment over time. Then, the traffic diffusion probability between two connected segments and the residual ratio of traffic volume for each segment can be computed using historical trajectory data. We then propose two different algorithmic approaches to solve the problem. The first one is a best-first algorithm BF, with an approximation ratio of 1-1/e. To further accelerate the identification process in larger datasets, we also propose a sampling-based greedy algorithm SG. Finally, comprehensive experiments using three different datasets compare and contrast various solutions, and provide insights into important efficiency and effectiveness trade-offs among the respective methods.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Luo, Hui and Bao, Zhifeng and Cong, Gao and Culpepper, J. Shane and Khoa, Nguyen Lu Dang},
	doi = {10.1145/3465058},
	issn = {2157-6904},
	issue_date = {February 2022},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {road segments influence, traffic bottleneck, Traffic spread},
	month = {nov},
	number = {1},
	numpages = {21},
	publisher = {Association for Computing Machinery},
	title = {Let Trajectories Speak Out the Traffic Bottlenecks},
	url = {https://doi.org/10.1145/3465058},
	volume = {13},
	year = {2021},
}

@article{10.1145/3465060,
	abstract = {Location prediction has attracted much attention due to its important role in many location-based services, including taxi services, route navigation, traffic planning, and location-based advertisements. Traditional methods only use spatial-temporal trajectory data to predict where a user will go next. The divorce of semantic knowledge from the spatial-temporal one inhibits our better understanding of users’ activities. Inspired by the architecture of Long Short Term Memory (LSTM), we design ST-LSTM, which draws on semantic trajectories to predict future locations. Semantic data add a new dimension to our study, increasing the accuracy of prediction. Since semantic trajectories are sparser than the spatial-temporal ones, we propose a strategic filling algorithm to solve this problem. In addition, as the prediction is based on the historical trajectories of users, the cold-start problem arises. We build a new virtual social network for users to resolve the issue. Experiments on two real-world datasets show that the performance of our method is superior to those of the baselines.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Sun, Heli and Guo, Xianglan and Yang, Zhou and Chu, Xuguang and Liu, Xinwang and He, Liang},
	doi = {10.1145/3465060},
	issn = {2157-6904},
	issue_date = {February 2022},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {trajectory pattern mining, cold-start, data sparsity, semantic information, Location prediction},
	month = {jan},
	number = {1},
	numpages = {20},
	publisher = {Association for Computing Machinery},
	title = {Predicting Future Locations with Semantic Trajectories},
	url = {https://doi.org/10.1145/3465060},
	volume = {13},
	year = {2022},
}

@inproceedings{10.1145/3465480.3466931,
	abstract = {According to the American National Institute of Environmental Health Sciences (NIEHS), air pollutants are harmful to the health of humans and other living beings, and cause damage to the climate and to the ecosystem by polluting lakes, streams, and soils. Recent developments in sensor technology, and Internet of Things (IoT) technologies provide an opportunity to use sensor networks to measure air quality, in real time, at a large number of locations. The adoption and deployment of IoT technologies for sensing air quality raises a challenging research agenda related to big data processing, such as, data analysis, scalable architectures, and algorithms for best managing and processing IoT data at different edges in the IoT ecosystem.In response to the DEBS'2021 contest, we design and implement a scalable solution for comparing previous year and current year air quality indexes for German Cities, as well as the calculus of cities' longest streaks of good air quality. Our solution is designed to be scalable. It's based on primo Apache Spark - an open-source unified analytics engine for large-scale data processing, and secundo Apache Sedona for creating spatial indexes, and performing spatial operations over large-scale spatial data.},
	address = {New York, NY, USA},
	author = {Moussa, Rim},
	booktitle = {Proceedings of the 15th ACM International Conference on Distributed and Event-Based Systems},
	doi = {10.1145/3465480.3466931},
	isbn = {9781450385558},
	keywords = {time-series analytic, internet of things, big data, air quality index},
	location = {Virtual Event, Italy},
	numpages = {6},
	pages = {154–159},
	publisher = {Association for Computing Machinery},
	series = {DEBS '21},
	title = {Scalable analytics of air quality batches with Apache Spark and Apache Sedona},
	url = {https://doi.org/10.1145/3465480.3466931},
	year = {2021},
}

@article{10.1145/3467976,
	abstract = {This research explores the possibilities resulting from the use of three-dimensional (3D) models designed in GIS environments for their application to the management and conservation of historical architectonic heritage. This 3D modelling work is one of the strategic actions of the recently finished Master Plan for Conservation of Heritage Municipal Buildings (PD-PHiM) for the City of Seville (Spain). This plan deals with the analysis of a group of 115 municipally owned buildings of high heritage interest that include different typologies, chronologies scales, and uses. This investigation has complemented and continued the initial work begun by the Seville Spatial Data Infrastructure (ide.SEVILLA) in the field of 3D mapping of urban environments and its publication as institutional open data.The implemented improvements started on an initial diagnosis of a preliminary urban model, which reached a level of detail (LOD) of 2, as defined by the CityGML standard, in only 20\% of the registered assets in the PD-PHiM database. The proposed methodology has achieved the automation of most of the process of building 3D geo-referenced models to increase the percentage of assets that reach the LOD2 to 75\%. The initial information comes from the use of institutional spatial data of different types and sources: Light Detection and Ranging (LiDAR), Spanish Cadastre Office, and so on. Additionally, the generated entities have been linked to a complex, multidisciplinary and multiscale database, designed within the framework of the strategic actions of the PD-PHiM.The contributions of the proposal, especially in the automation of processes, imply a considerable saving of resources in comparison with other methods in which the modelling is eminently carried out manually. Thus, they are complementary to those that are related to the use of 3D modelling software intended for other purposes, with the consequent incompatibilities and hard interoperability procedures with GIS environments that this implies.},
	address = {New York, NY, USA},
	articleno = {17},
	author = {Hidalgo-S\'{a}nchez, Francisco M. and Mascort-Albea, Emilio J. and Kada, Martin and Romero-Hern\'{a}ndez, Roc\'{\i}o and Canivell, Jacinto and L\'{o}pez-Larr\'{\i}naga, Francisco},
	doi = {10.1145/3467976},
	issn = {1556-4673},
	issue_date = {February 2022},
	journal = {J. Comput. Cult. Herit.},
	keywords = {LOD2, interactive models, cultural heritage, LiDAR},
	month = {jan},
	number = {1},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {3D GIS Semi-automatized Modelling Procedure for the Conservation of the PHiM: Heritage Municipal Buildings of Seville (Spain). A New Dimension for Urban Cultural Data Management},
	url = {https://doi.org/10.1145/3467976},
	volume = {15},
	year = {2022},
}

@article{10.1145/3469085,
	abstract = {Urban dispersal events occur when an unexpectedly large number of people leave an area in a relatively short period of time. It is beneficial for the city authorities, such as law enforcement and city management, to have an advance knowledge of such events, as it can help them mitigate the safety risks and handle important challenges such as managing traffic, and so forth. Predicting dispersal events is also beneficial to Taxi drivers and/or ride-sharing services, as it will help them respond to an unexpected demand and gain competitive advantage. Large urban datasets such as detailed trip records and point of interest (POI) data make such predictions achievable. The related literature mainly focused on taxi demand prediction. The pattern of the demand was assumed to be repetitive and proposed methods aimed at capturing those patterns. However, dispersal events are, by definition, violations of those patterns and are, understandably, missed by the methods in the literature. We proposed a different approach in our prior work [32]. We showed that dispersal events can be predicted by learning the complex patterns of arrival and other features that precede them in time. We proposed a survival analysis formulation of this problem and proposed a two-stage framework (DILSA), where a deep learning model predicted the survival function at each point in time in the future. We used that prediction to determine the time of the dispersal event in the future, or its non-occurrence. However, DILSA is subject to a few limitations. First, based on evidence from the data, mobility patterns can vary through time at a given location. DILSA does not distinguish between different mobility patterns through time. Second, mobility patterns are also different for different locations. DILSA does not have the capability to directly distinguish between different locations based on their mobility patterns. In this article, we address these limitations by proposing a method to capture the interaction between POIs and mobility patterns and we create vector representations of locations based on their mobility patterns. We call our new method DILSA+. We conduct extensive case studies and experiments on the NYC Yellow taxi dataset from 2014 to 2016. Results show that DILSA+ can predict events in the next 5 hours with an F1-score of 0.66. It is significantly better than DILSA and the state-of-the-art deep learning approaches for taxi demand prediction.},
	address = {New York, NY, USA},
	articleno = {49},
	author = {Khezerlou, Amin Vahedian and Zhou, Xun and Li, Xinyi and Street, W. Nick and Li, Yanhua},
	doi = {10.1145/3469085},
	issn = {2157-6904},
	issue_date = {August 2021},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {dispersal events, survival analysis, deep learning, Data mining},
	month = {aug},
	number = {4},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {DILSA+: Predicting Urban Dispersal Events through Deep Survival Analysis with Enhanced Urban Features},
	url = {https://doi.org/10.1145/3469085},
	volume = {12},
	year = {2021},
}

@article{10.1145/3470649,
	abstract = {Obstacle avoidance is a major challenge during independent mobility for blind or visually impaired (BVI) people. Typically, BVI people can only perceive obstacles at a short distance (about 1&nbsp;m, in case they are using the white cane), and some obstacles are hard to detect (e.g., those elevated from the ground), or should not be hit by the white cane (e.g., a standing person). A solution to these problems can be found in recent computer-vision techniques that can run on mobile and wearable devices to detect obstacles at a distance. However, in addition to detecting obstacles, it is also necessary to convey information about them in real time. This contribution presents WatchOut, a sonification technique for conveying real-time information about the main properties of an obstacle to a BVI person, who can then use this additional feedback to safely navigate in the environment. WatchOut was designed with a user-centered approach, involving four iterations of online listening tests with BVI participants in order to define, improve and evaluate the sonification technique, eventually obtaining an almost perfect recognition accuracy. WatchOut was also implemented and tested as a module of a mobile app that detects obstacles using state-of-the-art computer vision technology. Results show that the system is considered usable and can guide the users to avoid more than 85\% of the obstacles.},
	address = {New York, NY, USA},
	articleno = {19},
	author = {Presti, Giorgio and Ahmetovic, Dragan and Ducci, Mattia and Bernareggi, Cristian and Ludovico, Luca A. and Barat\`{e}, Adriano and Avanzini, Federico and Mascetti, Sergio},
	doi = {10.1145/3470649},
	issn = {1936-7228},
	issue_date = {December 2021},
	journal = {ACM Trans. Access. Comput.},
	keywords = {navigation assistance, orientation \&amp; mobility, Turn-by-turn navigation},
	month = {oct},
	number = {4},
	numpages = {27},
	publisher = {Association for Computing Machinery},
	title = {Iterative Design of Sonification Techniques to Support People with Visual Impairments in Obstacle Avoidance},
	url = {https://doi.org/10.1145/3470649},
	volume = {14},
	year = {2021},
}

@proceedings{10.1145/3474717,
	abstract = {These proceedings contain the papers from the 29th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems (ACM SIGSPATIAL 2021), held online November 2-5, 2021. The conference started as a series of workshops and symposia back in 1993 with the aim of promoting interdisciplinary discussions among researchers, developers, users, and practitioners and fostering research in all aspects of geographic information systems, especially in relation to novel systems based on geospatial data and knowledge. It continues to provide a forum for original research contributions covering all conceptual, design and implementation aspects of geospatial data ranging from applications, user interfaces and visualization, to data storage, query processing, indexing and data mining. The conference is the premier annual event of the ACM Special Interest Group on Spatial Information (ACM SIGSPATIAL).},
	address = {New York, NY, USA},
	isbn = {9781450386647},
	location = {Beijing, China},
	publisher = {Association for Computing Machinery},
	title = {SIGSPATIAL '21: Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483628,
	abstract = {We present OSM Ski Resort Routing, an app that combines the concept of pathfinding and navigation with skiing. It provides an interactive 2D and 3D visualisation of arbitrary ski resorts using OpenStreetMap data and can be used to compute and display the optimal route between any waypoints in the resort.},
	address = {New York, NY, USA},
	author = {Friedsam, Wenzel and Hieber, Robin and Kharitonov, Alexander and Rupp, Tobias},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483628},
	isbn = {9781450386647},
	keywords = {ski resorts, routing, OpenStreetMap, 3D visualisation},
	location = {Beijing, China},
	numpages = {4},
	pages = {11–14},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {OSM Ski Resort Routing},
	url = {https://doi.org/10.1145/3474717.3483628},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483652,
	abstract = {We study the problem faced by an online navigation platform that wishes to improve the total cost experienced by drivers in the road network, i.e., minimize the total travel time of vehicles on the streets. The platform has access to a fixed fraction of the traffic and needs to route it in a manner that improves the overall conditions in the network, both for platform users and non-users. This setting has been studied in the context of designing Stackelberg strategies for selfish routing games. An additional consideration for a routing platform in this setting is that it should ensure a positive experience for its users. The reasons for this are twofold: (a) the platform has a customer-service provider relationship with its users and (b) the users will opt out of following the platform's recommendations if they are systematically poor and the overall routing optimization effort will fail. This aspect is not explicitly addressed in standard Stackelberg algorithms and, in particular, some of them (e.g., Largest Latency First) move in the opposite direction of assigning user traffic to the most expensive paths so that the (selfish) non-user traffic can utilize the better part of the network. To address this challenge we formulate a weighted version of the Stackelberg routing problem in which the delay experienced by non-users of the platform is discounted by some parameter β &lt; 1. We study natural algorithms for this problem and provide provable guarantees in the form of constant approximation ratios for various settings. In simulations with real graphs, data-induced delay functions, and realistic demands, we exhibit that such strategies improve the experience of both platform users and independent traffic in the road network and extract the trade-off between providing high quality service to the platform's users and improving the overall total cost.},
	address = {New York, NY, USA},
	author = {Kollias, Kostas and Chandrashekharapuram, Arun and Fawcett, Lisa and Gollapudi, Sreenivas and Sinop, Ali Kemal},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483652},
	isbn = {9781450386647},
	keywords = {routing, road networks, game theory, algorithms},
	location = {Beijing, China},
	numpages = {12},
	pages = {57–68},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Weighted Stackelberg Algorithms for Road Traffic Optimization},
	url = {https://doi.org/10.1145/3474717.3483652},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483911,
	abstract = {Taking into account the availability of the historical GPS trajectories of drivers, given a new GPS trajectory, Driver mobility fingerprint (DMF) identification aims at (i) determining whether a generated trajectory belongs to a potential driver, and (ii) detecting if a trajectory is likely anomalous based on a driver's historical data. Prior studies often consider hand-crafted feature engineering techniques to extract DMFs while contextual factors like weather and points-of-interest (POIs) are hardly accounted for, which might not achieve satisfactory identification results. To address above, we propose RM-Drive, a novel framework based on reinforced feature extraction and multi-resolution learning. Specifically, we first employ spatio-temporal inverse reinforcement learning (ST-IRL) to extract DMFs from historical trajectories. Then, we generate trajectory embeddings by fusing the extracted DMFs and the contextual factors using the multi-resolution trajectory embedding network (MTE-Net). Our proposed MTE-Net consists of multi-resolution convolutional neural network (MR-CNN), which enables the model to learn the multi-resolution features of the DMFs. Finally, we leverage the trajectory embeddings for the driver classification and anomaly detection. We have conducted extensive evaluation studies upon RM-Drive with two real-world datasets, and our results demonstrate the performance improvements from the state-of-the-art of driver classification and anomaly detection respectively by 21\% and 11\% on average based on several evaluation metrics, including accuracy, precision, and recall, etc.},
	address = {New York, NY, USA},
	author = {Tabatabaie, Mahan and He, Suining and Yang, Xi},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483911},
	isbn = {9781450386647},
	keywords = {Multi-Resolution Feature Learning, Inverse Reinforcement Learning, Driver Mobility Fingerprint, Driver Classification, Anomaly Detection},
	location = {Beijing, China},
	numpages = {12},
	pages = {69–80},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Reinforced Feature Extraction and Multi-Resolution Learning for Driver Mobility Fingerprint Identification},
	url = {https://doi.org/10.1145/3474717.3483911},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483912,
	abstract = {A trajectory, defined as a sequence of location measurements, contains valuable information about movements of an individual. Its value of information (VOI) may change depending on the specific application. However, in a variety of applications, knowing the intrinsic VOI of a trajectory is important to guide other subsequent tasks or decisions. This work aims to find a principled framework to quantify the intrinsic VOI of trajectories from the owner's perspective. This is a challenging problem because an appropriate framework needs to take into account various characteristics of the trajectory, prior knowledge, and different types of trajectory degradation. We propose a framework based on information gain (IG) as a principled approach to solve this problem. Our IG framework transforms a trajectory with discrete-time measurements to a canonical representation, i.e., continuous in time with continuous mean and variance estimates, and then quantifies the reduction of uncertainty about the locations of the owner over a period of time as the VOI of the trajectory. Qualitative and extensive quantitative evaluation show that the IG framework is capable of effectively capturing important characteristics contributing to the VOI of trajectories.},
	address = {New York, NY, USA},
	author = {Nguyen, Kien and Krumm, John and Shahabi, Cyrus},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483912},
	isbn = {9781450386647},
	keywords = {value of information, trajectory, measurement uncertainty, location measurements, location inference, information gain},
	location = {Beijing, China},
	numpages = {10},
	pages = {81–90},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Quantifying Intrinsic Value of Information of Trajectories},
	url = {https://doi.org/10.1145/3474717.3483912},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483913,
	abstract = {We propose a novel automated deep learning framework, namely Automated Spatio-Temporal Dual Graph Convolutional Networks (Auto-STDGCN), for travel time estimation. Specifically, a hierarchical neural architecture search approach is introduced to capture the joint spatio-temporal correlations of intersections and road segments, whose search space is composed of internal and external search space. In the internal search space, spatial graph convolution and temporal convolution operations are adopted to capture the spatio-temporal correlations of the dual graphs. In the external search space, the node-wise and edge-wise graph convolution operations from the internal architecture search are built to capture the interaction patterns between the intersections and road segments. We conduct several experiments on two real-world datasets, and the results demonstrate that Auto-STDGCN is significantly superior to the state-of-art methods.},
	address = {New York, NY, USA},
	author = {Jin, Guangyin and Yan, Huan and Li, Fuxian and Li, Yong and Huang, Jincai},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483913},
	isbn = {9781450386647},
	keywords = {travel time estimation, neural architecture search, graph neural networks},
	location = {Beijing, China},
	numpages = {4},
	pages = {91–94},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Hierarchical Neural Architecture Search for Travel Time Estimation},
	url = {https://doi.org/10.1145/3474717.3483913},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483919,
	abstract = {Last-mile delivery (LMD) is known as the task of delivering items from a warehouse to a customer. In LMD's original version, each item is to be delivered to a single location in a city's network. In this paper, we extend such a scenario by assuming that there can be alternative delivery locations for each item, depending on the time of the day, and aim at maximizing the number of deliveries completed by a courier. We call this new problem the Last Mile Flexible Delivery (LMFD) problem. In fact, at least one e-commerce company, namely Amazon, currently offers the possibility of delivering items to a customer's parked car, thus making the LMFD problem, not only computationally challenging, but also practically relevant. After showing LMFD's NP-hardness, we propose an efficient heuristic approach based on provably correct pruning of the search space, and, using a real and large mobility dataset, we show that such heuristic is both effective and highly efficient.},
	address = {New York, NY, USA},
	author = {Costa, Camila F. and Nascimento, Mario A.},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483919},
	isbn = {9781450386647},
	keywords = {Routing, Road Networks, Last Mile Delivery},
	location = {Beijing, China},
	numpages = {12},
	pages = {121–132},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Last Mile Delivery Considering Time-Dependent Locations},
	url = {https://doi.org/10.1145/3474717.3483919},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483942,
	abstract = {The Brownian bridge is a method for probabilistically interpolating the location of a moving person, animal, or object between two measured points. This type of probabilistic interpolation is useful, because it represents the uncertainty of the interpolated points. It can be used to infer the probability of having visited a certain location, including possible exposure to disease. In the class of probabilistic interpolators, the Brownian bridge is attractive, because it has only a single adjustable parameter, the diffusion coefficient. This paper investigates the suitability of the Brownian bridge for interpolating human locations using mobility data from over 12 million people. One section looks at the consistency of the diffusion coefficient from person to person. As part of this, the paper presents, for the first time, a closed form solution for the maximum likelihood estimate of this parameter. The paper also presents statistical tests aimed at evaluating the accuracy of the Brownian bridge for interpolating human location.},
	address = {New York, NY, USA},
	author = {Krumm, John},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483942},
	isbn = {9781450386647},
	keywords = {probabilistic location, location interpolation, human mobility, brownian bridge},
	location = {Beijing, China},
	numpages = {9},
	pages = {175–183},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Brownian Bridge Interpolation for Human Mobility?},
	url = {https://doi.org/10.1145/3474717.3483942},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483947,
	abstract = {Relief and recovery from disasters (both natural and human-made) require a coordinated approach across several federal and state government agencies. In order to achieve optimal resource allocation and deployment of first responders, accurate and timely assessment of the impact and extent of destruction are the cornerstones to any recovery effort. Ideally, this knowledge should be gathered and shared within the first 0-24 hours (termed as "Acute Phase" by the U.S. CDC guideline) for informed decision-making. But achieving this poses significant challenges for the data collection and data harmonization processes, particularly when voluminous data are being generated from diverse and distributed sources during the disaster responses. To this end, this work developed a scalable and efficient workflow to dynamically collect and harmonize crowd-sourced geographic multi-modal data, and then assess critical infrastructure (CI) damaged during disaster events. We demonstrate the application of our framework with two real-world experiences in addressing post-disaster recovery efforts - for the Bahamas (Natural - due to Hurricane Dorian, 2019) and Beirut (Human-made - due to explosion caused by the ammonium nitrate stored in a warehouse, 2020). We have illustrated that a coordinated effort is needed for planning as well as for execution to achieve informed decision making.},
	address = {New York, NY, USA},
	author = {Thakur, Gautam and Sims, Kelly and Rittmaier, Chantelle and Bentley, Joseph and De, Debraj and Fan, Junchuan and Liu, Tao and Palumbo, Rachel and McGaha, Jesse and Nugent, Phil and Eaton, Bryan and Burdette, Jordan and Sheldon, Tyler and Sparks, Kevin},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483947},
	isbn = {9781450386647},
	keywords = {machine learning, geographic information system, geographic information retrieval, disaster response, data reliability and quality, data curation and management, damage assessment, assessment of critical infrastructure, Spatial data mining and knowledge discovery},
	location = {Beijing, China},
	numpages = {12},
	pages = {195–206},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Accelerated Assessment of Critical Infrastructure in Aiding Recovery Efforts During Natural and Human-made Disaster},
	url = {https://doi.org/10.1145/3474717.3483947},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483948,
	abstract = {Sewer pipe is an essential infrastructure in the city as it undertakes the transportation and circulation of wastewater resources. But sewer pipe it is easy to have faults and cause serious secondary urban accidents, such as road holes and road collapse. Because of the complex underground circumstance, inspecting large-area sewer pipes using closed-circuit television or periscope television is difficult. In this study, we proposed a collaborative sewer pipe inspection approach by using novel low-cost pipe robotic capsules, which capture the images of the pipeline inner walls when floating with the water flow. A set of workers collaboratively drop and salvage capsules to cover a large-area pipe network. The routes of workers and pipe capsules are optimized by a meta-heuristic algorithm integrating local search and simulated annealing. The deep neural network is used to recognize faults from raw captured images. A field experiment in Shenzhen was conducted to evaluate the performance of the proposed approach. The results demonstrate that it outperforms the naive inspection method with a shorter travel distance and less waiting time. It is also effective for inspecting the large-area sewer pipe networks with an overall precision of 0.92. It will help us to eliminate the potential safety risk of the public and promote the level of urban governance.},
	address = {New York, NY, USA},
	author = {Gu, Yu and Tu, Wei and Li, Qingquan and Zhao, Tianhong and Zhao, Dingyi and Zhu, Song and Zhu, Jiasong},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483948},
	isbn = {9781450386647},
	keywords = {urban sewer pipelines inspection, metaheuristic, fault recognition, deep learning, collaborative inspection},
	location = {Beijing, China},
	numpages = {10},
	pages = {211–220},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Collaboratively inspect large-area sewer pipe networks using pipe robotic capsules},
	url = {https://doi.org/10.1145/3474717.3483948},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483949,
	abstract = {Trajectory analysis has been a central problem in applications of location tracking systems. Recently, the (discrete) Fr\'{e}chet distance becomes a popular approach for measuring the similarity of two trajectories because of its high feature extraction capability. Despite its importance, the Fr\'{e}chet distance has several limitations: (i) sensitive to noise as a trade-off for its high feature extraction capability; and (ii) it cannot be incorporated into machine learning frameworks due to its non-smooth functions. To address these problems, we propose the Fr\'{e}chet kernel (FRK), which is associated with a smoothed Fr\'{e}chet distance using a combination of two approximation techniques. FRK can adaptively acquire appropriate extraction capability from trajectories while retaining robustness to noise. Theoretically, we find that FRK has a positive definite property, hence FRK can be incorporated into the kernel method. We also provide an efficient algorithm to calculate FRK. Experimentally, FRK outperforms other methods, including other kernel methods and neural networks, in various noisy real-data classification tasks.},
	address = {New York, NY, USA},
	author = {Takeuchi, Koh and Imaizumi, Masaaki and Kanda, Shunsuke and Tabei, Yasuo and Fujii, Keisuke and Yoda, Ken and Ishihata, Masakazu and Maekawa, Takuya},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483949},
	isbn = {9781450386647},
	keywords = {text tagging, neural networks, gaze detection, datasets},
	location = {Beijing, China},
	numpages = {4},
	pages = {221–224},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Fr\'{e}chet Kernel for Trajectory Data Analysis},
	url = {https://doi.org/10.1145/3474717.3483949},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483955,
	abstract = {Computing the shortest path in a road network is a fundamental problem that has attracted lots of attention. However, in many real-world scenarios, determining solely the shortest path is not enough as users want to have additional, alternative ways of reaching their destination. In this paper, we investigate a novel variant of alternative routing, termed the k-Most Diverse Near-Shortest Paths (kMDNSP). In contrast to previous work, kMDNSP aims at maximizing the diversity of the recommended paths, while bounding their length based on a user-defined constraint. Our theoretical analysis proves the NP-hardness of the problem at hand. To compute an exact solution to kMDNSP, we present an algorithm which iterates over all paths that abide by the length constraint and generates k-subsets of them as candidate results. Furthermore, in order to achieve scalability, we also design three heuristic algorithms that trade the diversity of the result for performance. Our experimental analysis compares all proposed algorithms in terms of their runtime and the quality of the recommended paths.},
	address = {New York, NY, USA},
	author = {H\"{a}cker, Christian and Bouros, Panagiotis and Chondrogiannis, Theodoros and Althaus, Ernst},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483955},
	isbn = {9781450386647},
	keywords = {Route planning, Path similarity, Path diversification, Near-shortest paths, Alternative routing},
	location = {Beijing, China},
	numpages = {11},
	pages = {229–239},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Most Diverse Near-Shortest Paths},
	url = {https://doi.org/10.1145/3474717.3483955},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483961,
	abstract = {Generating alternative routes in road networks is an application of significant interest for online navigation systems. A high quality set of diverse alternate routes offers two functionalities - a) support multiple (unknown) preferences that the user may have; and b) robust to changes in network conditions. We address the latter in this paper. The main techniques that produce alternative routes in road networks are the penalty and the plateau methods, with the former providing high quality results but being too slow for practical use and the latter being fast but suffering in terms of quality. In this work we propose a novel method to produce alternative routes that is fundamentally different from the aforementioned approaches. Our algorithm borrows concepts from electrical flows and their decompositions. We evaluate our method against the penalty and plateau methods, showing that it is as fast as the plateau method while also recovering much of the headroom towards the quality of the penalty method. The metrics we use to evaluate performance include the stretch (the average cost of the routes), the diversity, and the robustness (the connectivity between the origin and destination) of the induced set of routes.},
	address = {New York, NY, USA},
	author = {Sinop, Ali Kemal and Fawcett, Lisa and Gollapudi, Sreenivas and Kollias, Kostas},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483961},
	isbn = {9781450386647},
	keywords = {routing, road networks, electrical flows, alternates, algorithms},
	location = {Beijing, China},
	numpages = {11},
	pages = {282–292},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Robust Routing Using Electrical Flows},
	url = {https://doi.org/10.1145/3474717.3483961},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483963,
	abstract = {Evacuation planning is a critical task in disaster management. Especially in situations such as natural disasters or terrorist attacks, large crowds need to move away from danger and reach designated safe zones. For this purpose, various approaches that efficiently compute evacuation plans in urban areas have been proposed. To evaluate the computed plans, previous works employ heuristics that can only roughly estimate the egress time of each plan. Intuitively, a much better approach is to estimate the egress time via simulation. However, designing a simulation model is usually a time-consuming task and, what is more, this model can only be used to evaluate evacuation plans for a specific area. In this paper, we address these issues presenting EURASIM. Our system enables the automated generation of simulation models for urban areas. Furthermore, EURASIM is designed in a way that algorithms for evacuation planning can be easily integrated, thus functioning as a testbed for the development of even better solutions.},
	address = {New York, NY, USA},
	author = {Chondrogiannis, Theodoros and Bouros, Panagiotis and Emser, Winfried},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483963},
	isbn = {9781450386647},
	keywords = {Simulation, Evacuation Planning, Disaster Management},
	location = {Beijing, China},
	numpages = {4},
	pages = {297–300},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Simulation-based Evacuation Planning for Urban Areas},
	url = {https://doi.org/10.1145/3474717.3483963},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483965,
	abstract = {We propose the combining of region connection calculi with nested hierarchical grids for representing spatial region data in the context of knowledge graphs, thereby avoiding reliance on vector representations. We present a resulting region calculus, and provide qualitative and formal evidence that this representation can be favorable with large data volumes in the context of knowledge graphs; in particular we study means of efficiently choosing which triples to store to minimize space requirements when data is represented this way, and we provide an algorithm for finding the smallest possible set of triples for this purpose including an asymptotic measure of the size of this set for a special case. We prove that a known constraint calculus is adequate for the reconstruction of all triples describing a region from such a pruned representation, but problematic for reasoning with hierarchical grids in general.},
	address = {New York, NY, USA},
	author = {Zalewski, Joseph and Hitzler, Pascal and Janowicz, Krzysztof},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483965},
	isbn = {9781450386647},
	keywords = {knowledge graphs, hierarchical grids, RCC5},
	location = {Beijing, China},
	numpages = {4},
	pages = {305–308},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Semantic Compression with Region Calculi in Nested Hierarchical Grids},
	url = {https://doi.org/10.1145/3474717.3483965},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483973,
	abstract = {Identifying various mentions of the same real-world locations is known as spatial entity matching. GEM is an end-to-end Geospatial EM framework that matches polygon geometry entities in addition to point geometry type. Blocking, feature vector creation, and classification are the core steps of our system. GEM comprises of an efficient and lightweight blocking technique, GeoPrune, that uses the geohash encoding mechanism. We re-purpose the spatial proximality operators from Apache Sedona to create semantically rich spatial feature vectors. The classification step in GEM is a pluggable component, which consumes a unique feature vector and determines whether the geolocations match or not. We conduct experiments with three classifiers upon multiple large-scale geospatial datasets consisting of both spatial and relational attributes. GEM achieves an F-measure of 1.0 for a point x point dataset with 176k total pairs, which is 42\% higher than a state-of-the-art spatial EM baseline. It achieves F-measures of 0.966 and 0.993 for the point x polygon dataset with 302M total pairs, and the polygon x polygon dataset with 16M total pairs respectively.},
	address = {New York, NY, USA},
	author = {Shah, Setu and Meduri, Vamsi and Sarwat, Mohamed},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483973},
	isbn = {9781450386647},
	keywords = {spatial entity matching, spatial blocking, geohash, Apache Sedona},
	location = {Beijing, China},
	numpages = {4},
	pages = {346–349},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {GEM: An Efficient Entity Matching Framework for Geospatial Data},
	url = {https://doi.org/10.1145/3474717.3483973},
	year = {2021},
}

@inproceedings{10.1145/3474717.3483985,
	abstract = {One of the most popular applications of Location Based Services (LBS) is recommending a Point of Interest (POI) based on user's preferences and geo-locations. However, the existing approaches have not tackled the problem of jointly determining: (a) a sequence of POIs that can be traversed within certain budget (i.e., limit on distance) and simultaneously provide a high-enough diversity; and (b) recommend the best origin (i.e., the hotel) for a given user, so that the desired route of POIs can be traversed within the specified constraints. In this work, we take a first step towards identifying this new problem and formalizing it as a novel type of a query. Subsequently, we present na\"{\i}ve solutions and experimental observations over a real-life datasets, illustrating the trade-offs in terms of (dis)associating the initial location from the rest of the POIs.},
	address = {New York, NY, USA},
	author = {Teng, Xu and Trajcevski, Goce and Z\"{u}fle, Andreas},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3483985},
	isbn = {9781450386647},
	keywords = {Range-constraint, POI sequence, Origin, Diversity},
	location = {Beijing, China},
	numpages = {4},
	pages = {375–378},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Semantically Diverse Paths with Range and Origin Constraints},
	url = {https://doi.org/10.1145/3474717.3483985},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484209,
	abstract = {The rapid development of positioning technology produces an extremely large volume of spatio-temporal data with various geometry types such as point, line string, polygon, or a mixed combination of them. As one of the most basic but time-consuming operations, k nearest neighbors join (kNN join) has attracted much attention. However, most existing works for kNN join either ignore temporal information or consider point data only.This paper proposes a novel and useful problem, i.e., ST-kNN join, which considers both spatial closeness and temporal concurrency. To support ST-kNN join over a huge amount of spatio-temporal data with any geometry types efficiently, we propose a novel distributed solution based on Apache Spark. Specifically, our method adopts a two-round join framework. In the first round join, we propose a new spatio-temporal partitioning method that achieves spatio-temporal locality and load balance at the same time. We also propose a lightweight index structure, i.e., Time Range Count Index (TRC-index), to enable efficient ST-kNN join. In the second round join, to reduce the data transmission among different machines, we remove duplicates based on spatio-temporal reference points before shuffling local results. Extensive experiments are conducted using three real big datasets, showing that our method is much more scalable and achieves 9X faster than baselines. A demonstration system is deployed and the source code is released.},
	address = {New York, NY, USA},
	author = {Li, Ruiyuan and Wang, Rubin and Liu, Junwen and Yu, Zisheng and He, Huajun and He, Tianfu and Ruan, Sijie and Bao, Jie and Chen, Chao and Gu, Fuqiang and Hong, Liang and Zheng, Yu},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484209},
	isbn = {9781450386647},
	keywords = {kNN Join, Spatio-Temporal kNN Join, Distributed Computing},
	location = {Beijing, China},
	numpages = {11},
	pages = {435–445},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Distributed Spatio-Temporal k Nearest Neighbors Join},
	url = {https://doi.org/10.1145/3474717.3484209},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484215,
	abstract = {Given a collection of geospatial points of different types, mixture-based best region search aims at discovering spatial regions exhibiting either very high or very low mixture with respect to the types of enclosed points. Existing works detect fixed-shape regions, such as circles or rectangles, thus often missing interesting regions occurring in real-world data that may have arbitrary shapes. In this paper, we formulate the problem of mixture-based best region search for arbitrarily shaped regions, introducing certain desired properties to ensure their cohesiveness and completeness. Since computing exact solutions to this problem has exponential cost with respect to the number of points, we propose anytime algorithms that efficiently search the space of candidate solutions to produce high-scoring regions under any given time budget. Our experiments on several real-world datasets show that our algorithms can produce high-quality results even within tight time constraints.},
	address = {New York, NY, USA},
	author = {Skoutas, Dimitrios and Sacharidis, Dimitris and Patroumpas, Kostas},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484215},
	isbn = {9781450386647},
	keywords = {spatial mixture pattern, spatial analytics, areas of interest},
	location = {Beijing, China},
	numpages = {12},
	pages = {468–479},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Discovering Mixture-Based Best Regions of Arbitrary Shapes},
	url = {https://doi.org/10.1145/3474717.3484215},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484218,
	abstract = {Maps Auto-complete is an essential service complementing the functionality of map search engines. It allows users to formulate their queries faster and also provides better query formatting, which increases the chance of returning a relevant search result.Intuitively, the engagement with the service depends primarily on the quality of the suggestions it recommends. We notice, however, an interesting phenomenon that has not received much attention previously - often Auto-complete correctly identifies the most relevant suggestion, yet users do not click on it right away, if at all. Here we reason over the causes for the phenomenon, provide empirical evidence, and then propose a mitigation based on query expansion. Two models are proposed which generate word or phrase query expansions, allowing users to reach faster a 'mental pause' during which they are more likely to engage with the Auto-complete suggestions. Evaluation of the models is presented.},
	address = {New York, NY, USA},
	author = {Mokhtari, Shekoofeh and Rusnak, Alex and Mutungu, Tsheko and Yankov, Dragomir},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484218},
	isbn = {9781450386647},
	keywords = {query completion, natural language processing},
	location = {Beijing, China},
	numpages = {4},
	pages = {484–487},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Improving Maps Auto-Complete Through Query Expansion (Demo Paper)},
	url = {https://doi.org/10.1145/3474717.3484218},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484219,
	abstract = {The breach of users' location privacy can be catastrophic. To provide users with privacy protections, numerous location privacy methods have been developed in the last two decades. While several studies surveyed existing location privacy methods, the lack of comparative, empirical evaluations imposes challenges for adopting location privacy by applications and researchers who may not be privacy experts. This study fills the gap by conducting a comparative evaluation among a range of location privacy methods with real-world datasets. To evaluate utility, we consider different types of measures, e.g., distortion and mobility metrics; to evaluate privacy protection, we design two empirical privacy risk measures via inference and re-identification attacks. Furthermore, we study the computational overheads inflicted by location privacy in CPU time and memory requirement. The results are thoroughly examined in our work and show that it is possible to strike a balance between utility and privacy when sharing location data with untrusted servers.},
	address = {New York, NY, USA},
	author = {Fan, Liyue and Gote, Ishan},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484219},
	isbn = {9781450386647},
	keywords = {Real-World Data, Location Privacy, Comparative Evaluation},
	location = {Beijing, China},
	numpages = {12},
	pages = {488–499},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {A Closer Look: Evaluating Location Privacy Empirically},
	url = {https://doi.org/10.1145/3474717.3484219},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484220,
	abstract = {A major limiting factor for prediction algorithms is the forecast of new or never before-visited locations. Conventional personal models utterly relying on personal location data perform poorly when it comes to discoveries of new regions. The reason is explained by the prediction relying only on previously visited/seen (or known) locations. As a side effect, locations that were never visited before (or explorations) by a user cause disturbance to known location's prediction. Besides, such explorations cannot be accurately predicted. We claim the tackling of such limitation first requires identifying the purpose of the next probable movement. In this context, we propose a novel framework for adjusting prediction resolution when probable explorations are going to happen. As recently demonstrated [3, 15], there exist regularities in returning and exploring visits. Moreover, the geographical occurrences of explorations are far from being random in a coarser-grained spatial resolution. Exploiting these properties, instead of directly predicting a user's next location, we design a two-step predictive framework. First, we infer an individual's next type of transition: (i) a return, i.e., a visit to a previously known location, or (ii) an exploration, i.e., a discovery of a new place. Next, we predict the next location or the next coarse-grained zone depending on the inferred type of movement. We conduct extensive experiments on three real-world GPS mobility traces. The results demonstrate substantial improvements in the accuracy of prediction by dint of fruitfully forecasting coarse-grained zones used for exploration activities. To the best of our knowledge, we are the first to propose a framework solely based on personal location data to tackle the prediction of visits to new places.},
	address = {New York, NY, USA},
	author = {Amichi, Licia and Viana, Aline Carneiro and Crovella, Mark and Loureiro, Antonio A.F.},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484220},
	isbn = {9781450386647},
	keywords = {Prediction, Human Mobility, Exploration},
	location = {Beijing, China},
	numpages = {12},
	pages = {500–511},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {From movement purpose to perceptive spatial mobility prediction},
	url = {https://doi.org/10.1145/3474717.3484220},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484254,
	abstract = {Deep learning has assisted modern life in various ways. One example is that accurate economic prediction helps people better allocate and distribute their resources. In the U.S., home prices have been accelerating during the COVID-19 pandemic and climbed 13.3\% in March 2021 from the previous year. Real estate market prediction is critical for home buyers and investors to make wise decisions. In some circumstances, accurate predictions on home prices are more important than usual in helping decision-makers to reduce financial mistakes.In this paper, we introduce a large-scale real estate-related dataset for the value prediction task. It consists of numerical real estate price history data from Zillow1 and survey data from Census Bureau public dataset. Our goal is to utilize data from different levels to model the real-estate dynamics with temporal and non-temporal data. We propose to embed sequential temporal features using a transformer and combine them with non-temporal features for subsequent prediction tasks, and evaluate using a different number of classes L ϵ {2, 3, 4, 5}. As an example, when L = 2, we have achieved 93.5\% accuracy with our proposed model, and when L = 3, our proposed model has achieved 90.1\% prediction accuracy. The results suggest that the proposed model overall outperforms all the baseline models.},
	address = {New York, NY, USA},
	author = {Jiang, Chen and Li, Jingjing and Wang, Wenlu and Ku, Wei-Shinn},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484254},
	isbn = {9781450386647},
	keywords = {Transformer, Spatial-temporal Data, Deep Learning},
	location = {Beijing, China},
	numpages = {10},
	pages = {516–525},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Modeling Real Estate Dynamics Using Temporal Encoding},
	url = {https://doi.org/10.1145/3474717.3484254},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484255,
	abstract = {GIS and spatial data science (SDS) tools have been recently approaching each other by establishing bridge technologies between them. R as one of the most prominent programming languages used in SDS projects has been granted access to GIS infrastructure, while R scripts can be integrated and executed in GIS functions. Unfortunately, the treatment of spatial fuzziness has so far not been considered in SDS projects and bridge technologies due to a lack of software packages that can handle fuzzy spatial objects. This paper introduces an R package named fsr as an implementation of the fuzzy spatial data types, operations, and predicates of the Spatial Plateau Algebra that is based on the abstract Fuzzy Spatial Algebra. This R package solves the problem of constructing fuzzy spatial objects as spatial plateau objects from real datasets and describes how to conduct exploratory spatial data analysis by issuing geometric operations and topological predicates on fuzzy spatial objects. Further, fsr provides the possibility of designing fuzzy spatial inference models to discover new findings from fuzzy spatial objects. It optimizes the inference process by deploying the particle swarm optimization to obtain the point locations with the maximum or minimum inferred values that answer a specific user request.},
	address = {New York, NY, USA},
	author = {Carniel, Anderson Chaves and Galdino, Felippe and Philippsen, Juliana Strieder and Schneider, Markus},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484255},
	isbn = {9781450386647},
	keywords = {spatial fuzziness, spatial database, particle swarm optimization, fuzzy spatial inference model, Spatial data science, Spatial Plateau Algebra},
	location = {Beijing, China},
	numpages = {10},
	pages = {526–535},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Handling Fuzzy Spatial Data in R Using the fsr Package},
	url = {https://doi.org/10.1145/3474717.3484255},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484257,
	abstract = {Estimating the market price of a house is important for many businesses such as real estate and mortgage lending companies. The price of a house depends not only on its structural features (e.g. area and number of bedrooms) but also on the spatial context where it is located. In this work we estimate the price of a house based solely on its structural features and the characteristics and price of its neighbors. For that, we propose a hybrid attention mechanism that weights neighbors based on their similarity to the house in terms of structural features and geographic location. For the structural features, we apply an euclidean-based attention and, for the geographic location, we propose an attention layer based on a radial basis function kernel. Those attention mechanisms are then used by a neural network regressor to predict the price of a house and to generate a vector representation of the house based on its implicit context: the house embedding, which can be used as a feature set by any regressor to perform house price prediction. We have performed an extensive experimental evaluation on real-world datasets that shows that: (1) regressors using house embedding obtained the best results on all 4 datasets, outperforming baseline models; (2) the learned house embedding improves the performance of the evaluated regressors in almost all scenarios in comparison to raw features; and (3) simple regressor models such as Linear Regression using house embedding achieved comparable results to more competitive algorithms (e.g. Random Forest and Xgboost).},
	address = {New York, NY, USA},
	author = {Viana, Darniton and Barbosa, Luciano},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484257},
	isbn = {9781450386647},
	keywords = {Spatial Interpolation, House Price Prediction, Attention Mechanism},
	location = {Beijing, China},
	numpages = {10},
	pages = {540–549},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Attention-Based Spatial Interpolation for House Price Prediction},
	url = {https://doi.org/10.1145/3474717.3484257},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484261,
	abstract = {A common first step in the terrain processing pipeline of large Triangulated Irregular Networks (TINs) is simplifying the TIN to make it manageable for further processing. The major problem with TIN simplification algorithms is that they create or remove critical points in an uncontrolled way. Topology-aware operators have been defined to solve this issue by coarsening a TIN without affecting the topology of its underlying terrain, i.e., without modifying critical simplices describing pits, saddles, peaks, and their connectivity. While effective, existing algorithms are sequential in nature and are not scalable enough to perform well with large terrains on multicore systems. Here, we consider the problem of topology-aware simplification of very large meshes. We define a topology-aware simplification algorithm on a compact and distributed data structure for triangle meshes, namely the Terrain trees. Terrain trees reduce both the memory and time requirements of the simplification procedure by adopting a batched processing strategy of the mesh elements. Furthermore, we define a new parallel topology-aware simplification algorithm that takes advantage of the spatial domain decomposition at the basis of Terrain trees. Scalability and efficiency are experimentally demonstrated on real-world TINs originated from topographic and bathymetric LiDAR data. Our experiments show that topology-aware simplification on Terrain trees uses 40\% less memory and half the time than the same approach implemented on the most compact and efficient connectivity-based data structure for TINs. Beyond that, our parallel algorithm on the Terrain trees reaches a 12x speedup when using 20 threads.},
	address = {New York, NY, USA},
	author = {Song, Yunting and Fellegara, Riccardo and Iuricich, Federico and De Floriani, Leila},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484261},
	isbn = {9781450386647},
	keywords = {topological methods, terrain simplification, spatial indexes, shared memory processing, edge contraction},
	location = {Beijing, China},
	numpages = {12},
	pages = {576–587},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Efficient topology-aware simplification of large triangulated terrains},
	url = {https://doi.org/10.1145/3474717.3484261},
	year = {2021},
}

@inproceedings{10.1145/3474717.3484264,
	abstract = {We study the problem of sub-trajectory nearest-neighbor queries on polygonal curves under the continuous Fr\'{e}chet distance. Given a trajectory P with n vertices and a query trajectory Q, we seek to report a vertex-aligned sub-trajectory P' of P that is closest to Q, i.e. P' must start and end on contiguous vertices of P.Since in real data P typically contains a very large number of vertices, we focus on answering queries exactly, without restrictions on P or Q, using only pre-computed structures of O(n) size.We use three baseline algorithms from straightforward extensions of known work, however they have impractical performance on realistic inputs. Therefore, we propose a new Hierarchical Simplification Tree data structure and an adaptive clustering based query algorithm that efficiently explores relevant parts of P. Experiments on real and synthetic data show that our heuristic effectively prunes the search space and greatly reduces computations compared to baseline approaches.},
	address = {New York, NY, USA},
	author = {Gudmundsson, Joachim and Seybold, Martin P. and Pfeifer, John},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3484264},
	isbn = {9781450386647},
	keywords = {Sub-Trajectory, Nearest-Neighbor, Metric Pruning, Hierarchical Simplification Tree, Greedy Decision Algorithm, Fr\'{e}chet Distance},
	location = {Beijing, China},
	numpages = {10},
	pages = {596–605},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {On Practical Nearest Sub-Trajectory Queries under the Fr\'{e}chet Distance},
	url = {https://doi.org/10.1145/3474717.3484264},
	year = {2021},
}

@inproceedings{10.1145/3474717.3486796,
	abstract = {Recent generations of GPUs have seen the introduction of hardware-accelerated ray tracing algorithms that are suitable for real-time use. They provide hardware for massively parallel ray-geometry intersection computations, indicating a highly optimized spatial data structure derived from arbitrary triangle-based geometries. Spatial join is an ubiquitous problem in spatial databases, GIS applications, spatial statistics, and similar applications. On a fundamental level, spatial joins are based on point in polygon tests (PIP). We suggest exploiting the capabilities of ray tracing hardware to perform fast parallel point in polygon tests in order to implement hardware-accelerated spatial joins.},
	address = {New York, NY, USA},
	author = {Laass, Moritz},
	booktitle = {Proceedings of the 29th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3474717.3486796},
	isbn = {9781450386647},
	keywords = {spatial join, ray-tracing, point in polygon test, RTX, GPU},
	location = {Beijing, China},
	numpages = {2},
	pages = {666–667},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '21},
	title = {Point in Polygon Tests Using Hardware Accelerated Ray Tracing},
	url = {https://doi.org/10.1145/3474717.3486796},
	year = {2021},
}

@inproceedings{10.1145/3477314.3506964,
	abstract = {Efficient processing of RDF data is a basic requirement for querying RDF knowledge graphs, which are today a centerpiece in the semantic processing of information on the Web. In this paper, we investigate the issue of efficient processing of RDF spatial data. Our aim is to extend the query evaluation strategy that relies on graph exploration to support spatial processing. We propose several methods to achieve this goal. We shown the drawbacks and advantages of each method via a few sample queries. Our first results show that the proposal that combines graph exploration and spatial indexing can give very good and promising results compared to existing approaches.},
	address = {New York, NY, USA},
	author = {Yousfi, Houssameddine},
	booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
	doi = {10.1145/3477314.3506964},
	isbn = {9781450387132},
	keywords = {spatial data, query optimisation, data management, RDF, GeoSPAQL},
	location = {Virtual Event},
	numpages = {4},
	pages = {389–392},
	publisher = {Association for Computing Machinery},
	series = {SAC '22},
	title = {Spatial data processing meets RDF graph exploration: student research abstract},
	url = {https://doi.org/10.1145/3477314.3506964},
	year = {2022},
}

@inproceedings{10.1145/3477314.3507057,
	abstract = {In this article, we introduce a novel platform dedicated to the extraction, transformation and visualization of mobility data. This platform was developed in the framework of a French regional project (DA3T project) aiming at improving the management and valorisation of touristic cities via the fine-grained analysis of tourist mobility data. The system is totally modular, and non-computer scientists (such as geographers) can make processing pipelines from a variety of modules belonging to different categories (e.g., extraction, filtering, visualization, etc.). Each pipeline is created in order to help fulfil one or several reporting needs. Indeed, the results of those pipelines aim to be presented to local authorities and decision makers to assist them in improving infrastructure and tourism management. Beyond this main use case, the platform is also generic and aims to work with any kind of mobility data (not strictly limited to the tourism field). It is heavily inspired by traditional ETL (Extract, Transform, Load) software and processes.},
	address = {New York, NY, USA},
	author = {Masson, Maxime and Cay\`{e}r\'{e}, C\'{e}cile and Bessagnet, Marie-No\"{e}lle and Sallaberry, Christian and Roose, Philippe and Faucher, Cyril},
	booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
	doi = {10.1145/3477314.3507057},
	isbn = {9781450387132},
	keywords = {semantic trajectories, mobility tracks, geographic information, data integration, ETL},
	location = {Virtual Event},
	numpages = {9},
	pages = {547–555},
	publisher = {Association for Computing Machinery},
	series = {SAC '22},
	title = {An ETL-like platform for the processing of mobility data},
	url = {https://doi.org/10.1145/3477314.3507057},
	year = {2022},
}

@inproceedings{10.1145/3477314.3507070,
	abstract = {Social network analysis has widespread in recent years, especially in digital tourism. Indeed the large amount of data that tourists produce during their travels represents an effective source to understand their behavior and is of great importance for tourism stakeholders. This paper studies the propagation effect of tourists on the territory thanks to geotagged circulation graphs. Those graphs reflect traffic flows which need to be analyzed over time and space. A new weighted measure is introduced for circulation characterization based on both topologies and distances. This measure helps to determine the behavior of tourists on local and global areas. An optimization strategy based on spanning trees is applied to reduce the computation on the whole graph while keeping a good approximation of the behavior. The approach is simulated on various graphs and evaluated experimentaly over a real dataset at various geographic zones, scales, communities, and time.},
	address = {New York, NY, USA},
	author = {Prevoteau, Hugo and Djebali, Sonia and Laiping, Zhao and Travers, Nicolas},
	booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
	doi = {10.1145/3477314.3507070},
	isbn = {9781450387132},
	keywords = {spanning trees, graph data mining, digital tourism},
	location = {Virtual Event},
	numpages = {8},
	pages = {556–563},
	publisher = {Association for Computing Machinery},
	series = {SAC '22},
	title = {Propagation measure on circulation graphs for tourism behavior analysis},
	url = {https://doi.org/10.1145/3477314.3507070},
	year = {2022},
}

@article{10.1145/3479007,
	abstract = {This research explores technologically advanced means to enhance audiences’ connection with cultural heritage assets through participatory creative methods that particularly reinforce young people’s sense of identity and well-being during sensitive “transitional” periods of their lives. Hence, the research investigates how communities can meaningfully connect with cultural heritage through creative experiences, while aiming at lowering the entry barriers to increasing audiences’ participation. For this, the research deploys narrative approaches to illuminate different viewpoints and interpretations of cultural heritage within communities. The contribution of the article is twofold, as it includes a novel approach for developing and re-telling communities’ narratives linked to people, objects, sites, and events in the urban landscape. At the same time, it proposes a workflow to digitise and communicate these narratives through Augmented Reality (AR) Maps by proposing methods for digitisation and producing physical printed elements for the experience. These physical elements are then augmented with digital narratives, delivered through Immersive Web technology. This concept is proposed as a means to document and disseminate the narratives in a way that enhances the public understanding and appreciation of objects and sites. The approach was tested with a class of children in a local primary school in Brighton and Hove (UK) to understand its suitability for community engagement, targeting young audiences. The significance of the research is that it demonstrates the potential of the synergy between creative and digital approaches for enabling meaningful engagement with the cultural heritage while improving the well-being of the participants as well as their sense of community and place.},
	address = {New York, NY, USA},
	articleno = {33},
	author = {Echavarria, Karina Rodriguez and Samaroudi, Myrsini and Dibble, Laurie and Silverton, Edward and Dixon, Sophie},
	doi = {10.1145/3479007},
	issn = {1556-4673},
	issue_date = {June 2022},
	journal = {J. Comput. Cult. Herit.},
	keywords = {Identity and Place, Digital narratives, Culture Heritage, Computing for Wellbeing},
	month = {apr},
	number = {2},
	numpages = {19},
	publisher = {Association for Computing Machinery},
	title = {Creative Experiences for Engaging Communities with Cultural Heritage through Place-based Narratives},
	url = {https://doi.org/10.1145/3479007},
	volume = {15},
	year = {2022},
}

@article{10.1145/3480970,
	abstract = {In recent years, deep learning has achieved tremendous success in image segmentation for computer vision applications. The performance of these models heavily relies on the availability of large-scale high-quality training labels (e.g., PASCAL VOC 2012). Unfortunately, such large-scale high-quality training data are often unavailable in many real-world spatial or spatiotemporal problems in earth science and remote sensing (e.g., mapping the nationwide river streams for water resource management). Although extensive efforts have been made to reduce the reliance on labeled data (e.g., semi-supervised or unsupervised learning, few-shot learning), the complex nature of geographic data such as spatial heterogeneity still requires sufficient training labels when transferring a pre-trained model from one region to another. On the other hand, it is often much easier to collect lower-quality training labels with imperfect alignment with earth imagery pixels (e.g., through interpreting coarse imagery by non-expert volunteers). However, directly training a deep neural network on imperfect labels with geometric annotation errors could significantly impact model performance. Existing research that overcomes imperfect training labels either focuses on errors in label class semantics or characterizes label location errors at the pixel level. These methods do not fully incorporate the geometric properties of label location errors in the vector representation. To fill the gap, this article proposes a weakly supervised learning framework to simultaneously update deep learning model parameters and infer hidden true vector label locations. Specifically, we model label location errors in the vector representation to partially reserve geometric properties (e.g., spatial contiguity within line segments). Evaluations on real-world datasets in the National Hydrography Dataset (NHD) refinement application illustrate that the proposed framework outperforms baseline methods in classification accuracy.},
	address = {New York, NY, USA},
	articleno = {25},
	author = {Jiang, Zhe and He, Wenchong and Kirby, Marcus Stephen and Sainju, Arpan Man and Wang, Shaowen and Stanislawski, Lawrence V. and Shavers, Ethan J. and Usery, E. Lynn},
	doi = {10.1145/3480970},
	issn = {2157-6904},
	issue_date = {April 2022},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {weakly supervised learning, imperfect labels, earth imagery segmentation, Deep learning},
	month = {jan},
	number = {2},
	numpages = {20},
	publisher = {Association for Computing Machinery},
	title = {Weakly Supervised Spatial Deep Learning for Earth Image Segmentation Based on Imperfect Polyline Labels},
	url = {https://doi.org/10.1145/3480970},
	volume = {13},
	year = {2022},
}

@article{10.1145/3481043,
	abstract = {Given earth imagery with spectral features on a terrain surface, this paper studies surface segmentation based on both explanatory features and surface topology. The problem is important in many spatial and spatiotemporal applications such as flood extent mapping in hydrology. The problem is uniquely challenging for several reasons: first, the size of earth imagery on a terrain surface is often much larger than the input of popular deep convolutional neural networks; second, there exists topological structure dependency between pixel classes on the surface, and such dependency can follow an unknown and non-linear distribution; third, there are often limited training labels. Existing methods for earth imagery segmentation often divide the imagery into patches and consider the elevation as an additional feature channel. These methods do not fully incorporate the spatial topological structural constraint within and across surface patches and thus often show poor results, especially when training labels are limited. Existing methods on semi-supervised and unsupervised learning for earth imagery often focus on learning representation without explicitly incorporating surface topology. In contrast, we propose a novel framework that explicitly models the topological skeleton of a terrain surface with a contour tree from computational topology, which is guided by the physical constraint (e.g., water flow direction on terrains). Our framework consists of two neural networks: a convolutional neural network (CNN) to learn spatial contextual features on a 2D image grid, and a graph neural network (GNN) to learn the statistical distribution of physics-guided spatial topological dependency on the contour tree. The two models are co-trained via variational EM. Evaluations on the real-world flood mapping datasets show that the proposed models outperform baseline methods in classification accuracy, especially when training labels are limited.},
	address = {New York, NY, USA},
	articleno = {26},
	author = {He, Wenchong and Sainju, Arpan Man and Jiang, Zhe and Yan, Da and Zhou, Yang},
	doi = {10.1145/3481043},
	issn = {2157-6904},
	issue_date = {April 2022},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {semi-supervised, physic-guided, contour tree, terrain surface, Earth image segmentation},
	month = {jan},
	number = {2},
	numpages = {22},
	publisher = {Association for Computing Machinery},
	title = {Earth Imagery Segmentation on Terrain Surface with Limited Training Labels: A Semi-supervised Approach based on Physics-Guided Graph Co-Training},
	url = {https://doi.org/10.1145/3481043},
	volume = {13},
	year = {2022},
}

@article{10.1145/3481617,
	abstract = {Estimating human mobility responses to the large-scale spreading of the COVID-19 pandemic is crucial, since its significance guides policymakers to give Non-pharmaceutical Interventions, such as closure or reopening of businesses. It is challenging to model due to complex social contexts and limited training data. Recently, we proposed a conditional generative adversarial network (COVID-GAN) to estimate human mobility response under a set of social and policy conditions integrated from multiple data sources. Although COVID-GAN achieves a good average estimation accuracy under real-world conditions, it produces higher errors in certain regions due to the presence of spatial heterogeneity and outliers. To address these issues, in this article, we extend our prior work by introducing a new spatio-temporal deep generative model, namely, COVID-GAN+. COVID-GAN+ deals with the spatial heterogeneity issue by introducing a new spatial feature layer that utilizes the local Moran statistic to model the spatial heterogeneity strength in the data. In addition, we redesign the training objective to learn the estimated mobility changes from historical average levels to mitigate the effects of spatial outliers. We perform comprehensive evaluations using urban mobility data derived from cell phone records and census data. Results show that COVID-GAN+ can better approximate real-world human mobility responses than prior methods, including COVID-GAN.},
	address = {New York, NY, USA},
	articleno = {27},
	author = {Bao, Han and Zhou, Xun and Xie, Yiqun and Zhang, Yingxue and Li, Yanhua},
	doi = {10.1145/3481617},
	issn = {2157-6904},
	issue_date = {April 2022},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {COVID-19, conditional generative adversarial networks, Mobility estimation},
	month = {jan},
	number = {2},
	numpages = {23},
	publisher = {Association for Computing Machinery},
	title = {COVID-GAN+: Estimating Human Mobility Responses to COVID-19 through Spatio-temporal Generative Adversarial Networks with Enhanced Features},
	url = {https://doi.org/10.1145/3481617},
	volume = {13},
	year = {2022},
}

@article{10.1145/3485594,
	address = {New York, NY, USA},
	author = {Van Hentenryck, Pascal},
	doi = {10.1145/3485594},
	issn = {0001-0782},
	issue_date = {November 2021},
	journal = {Commun. ACM},
	month = {oct},
	number = {11},
	numpages = {1},
	pages = {120},
	publisher = {Association for Computing Machinery},
	title = {Technical perspective: Finding the sweet spot amid accuracy and performance},
	url = {https://doi.org/10.1145/3485594},
	volume = {64},
	year = {2021},
}

@article{10.1145/3485626,
	abstract = {In this paper, we describe multi-itinerary optimization (MIO)---a novel Bing Maps service that automates the process of building itineraries for multiple agents while optimizing their routes to minimize travel time or distance. MIO can be used by organizations with a fleet of vehicles and drivers, mobile salesforce, or a team of personnel in the field, to maximize workforce efficiency. It supports a variety of constraints, such as service time windows, duration, priority, pickup and delivery dependencies, and vehicle capacity. MIO also considers traffic conditions between locations, resulting in algorithmic challenges at multiple levels (e.g., calculating time-dependent travel-time distance matrices at scale and scheduling services for multiple agents).To support an end-to-end cloud service with turnaround times of a few seconds, our algorithm design targets a sweet spot between accuracy and performance. Toward that end, we build a scalable approach based on the ALNS metaheuristic. Our experiments show that accounting for traffic significantly improves solution quality: MIO finds efficient routes that avoid late arrivals, whereas traffic-agnostic approaches result in a 15\% increase in the combined travel time and the lateness of an arrival. Furthermore, our approach generates itineraries with substantially higher quality than a cutting-edge heuristic (LKH), with faster running times for large instances.},
	address = {New York, NY, USA},
	author = {Cristian, Alexandru and Marshall, Luke and Negrea, Mihai and Stoichescu, Flavius and Cao, Peiwei and Menache, Ishai},
	doi = {10.1145/3485626},
	issn = {0001-0782},
	issue_date = {November 2021},
	journal = {Commun. ACM},
	month = {oct},
	number = {11},
	numpages = {9},
	pages = {121–129},
	publisher = {Association for Computing Machinery},
	title = {Multi-itinerary optimization as cloud service},
	url = {https://doi.org/10.1145/3485626},
	volume = {64},
	year = {2021},
}

@inproceedings{10.1145/3486183.3490997,
	abstract = {Foot traffic is a business term to describe the number of customers that enter a point of interest (POI). This work aims to predict future foot traffic: the number of people from each census block group (CBG) that will visit each POI of a study region with potential applications in marketing and advertising. Existing techniques for spatiotemporal prediction of foot traffic use location-based social network data that suffer from sparsity, capturing only a handful of visits per day. This study utilizes highly granular foot traffic data from SafeGraph, a data company that collects mobility data regarding hundreds of millions of visits per day in the United States alone. Using this data, we explore solutions to predict weekly foot traffic data at the POI level. We propose a collaborative filtering approach using tensor factorization on the (POIs x CBGs x Weeks) data tensor. This approach provides us with a de-noised estimation of visits in previous weeks for all POI-CBG pairs. Using this tensor, we explore various time series prediction models: weekly rolling average, weighted weekly rolling average, univariate linear regression, polynomial regression, and long short-term memory (LSTM) recurrent neural networks. Our results show that of all the prediction models, the collaborative filtering step consistently improves prediction results. We also found that a simple weighted average consistently performed better than the more sophisticated approaches. Given this abundance of foot traffic data, this result shows that we can improve the spatiotemporal prediction of foot traffic data by harnessing collaborative filtering.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Islam, Samiul and Gandhi, Dhruv and Elarde, Justin and Anderson, Taylor and Roess, Amira and Leslie, Timothy F. and Kavak, Hamdi and Z\"{u}fle, Andreas},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3486183.3490997},
	isbn = {9781450391009},
	keywords = {visitor prediction, spatiotemporal prediction, next POI prediction, location prediction, human mobility, foot traffic},
	location = {Beijing, China},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '21},
	title = {Spatiotemporal prediction of foot traffic},
	url = {https://doi.org/10.1145/3486183.3490997},
	year = {2021},
}

@inproceedings{10.1145/3486183.3491000,
	abstract = {The rapid increase of GPS-enabled devices has led to immense amounts of trajectory data being collected and analyzed. To provide insight into these datasets, a number of spatio-temporal queries need to be executed efficiently and at scale. One such important query is the Query by Path, which given a series of road segments and a time interval, retrieves all trajectories that have passed through the road segments within a given time interval. The Query by Path finds application in many areas, including traffic management, transportation planning and fleet monitoring.In this paper we develop an approach to partition and distribute trajectories across a cluster and execute queries by path at scale. At the center of our approach is the partitioning of the entire dataset and indexing each partition with a Trie. We develop a basic set of partitioning approaches and show that each can be rendered inefficient by skew in the dataset. We consequently propose a HYbrid PartitiOning algorithm (HYPO) that performs robustly in face of skew. We also provide the cost models to configure HYPO. Finally we assess its performance extensively using both real and synthetic datasets to demonstrate that it scales well in face of skew.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Evagorou, Giannis and Ghosh, Abhirup and Heinis, Thomas},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3486183.3491000},
	isbn = {9781450391009},
	keywords = {trajectory indexing, spatio-temporal indexing, skewed trajectory distributions, path queries, network-constrained indexing},
	location = {Beijing, China},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '21},
	title = {HYPO: skew-resilient partitioning for trajectory datasets},
	url = {https://doi.org/10.1145/3486183.3491000},
	year = {2021},
}

@inproceedings{10.1145/3486183.3491066,
	abstract = {This paper reviews several approaches to the problem of toponym resolution for news articles referring to 'Portland.' We train several models to differentiate between Portland, Maine and Portland, Oregon, generating features using only the text of the articles. The data used is in the form of articles pulled from NewsStand. The labels, which are provided by NewsStand's interpretation of the articles, allow for a supervised learning approach. We apply Natural Language Processing (NLP) and data cleaning techniques to process the article data, perform feature reduction, and then feed the data to the models. We show that the logistic regression model performs the best of the four models that we test. We also demonstrate that this model learns a more robust representation of the two classes than the other three models do.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Schneider, Nicole R. and Samet, Hanan},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3486183.3491066},
	isbn = {9781450391009},
	keywords = {toponym resolution, spatio-textual, machine learning, geotagging, experimental},
	location = {Beijing, China},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '21},
	title = {Which portland is it? a machine learning approach},
	url = {https://doi.org/10.1145/3486183.3491066},
	year = {2021},
}

@inproceedings{10.1145/3486187.3490204,
	abstract = {This talk discusses the development of a spatiotemporal data model for an urban gazetteer. The function of gazetteers is to obtain descriptions uniquely identifying places referred to in discourse. Often, they are lists of places containing place name, feature type and geographical extent. Contemporary digital gazetteers (e.g. World Historical Gazetteer and Pleiades) are valuable tools for geographical knowledge of the past and the structuring of humanities data. However, scholars and GLAM (Galleries, Libraries, Archives and Museums) specialists often require information about entities on an intra-city scale. This presentation explores the model and implementation of an urban gazetteer using CIDOC CRM as a top-level ontology. The model will closely follow international gazetteer standards (i.e. Linked Places Format) in order to ensure interoperability with other gazetteer datasets. To move towards a FAIR (Findable, Accessible, Interoperable, and Reusable) approach, humanities data from the urban gazetteer will be published as Linked Open Data (LOD) and searchable via (Geo)SPARQL.},
	address = {New York, NY, USA},
	author = {Ducatteeuw, Vincent},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3486187.3490204},
	isbn = {9781450391023},
	keywords = {urban history, urban gazetteer, spatiotemporal analysis, spatial humanities, spatial history, semantic technologies, modelling geohistorical data, linked open data, gazetteer development, digital humanities, GeoSPARQL, CIDOC CRM},
	location = {Beijing, China},
	numpages = {4},
	pages = {36–39},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '21},
	title = {Developing an Urban Gazetteer: A Semantic Web Database for Humanities Data},
	url = {https://doi.org/10.1145/3486187.3490204},
	year = {2021},
}

@inproceedings{10.1145/3486187.3490207,
	abstract = {This paper presents the current status and challenges of the La Sfera Project, a collaborative effort to transcribe, translate, and create a digital interface for Gregorio Dati's early 15th century treatise La sfera (The Globe). The treatise's survival in over 150 manuscript copies, many of them illustrated, provides a rich corpus for spatial analysis, but also presents numerous challenges, among them: how can we create a nimble digital interface showcasing the work's unique combination of text, maps, and images? How can digital techniques document vague, anachronistic, or imaginary places? How can we make the interface user-friendly while respecting each manuscript's idiosyncrasies of labeling, orthography, and visual representation? While researchers are increasingly interested in understanding how geographical knowledge and cartography developed in and around the Mediterranean in the 14th and 15th centuries, few reference resources exist which summarize that knowledge or help scholars identify toponym variants. Dati's treatise is unique in how it spans the practical world of cartography (specifically, the history of portolan charts) and the more impressionistic world of travel literature (such as the works of Marco Polo or Ibn Battutah). The La Sfera Project will thus create a multifunction, multimedia interface that can be used both for reference purposes and for teaching---presenting Dati's integrated world of cosmology, geography, and cartography using visual, textual, and spatial data.},
	address = {New York, NY, USA},
	author = {Agostini, Caterina and Bene\v{s}, Carrie},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3486187.3490207},
	isbn = {9781450391023},
	keywords = {spatio-temporal analysis, portolan charts, medieval manuscripts, digital humanities, Semantic annotation, Renaissance, IIIF},
	location = {Beijing, China},
	numpages = {6},
	pages = {22–27},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '21},
	title = {A Geospatial La Sfera: Navigating the Renaissance in the Mediterranean},
	url = {https://doi.org/10.1145/3486187.3490207},
	year = {2021},
}

@inproceedings{10.1145/3486626.3493430,
	abstract = {Urban information discovery has a long history in geography and other related disciplines. Routing and Point-of-Interest (POI) searching services have been implemented in many Geographic Information Systems (GISs)-based applications, where spatial information (e.g., POI data) is projected to a digital map as a source of auxiliary data to support intelligent spatial decision-making. In recent years, Augmented Reality (AR) has become a growing trend to visualize spatial objects through digital visual elements. However, a research gap exists between GIS and AR systems in terms of computational efficiency and interoperability. This paper aims to develop a geospatial cyberinfrastructure that integrates urban information and AR to enhance spatial knowledge visualization and discovery. Our experiments demonstrate how spatial analysis and urban sentiment computing can efficiently be integrated into the AR-based framework to support real-time decision-making.},
	address = {New York, NY, USA},
	author = {Li, Diya and Zhang, Zhe},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
	doi = {10.1145/3486626.3493430},
	isbn = {9781450391160},
	keywords = {urban computing, machine learning infrastructure, augmented reality, GIS cyberinfrastructure},
	location = {Beijing, China},
	numpages = {5},
	pages = {27–31},
	publisher = {Association for Computing Machinery},
	series = {ARIC '21},
	title = {Urban computing cyberinfrastructure: visualizing human sentiment using social media and augmented reality},
	url = {https://doi.org/10.1145/3486626.3493430},
	year = {2021},
}

@inproceedings{10.1145/3486626.3493432,
	abstract = {With increased big data and computing power, machine learning is predominantly used for classification to object detection and forecasting of phenomena and relationships. Forecasting, mapping and impact assessment of flood events is one such area where machine learning is gaining momentum. While machine learning has been widely used for forecasting of flood extent and depth using rainfall/runoff datasets, impact assessment based on flood severity distribution using machine learning is still a long way from maturity. In this study, we used several machine learning classifiers such as Decision Tree (DT), Random Forest (RF), Gradient Boosting (GB), Support Vector Machine (SVM) and Multinomial Logit (ML) to classify flood severity into four classes: Information, Advisory, Watch and Warning based on the training datasets obtained from the Model of Models. The Model of Models is an ensemble model which integrates flood forecasting models to determine flood severity globally at sub-watershed level based on spatial extent and duration of flooding, risk scores associated with historic flooding events. The severity classes are used to disseminate alerts to stakeholders globally. The initial results reveal that the GB followed by DT and RF classifier performed better for classifying severity based on the performance assessment metrics. While this study has implemented a first version machine learner, future advancements will focus on deploying adaptive learners to increase the forecasting ability of the machine learner with new datasets generated daily.},
	address = {New York, NY, USA},
	author = {Sharma, Prativa and Kar, Bandana and Wang, Jun and Bausch, Doug},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
	doi = {10.1145/3486626.3493432},
	isbn = {9781450391160},
	keywords = {model of models, machine learning, global, flood forecasting, alerting},
	location = {Beijing, China},
	numpages = {6},
	pages = {42–47},
	publisher = {Association for Computing Machinery},
	series = {ARIC '21},
	title = {A machine learning approach to flood severity classification and alerting},
	url = {https://doi.org/10.1145/3486626.3493432},
	year = {2021},
}

@inproceedings{10.1145/3486626.3493438,
	abstract = {Urban floods often cause the functional disruption of public transit systems, thereby impeding people's mobility and resulting in adverse socio-economic consequences. Climate change, rapid urbanization, and unplanned disaster management further increase trends of urban floods with higher frequency and intensity. This study employs data-driven machine learning (ML) models for predicting the flooding susceptibility of public transit systems in Toronto, ON, Canada. Four ML approaches are employed to evaluate the future risks of public transit systems being inundated by flooding events: 1) Random Forest (RF); 2) eXtreme Gradient Boosting (XGBoost); 3) K Nearest Neighbor (KNN); and 4) Na\"{\i}ve Bayes (NB). We estimate flooding probability based on the relationship between flood inundation events and their contributing factors. Flood-plain maps by Toronto and Region Conservation Authority (TRCA) are used to generate flood and non-flood locations as a basis for training (70\% of samples) and validating (30\% of samples) ML models. We use the Area-Under Receiver Operating Characteristic (AUROC) curve to evaluate the prediction results of ML models. All four models show a high level of accuracy higher than 95\%. Our results demonstrate public transit systems near major river channels are highly susceptible to floods which corroborated with historical flooding incidents in 2013 and 2018. The outcome of the study can be helpful to enhance the resilience of public transit systems in the city of Toronto and can facilitate evidence-based planning and policy to make cities more sustainable, livable, and resilient against flood hazards.},
	address = {New York, NY, USA},
	author = {Ahmed, Naser and Lee, Jinhyung},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
	doi = {10.1145/3486626.3493438},
	isbn = {9781450391160},
	keywords = {urban flood, susceptibility, public transit, machine learning, Toronto, GIS},
	location = {Beijing, China},
	numpages = {10},
	pages = {32–41},
	publisher = {Association for Computing Machinery},
	series = {ARIC '21},
	title = {Predicting urban flooding susceptibility of public transit systems using machine learning approaches: a case study of the largest city in Canada},
	url = {https://doi.org/10.1145/3486626.3493438},
	year = {2021},
}

@inproceedings{10.1145/3486629.3490695,
	abstract = {Rerouting is needed for a number of reasons such as overcoming a road incident, recovering from a navigation error, or avoiding a difficult turn. When obtaining a reroute, there is generally no choice but to take an inconvenient reroute - e.g., an overly long one. Although rerouting is a common operation, route guidance systems consider it as nothing more than finding an alternate route when needed. Such a view misses the opportunity to anticipate potential problems with reroutes on the way. This paper proposes a different perspective for rerouting where a path's potential reroutes are computed before navigation starts. This approach has the potential to identify desirable and adverse reroute properties and, thus, guide path planning. Such analysis requires formalising the concept of reroute. However, reroutes have previously been ambiguously or minimally defined. This research introduces formal definitions of reroutes and presents computational methods for obtaining sets of reroutes in a path. Subsequently, we assess the reroutes of shortest paths by running simulations in real-life street networks. The results show that at least 15\% of shortest paths have one detour that adds more than 50\% of travel time to the trip. Another result shows that in congestion, taking a reroute can save up to 9 minutes in trips that should take up to 15 minutes. The paper finishes by justifying and encouraging the use of rerouting as part of navigation query processing. Thus, a path planning method can more suitably handle road incidents, navigation errors, or dynamic navigation in general.},
	address = {New York, NY, USA},
	articleno = {8},
	author = {Amores, David and Tanin, Egemen and Vasardani, Maria},
	booktitle = {Proceedings of the 14th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3486629.3490695},
	isbn = {9781450391177},
	keywords = {spatial queries, path planning, navigation, graph algorithms},
	location = {Beijing, China},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '21},
	title = {A study of rerouting beyond ad hoc decision making},
	url = {https://doi.org/10.1145/3486629.3490695},
	year = {2021},
}

@inproceedings{10.1145/3486635.3491074,
	abstract = {Road safety barriers (e.g., concrete barriers, metal crash barriers, rumble strips) play an important role in preventing or mitigating vehicle crashes. Accurate maps of road safety barriers are critical components of safety infrastructure management systems at federal or state transportation agencies. In current practice, mapping road safety barriers is largely done manually (e.g., driving on the road or visual interpretation of street view imagery), which is slow, tedious, and expensive. We propose a deep learning approach to automatically map road safety barriers from street view imagery. Our approach considers road barriers as long objects spanning across consecutive street view images in a sequence and use a hybrid object-detection and recurrent-network model. Preliminary results on real-world street view imagery show that the proposed model outperforms several baseline methods.},
	address = {New York, NY, USA},
	author = {Rahman, Md Mostafijur and Sainju, Arpan Man and Yan, Da and Jiang, Zhe},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3486635.3491074},
	isbn = {9781450391207},
	keywords = {road safety features, neural networks, deep learning},
	location = {Beijing, China},
	numpages = {4},
	pages = {47–50},
	publisher = {Association for Computing Machinery},
	series = {GEOAI '21},
	title = {Mapping Road Safety Barriers Across Street View Image Sequences: A Hybrid Object Detection and Recurrent Model},
	url = {https://doi.org/10.1145/3486635.3491074},
	year = {2021},
}

@inproceedings{10.1145/3486635.3491076,
	abstract = {Representation learning of spatial and geographic data is a rapidly developing field which allows for similarity detection between areas and high-quality inference using deep neural networks. Past approaches however concentrated on embedding raster imagery (maps, street or satellite photos), mobility data or road networks. In this paper we propose the first approach to learning vector representations of OpenStreetMap regions with respect to urban functions and land-use in a micro-region grid. We identify a subset of OSM tags related to major characteristics of land-use, building and urban region functions, types of water, green or other natural areas. Through manual verification of tagging quality, we selected 36 cities were for training region representations. Uber's H3 index was used to divide the cities into hexagons, and OSM tags were aggregated for each hexagon. We propose the hex2vec method based on the Skip-gram model with negative sampling. The resulting vector representations showcase semantic structures of the map characteristics, similar to ones found in vector-based language models. We also present insights from region similarity detection in six Polish cities and propose a region typology obtained through agglomerative clustering.},
	address = {New York, NY, USA},
	author = {Wo\'{z}niak, Szymon and Szyma\'{n}ski, Piotr},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3486635.3491076},
	isbn = {9781450391207},
	keywords = {urban function and land-use embeddings, spatial representation learning, embedding, clustering, OpenStreetMap embeddings},
	location = {Beijing, China},
	numpages = {11},
	pages = {61–71},
	publisher = {Association for Computing Machinery},
	series = {GEOAI '21},
	title = {hex2vec: Context-Aware Embedding H3 Hexagons with OpenStreetMap Tags},
	url = {https://doi.org/10.1145/3486635.3491076},
	year = {2021},
}

@inproceedings{10.1145/3486637.3489495,
	abstract = {Clustering a set of given objects is a standard component of many data analysis tasks. The well-known k-means algorithm is a centroid-based clustering algorithm that optimizes the sum of distances between data objects and their assigned cluster centers. Each centroid then represents all objects assigned to a given cluster.In this paper, we study the special case of clustering semantically enriched spatio-temporal trajectories, i. e., trajectories where each trace point can be annotated with arbitrary, possibly categorical semantic data in addition to numerical spatio-temporal data. Such trajectories result from, e. g., tracking animals, humans, or weather phenomena and capture semantic contexts analysts may want to be aware of when interpreting the resulting clusters.Most current clustering algorithms for spatio-temporal categories take into account the numerical spatio-temporal coordinates only; thus, the resulting clusters do not necessarily reflect the characteristics of the additional semantic data. Building upon our earlier work on computing a representative trajectory for a given set of semantically enriched spatio-temporal trajectories, we describe how to implement the k-means algorithm to work with such data. In particular, we define a similarity measure called EFSMSim between a trajectory and a graph-based representation of a cluster centroid and show how to use this in the context of the k-means algorithm.We evaluate our EFSMClust approach by comparing it with state-of-the-art clustering algorithms taking into account either spatio-temporal information only or semantic attributes as well. Our experiments show that our algorithm is competitive even with respect to purely geometric performance measure and at the same time returns a representation of the centroids that can be used by domain experts to interpret both spatio-temporal and semantic information as well as to explore their possible relationships.},
	address = {New York, NY, USA},
	author = {Seep, Jana and Vahrenhold, Jan},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Animal Movement Ecology and Human Mobility},
	doi = {10.1145/3486637.3489495},
	isbn = {9781450391221},
	keywords = {semantic trajectories, semantic clustering, k-means},
	location = {Beijing, China},
	numpages = {10},
	pages = {38–47},
	publisher = {Association for Computing Machinery},
	series = {HANIMOB '21},
	title = {K-means for semantically enriched trajectories},
	url = {https://doi.org/10.1145/3486637.3489495},
	year = {2021},
}

@inproceedings{10.1145/3486640.3491392,
	abstract = {We selected 48 European cities and gathered their public transport timetables in the GTFS format. We utilized Uber's H3 spatial index to divide each city into hexagonal micro-regions. Based on the timetables data we created certain features describing the quantity and variety of public transport availability in each region. Next, we trained an auto-associative deep neural network to embed each of the regions. Having such prepared representations, we then used a hierarchical clustering approach to identify similar regions. To do so, we utilized an agglomerative clustering algorithm with a euclidean distance between regions and Ward's method to minimize in-cluster variance. Finally, we analyzed the obtained clusters at different levels to identify some number of clusters that qualitatively describe public transport availability. We showed that our typology matches the characteristics of analyzed cities and allows succesful searching for areas with similar public transport schedule characteristics.},
	address = {New York, NY, USA},
	author = {Gramacki, Piotr and Wo\'{z}niak, Szymon and Szyma\'{n}ski, Piotr},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
	doi = {10.1145/3486640.3491392},
	isbn = {9781450391238},
	keywords = {unsupervised representation learning, typology of public transport offering, public transport timetable embeddings},
	location = {Beijing, China},
	numpages = {8},
	pages = {5–12},
	publisher = {Association for Computing Machinery},
	series = {GeoSearch'21},
	title = {gtfs2vec: Learning GTFS Embeddings for comparing Public Transport Offer in Microregions},
	url = {https://doi.org/10.1145/3486640.3491392},
	year = {2021},
}

@inproceedings{10.1145/3486640.3491393,
	abstract = {Geospatial data has been widely used in Geographic Information Systems to understand spatial relationships in various application domains such as disaster response, agriculture risk management, environmental planning, and water resource protection. Many data sharing platforms such as NASA Open Data Portal and USGS Geo Data portal have been developed to enhance spatial data sharing services. However, enabling intelligent and efficient spatial data sharing and communication across different domains and stakeholders (e.g., data producers, researchers, and domain experts) is a formidable task. The challenges appear in building meaningful semantics between data products using spatiotemporal similarity measures to efficiently help users find all the relevant data and information at the space-time scale. In this paper, we developed a novel AI-based graph embedding algorithm to build semantic relationships between different spatial data sets to enable efficient and accurate data search. We applied the graph embedding algorithm to 30,000 NASA metadata records to test our algorithm's performance. In the end, we visualized the knowledge graph using the Neo4j database graphical user interface.},
	address = {New York, NY, USA},
	author = {Zhang, Zhe and Wang, Zhangyang and Li, Angela and Ye, Xinyue and Usery, E. Lynn and Li, Diya},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
	doi = {10.1145/3486640.3491393},
	isbn = {9781450391238},
	keywords = {Spatial Data Search, Knowledge Graph, KEYWORDS, Geographic Information System, Artificial Intelligence},
	location = {Beijing, China},
	numpages = {5},
	pages = {13–17},
	publisher = {Association for Computing Machinery},
	series = {GeoSearch'21},
	title = {An Al-based Spatial Knowledge Graph for Enhancing Spatial Data and Knowledge Search and Discovery},
	url = {https://doi.org/10.1145/3486640.3491393},
	year = {2021},
}

@inproceedings{10.1145/3486640.3491395,
	abstract = {This paper proposes a new method to join building footprint GIS data with the relevant buildings in a street-view image, taken by a vehicle-mounted camera. This is achieved by segmenting buildings in the street-view images and identifying the relevant building coordinates in the image. The building coordinates on the image are then estimated from the building vertices in the building footprint GIS data and vehicle trajectory history. Finally, the objective building is identified and relevant building attributes corresponding to each building image are linked together. This method enables the development of building image datasets with associated building attributes. The building image data, when linked to the relevant building attributes, could contribute to many innovative urban analyses, such as urban monitoring, the development of three-dimensional (3D) city models, and image datasets for training with annotated building attributes.},
	address = {New York, NY, USA},
	author = {Ogawa, Yoshiki and Oki, Takuya and Chen, Shenglong and Sekimoto, Yoshihide},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
	doi = {10.1145/3486640.3491395},
	isbn = {9781450391238},
	keywords = {Instance segmentation, Different types of data, Building detection, Angle},
	location = {Beijing, China},
	numpages = {7},
	pages = {18–24},
	publisher = {Association for Computing Machinery},
	series = {GeoSearch'21},
	title = {Joining Street-View Images and Building Footprint GIS Data},
	url = {https://doi.org/10.1145/3486640.3491395},
	year = {2021},
}

@article{10.1145/3486970,
	abstract = {COVID-19, the novel coronavirus that has disrupted lives around the world, continues to challenge how humans interact in public and shared environments. Repopulating the micro-spatial setting of an office building, with virus spread and transmission mitigation measures, is critical for a return to normalcy. Advice from public health experts, such as maintaining physical distancing from others and well-ventilated spaces, are essential, yet there is a lack of sound guidance on configuring office usage that allows for a safe return of workers. This paper highlights the potential for decision-making and planning insights through location analytics, particularly within an office setting. Proposed is a spatial analytic framework addressing the need for physical distancing and limiting worker interaction, supported by geographic information systems, network science, and spatial optimization. The developed modeling approach addresses dispersion of assigned office spaces as well as associated movement within the office environment. This can be used to support the design and utilization of offices in a manner that minimizes the risk of COVID-19 transmission. Our proposed model produces two main findings: (1) that the consideration of minimizing potential interaction as an objective has implications for the safety of work environments, and (2) that current social distancing measures may be inadequate within office settings. Our results show that leveraging exploratory spatial data analyses through the integration of geographic information systems, network science, and spatial optimization, enables the identification of workspace allocation alternatives in support of office repopulation efforts.},
	address = {New York, NY, USA},
	articleno = {18},
	author = {Burtner, Susan and Murray, Alan T.},
	doi = {10.1145/3486970},
	issn = {2374-0353},
	issue_date = {September 2022},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {health informatics, spatial interaction, network science, exploratory spatial data analysis, Spatial optimization},
	month = {jan},
	number = {3},
	numpages = {17},
	publisher = {Association for Computing Machinery},
	title = {COVID-19 and Minimizing Micro-Spatial Interactions},
	url = {https://doi.org/10.1145/3486970},
	volume = {8},
	year = {2022},
}

@inproceedings{10.1145/3488560.3508495,
	abstract = {Many online trip planning and navigation software need to routinely solve the problem of deciding where to take stops during a journey for various services such as refueling (or EV charging), rest stops, food, etc. The goal is to minimize the overhead of these stops while ensuring that the traveler is not starved of any essential resource (such as fuel, rest, or food) during the journey. In this paper, we formally model this problem and call it the pit stop problem. We design algorithms for this problem under various settings: single vs multiple types of stops, and offline vs online optimization (i.e., in advance of or during the trip). Our algorithms achieve provable guarantees in terms of approximating the optimal solution. We then extensively evaluate our algorithms on real world data and demonstrate that they significantly outperform baseline solutions.},
	address = {New York, NY, USA},
	author = {Kollias, Kostas},
	booktitle = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},
	doi = {10.1145/3488560.3508495},
	isbn = {9781450391320},
	keywords = {online algorithms, electric vehicle routing},
	location = {Virtual Event, AZ, USA},
	numpages = {1},
	pages = {1638},
	publisher = {Association for Computing Machinery},
	series = {WSDM '22},
	title = {The Pit Stop Problem: How to Plan Your Next Road Trip},
	url = {https://doi.org/10.1145/3488560.3508495},
	year = {2022},
}

@article{10.1145/3488723,
	abstract = {There has been a dramatic growth of shared mobility applications such as ride-sharing, food delivery, and crowdsourced parcel delivery. Shared mobility refers to transportation services that are shared among users, where a central issue is route planning. Given a set of workers and requests, route planning finds for each worker a route, i.e., a sequence of locations to pick up and drop off passengers/parcels that arrive from time to time, with different optimization objectives. Previous studies lack practicability due to their conflicted objectives and inefficiency in inserting a new request into a route, a basic operation called insertion. In addition, previous route planning solutions fail to exploit the appearance patterns of future requests hidden in historical data for optimization. In this paper, we present a unified formulation of route planning called URPSM. It has a well-defined parameterized objective function which eliminates the contradicted objectives in previous studies and enables flexible multi-objective route planning for shared mobility. We propose two insertion-based frameworks to solve the URPSM problem. The first is built upon the plain-insertion widely used in prior studies, which processes online requests only, whereas the second relies on a new insertion operator called prophet-insertion that handles both online and predicted requests. Novel dynamic programming algorithms are designed to accelerate both insertions to only linear time. Theoretical analysis shows that no online algorithm can have a constant competitive ratio for the URPSM problem under the competitive analysis model, yet our prophet-insertion-based framework can achieve a constant optimality ratio under the instance-optimality model. Extensive experimental results on real datasets show that our insertion-based solutions outperform the state-of-the-art algorithms in both effectiveness and efficiency by a large margin (e.g., up to 30 ( times )  more effective in the objective and up to 20 ( times )  faster).},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Tong, Yongxin and Zeng, Yuxiang and Zhou, Zimu and Chen, Lei and Xu, Ke},
	doi = {10.1145/3488723},
	issn = {0362-5915},
	issue_date = {March 2022},
	journal = {ACM Trans. Database Syst.},
	keywords = {Dynamic programming, Insertion, Ride-sharing, Route planning},
	month = {may},
	number = {1},
	numpages = {48},
	publisher = {Association for Computing Machinery},
	title = {Unified Route Planning for Shared Mobility: An Insertion-based Framework},
	url = {https://doi.org/10.1145/3488723},
	volume = {47},
	year = {2022},
}

@article{10.1145/3491063,
	abstract = {COVID-19 has spread worldwide, and over 140 million people have been confirmed infected, over 3 million people have died, and the numbers are still increasing dramatically. The consensus has been reached by scientists that COVID-19 can be transmitted in an airborne way, and human-to-human transmission is the primary cause of the fast spread of COVID-19. Thus, mobility should be restricted to control the epidemic, and many governments worldwide have succeeded in curbing the spread by means of control policies like city lockdowns. Against this background, we propose a novel fine-grained transmission model based on real-world human mobility data and develop a platform that helps the researcher or governors to explore the possibility of future development of the epidemic spreading and simulate the outcomes of human mobility and the epidemic state under different epidemic control policies. The proposed platform can also support users to determine potential contacts, discover regions with high infectious risks, and assess the individual infectious risk. The multi-functional platform aims at helping the users to evaluate the effectiveness of a regional lockdown policy and facilitate the process of screening and more accurately targeting the potential virus carriers.},
	address = {New York, NY, USA},
	articleno = {19},
	author = {Fan, Zipei and Yang, Chuang and Zhang, Zhiwen and Song, Xuan and Liu, Yinghao and Jiang, Renhe and Chen, Quanjun and Shibasaki, Ryosuke},
	doi = {10.1145/3491063},
	issn = {2374-0353},
	issue_date = {September 2022},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {human mobility simulation, Epidemic control policy simulation},
	month = {mar},
	number = {3},
	numpages = {16},
	publisher = {Association for Computing Machinery},
	title = {Human Mobility-based Individual-level Epidemic Simulation Platform},
	url = {https://doi.org/10.1145/3491063},
	volume = {8},
	year = {2022},
}

@article{10.1145/3493499.3493503,
	abstract = {Region interpolation methods impose restrictions that have an influence on how the evolution of concavities over time is represented, potentially, causing their evolution to look unnatural, e.g., a concavity unexpectedly appears (disappears) from (to) a point. In a previous work we presented an algorithm to transform a line segment to a concavity, and a line segment to a simple non-closed linestring with possibly several concavities. The algorithm is deterministic, it does not assume that an element in the source is transformed directly to an element (or set of elements) in the target, works in stages (steps), i.e., several different transformations can occur while an element in the source is transformed to an element in the target, and its output is a set of moving segments representing the transformation. The complexity of a non-optimized implementation of the transformation of a segment to a concavity using the algorithm is O(kn2), where k is the number of steps in the transformation (the number of intermediate transformations in the transformation) and n is the number of points in the target geometry. The algorithm is primarily devised to be integrated with the region interpolation methods proposed in the spatiotemporal databases literature. In this work we improve the complexity of the algorithm to be O(n2) in the wort case.},
	address = {New York, NY, USA},
	author = {Duarte, Jos\'{e} and McKenney, Mark},
	doi = {10.1145/3493499.3493503},
	issn = {1559-6915},
	issue_date = {September 2021},
	journal = {SIGAPP Appl. Comput. Rev.},
	keywords = {spatiotemporal data, moving regions, interpolating polylines, interpolating concavities},
	month = {oct},
	number = {3},
	numpages = {12},
	pages = {49–60},
	publisher = {Association for Computing Machinery},
	title = {Interpolating concavities},
	url = {https://doi.org/10.1145/3493499.3493503},
	volume = {21},
	year = {2021},
}

@article{10.1145/3498338,
	abstract = {With the continued deployment of the Internet of Things (IoT), increasing volumes of devices are being deployed that emit massive spatially referenced data. Due in part to the dynamic, decentralized, and heterogeneous architecture of the IoT, the varying and often low quality of spatial IoT data (SID) presents challenges to applications built on top of this data. This survey aims to provide unique insight to practitioners who intend to develop IoT-enabled applications and to researchers who wish to conduct research that relates to data quality in the IoT setting. The survey offers an inventory analysis of major data quality dimensions in SID and covers significant data characteristics and associated quality considerations. The survey summarizes data quality related technologies from both task and technique perspectives. Organizing the technologies from the task perspective, it covers recent progress in SID quality management, encompassing location refinement, uncertainty elimination, outlier removal, fault correction, data integration, and data reduction; and it covers low-quality SID exploitation, encompassing querying, analysis, and decision-making techniques. Finally, the survey covers emerging trends and open issues concerning the quality of SID.},
	address = {New York, NY, USA},
	articleno = {57},
	author = {Li, Huan and Lu, Hua and Jensen, Christian S. and Tang, Bo and Cheema, Muhammad Aamir},
	doi = {10.1145/3498338},
	issn = {0360-0300},
	issue_date = {March 2023},
	journal = {ACM Comput. Surv.},
	keywords = {spatiotemporal dependencies, spatial computing, spatial queries, spatiotemporal data cleaning, location refinement, quality management, geo-sensory data, Internet of Things},
	month = {feb},
	number = {3},
	numpages = {41},
	publisher = {Association for Computing Machinery},
	title = {Spatial Data Quality in the Internet of Things: Management, Exploitation, and Prospects},
	url = {https://doi.org/10.1145/3498338},
	volume = {55},
	year = {2022},
}

@inproceedings{10.1145/3501409.3501475,
	abstract = {This paper mainly makes an in-depth study on the three-dimensional modeling and management of the campus based on ArcGIS Pro and CityEngine. Taking Institute of Disaster Prevention as the research object, CiytEngine is used to model the main ground features such as buildings, roads, trees and vegetation in the campus, the functions of publishing, browsing and attribute query of scene model in ArcGIS Pro are realized, and the integrated management of 3D model is realized in ArcGIS Pro platform.},
	address = {New York, NY, USA},
	author = {Song, Ping and Sun, Guangtong and Liu, Xiaoyang},
	booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
	doi = {10.1145/3501409.3501475},
	isbn = {9781450384322},
	keywords = {CityEngine, Campus management, ArcGIS Pro, 3D campus modeling},
	location = {Xiamen, China},
	numpages = {6},
	pages = {355–360},
	publisher = {Association for Computing Machinery},
	series = {EITCE '21},
	title = {Research on 3D campus integrated management based on ArcGIS Pro and CityEngine},
	url = {https://doi.org/10.1145/3501409.3501475},
	year = {2022},
}

@inproceedings{10.1145/3501409.3501628,
	abstract = {As the significant internal connections inside a region, the highway network plays an important role in the regional sustainable development. This paper utilizes the method of geographical information system (GIS) to construct a topology model with nodes described by the intersections and entrances of highway network in the Pearl River Delta (PRD), China. Furthermore, the spatial econometric statistical analysis models are applied to detect and characterize the spatial accessibility differences and heterogeneity. This aims to reveal the spatial nonstationarity of the regional accessibility in the PRD, and thus support establish a sophisticated highway network system for promoting the development of the PRD.},
	address = {New York, NY, USA},
	author = {Chen, Shaopei},
	booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
	doi = {10.1145/3501409.3501628},
	isbn = {9781450384322},
	keywords = {spatial differences and heterogeneity, spatial auto-correction, highway network, accessibility},
	location = {Xiamen, China},
	numpages = {5},
	pages = {1246–1250},
	publisher = {Association for Computing Machinery},
	series = {EITCE '21},
	title = {Spatial Differences and Heterogeneity of Regional Accessibility Based on Highway Network: A Case Study of the Pearl River Delta, China},
	url = {https://doi.org/10.1145/3501409.3501628},
	year = {2022},
}

@inproceedings{10.1145/3501409.3501656,
	abstract = {In the life-cycle engineering management of a project, a large amount of diversified and heterogeneous data is involved. How to efficiently manage and visualize various data sources is the basis for realizing the refined management of project construction. Based on the concept of data fusion, this article sorts out the multiple data involved in project engineering management, analyses the data fusion method and process of multivariate structured data and geospatial data. By combining 3DGIS in data integration, management and visualization, data fusion and expression in a three-dimensional environment can be realized. Taking the engineering management system of the bank slope of Song Gang Area as an example, it verifies the validity of the multivariate data fusion method based on 3DGIS in this paper.},
	address = {New York, NY, USA},
	author = {Liu, Chengkun and Ma, Rui and Hu, Binbin and Fan, Qingsong},
	booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
	doi = {10.1145/3501409.3501656},
	isbn = {9781450384322},
	keywords = {Visualization, Multivariate Data Fusion, Engineering Management, 3DGIS},
	location = {Xiamen, China},
	numpages = {5},
	pages = {1393–1397},
	publisher = {Association for Computing Machinery},
	series = {EITCE '21},
	title = {Multivariate Data Fusion Method Based on 3DGIS and its Application in Engineering Management},
	url = {https://doi.org/10.1145/3501409.3501656},
	year = {2022},
}

@article{10.1145/3503513,
	abstract = {R-tree is a foundational data structure used in spatial databases and scientific databases. With the advancement of networks and computer architectures, in-memory data processing for R-tree in distributed systems has become a common platform. We have observed new performance challenges to process R-tree as the amount of multidimensional datasets become increasingly high. Specifically, an R-tree server can be heavily overloaded while the network and client CPU are lightly loaded, and vice versa.In this article, we present the design and implementation of Catfish, an RDMA-enabled R-tree for low latency and high throughput by adaptively utilizing the available network bandwidth and computing resources to balance the workloads between clients and servers. We design and implement two basic mechanisms of using RDMA for a client-server R-tree data processing system. First, in the fast messaging design, we use RDMA writes to send R-tree requests to the server and let server threads process R-tree requests to achieve low query latency. Second, in the RDMA offloading design, we use RDMA reads to offload tree traversal from the server to the client, which rescues the server as it is overloaded. We further develop an adaptive scheme to effectively switch an R-tree search between fast messaging and RDMA offloading, maximizing the overall performance. Our experiments show that the adaptive solution of Catfish on InfiniBand significantly outperforms R-tree that uses only fast messaging or only RDMA offloading in both latency and throughput. Catfish can also deliver up to one order of magnitude performance over the traditional schemes using TCP/IP on 1 and 40 Gbps Ethernet. We make a strong case to use RDMA to effectively balance workloads in distributed systems for low latency and high throughput.},
	address = {New York, NY, USA},
	articleno = {15},
	author = {Xiao, Mengbai and Wang, Hao and Geng, Liang and Lee, Rubao and Zhang, Xiaodong},
	doi = {10.1145/3503513},
	issn = {2374-0353},
	issue_date = {June 2022},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {R-tree, RDMA},
	month = {feb},
	number = {2},
	numpages = {26},
	publisher = {Association for Computing Machinery},
	title = {An RDMA-enabled In-memory Computing Platform for R-tree on Clusters},
	url = {https://doi.org/10.1145/3503513},
	volume = {8},
	year = {2022},
}

@article{10.1145/3505266,
	address = {New York, NY, USA},
	author = {Burke, Robin},
	doi = {10.1145/3505266},
	issn = {0001-0782},
	issue_date = {February 2022},
	journal = {Commun. ACM},
	month = {jan},
	number = {2},
	numpages = {1},
	pages = {100},
	publisher = {Association for Computing Machinery},
	title = {Personalized recommendation of PoIs to people with autism: technical perspective},
	url = {https://doi.org/10.1145/3505266},
	volume = {65},
	year = {2022},
}

@article{10.1145/3505267,
	abstract = {The suggestion of Points of Interest (PoIs) to people with autism spectrum disorders challenges the research about recommender systems by introducing an explicit need to consider both user preferences and aversions in item evaluation. The reason is that autistic users' perception of places is influenced by sensory aversions, which can cause stress and anxiety when they visit the suggested PoIs. Therefore, the management of individual preferences is not enough to provide these people with suitable recommendations.To address this issue, we propose a Top-N recommendation model that combines information about the user's idiosyncratic aversions with her/his preferences in a personalized way. The goal is that of suggesting the places that (s)he can like and smoothly experience at the same time. We are interested in finding a user-specific balance of compatibility and interest within a recommendation model that integrates heterogeneous evaluation criteria to appropriately take these aspects into account.We tested our model on 148 adults, 20 of which were people with autism spectrum disorders. The evaluation results show that, on both groups, our model achieves superior accuracy and ranking results than the recommender systems based on item compatibility, on user preferences, or which integrate these aspects using a uniform evaluation model. These findings encourage us to use our model as a basis for the development of inclusive recommender systems.},
	address = {New York, NY, USA},
	author = {Mauro, Noemi and Ardissono, Liliana and Cena, Federica},
	doi = {10.1145/3505267},
	issn = {0001-0782},
	issue_date = {February 2022},
	journal = {Commun. ACM},
	month = {jan},
	number = {2},
	numpages = {9},
	pages = {101–109},
	publisher = {Association for Computing Machinery},
	title = {Supporting people with autism spectrum disorders in the exploration of PoIs: an inclusive recommender system},
	url = {https://doi.org/10.1145/3505267},
	volume = {65},
	year = {2022},
}

@inproceedings{10.1145/3511808.3557097,
	abstract = {With the increased popularity of mobile devices, Web mapping services have become an indispensable tool in our daily lives. To provide user-satisfied services, such as location searches, the point of interest (POI) database is the fundamental infrastructure, as it archives multimodal information on billions of geographic locations closely related to people's lives, such as a shop or a bank. Therefore, verifying the correctness of a large-scale POI database is vital. To achieve this goal, many industrial companies adopt volunteered geographic information (VGI) platforms that enable thousands of crowdworkers and expert mappers to verify POIs seamlessly; but to do so, they have to spend millions of dollars every year. To save the tremendous labor costs, we devised DuMapper, an automatic system for large-scale POI verification with the multimodal street-view data at Baidu Maps. This paper presents not only DuMapper I, which imitates the process of POI verification conducted by expert mappers, but also proposes DuMapper II, a highly efficient framework to accelerate POI verification by means of deep multimodal embedding and approximate nearest neighbor (ANN) search. DuMapper II takes the signboard image and the coordinates of a real-world place as input to generate a low-dimensional vector, which can be leveraged by ANN algorithms to conduct a more accurate search through billions of archived POIs in the database for verification within milliseconds. Compared with DuMapper I, experimental results demonstrate that DuMapper II can significantly increase the throughput of POI verification by 50 times. DuMapper has already been deployed in production since June 2018, which dramatically improves the productivity and efficiency of POI verification at Baidu Maps. As of December 31, 2021, it has enacted over 405 million iterations of POI verification within a 3.5-year period, representing an approximate workload of 800 high-performance expert mappers.},
	address = {New York, NY, USA},
	author = {Fan, Miao and Huang, Jizhou and Wang, Haifeng},
	booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
	doi = {10.1145/3511808.3557097},
	isbn = {9781450392365},
	keywords = {web mapping services, street views, deep multimodal embedding, approximate nearest neighbor search, POI verification},
	location = {Atlanta, GA, USA},
	numpages = {9},
	pages = {3063–3071},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {DuMapper: Towards Automatic Verification of Large-Scale POIs with Street Views at Baidu Maps},
	url = {https://doi.org/10.1145/3511808.3557097},
	year = {2022},
}

@inproceedings{10.1145/3511808.3557153,
	abstract = {Understanding economic development and designing government policies requires accurate and timely measurements of socioeconomic activities. In this paper, we show how to leverage city structural information and urban imagery like satellite images and street view images to accurately predict multi-level socioeconomic indicators. Our framework consists of four steps. First, we extract structural information from cities by transforming real-world street networks into city graphs (GeoStruct). Second, we design a contrastive learning-based model to refine urban image features by looking at geographic similarity between images, with images that are geographically close together having similar features (GeoCLR). Third, we propose using street segments as containers to adaptively fuse the features of multi-view urban images, including satellite images and street view images (GeoFuse). Finally, given the city graph with a street segment as a node and a neighborhood area as a subgraph, we jointly model street- and neighborhood-level socioeconomic indicator predictions as node and subgraph classification tasks. The novelty of our method is that we introduce city structure to organize multi-view urban images and model the relationships between socioeconomic indicators at different levels. We evaluate our framework on the basis of real-world datasets collected in multiple cities. Our proposed framework improves performance by over 10\% when compared to state-of-the-art baselines in terms of prediction accuracy and recall.},
	address = {New York, NY, USA},
	author = {Li, Tong and Xin, Shiduo and Xi, Yanxin and Tarkoma, Sasu and Hui, Pan and Li, Yong},
	booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
	doi = {10.1145/3511808.3557153},
	isbn = {9781450392365},
	keywords = {urban imagery, sustainable development, socioeconomic indicators},
	location = {Atlanta, GA, USA},
	numpages = {10},
	pages = {3282–3291},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {Predicting Multi-level Socioeconomic Indicators from Structural Urban Imagery},
	url = {https://doi.org/10.1145/3511808.3557153},
	year = {2022},
}

@inproceedings{10.1145/3511808.3557720,
	abstract = {Profiling urban regions is essential for urban analytics and planning. Although existing studies have made great efforts to learn urban region representation from multi-source urban data, there are still limitations on modelling local-level signals, developing an effective yet integrated fusion framework, and performing well in regions with high variance socioeconomic attributes. Thus, we propose a multi-graph representation learning framework, called Region2Vec, for urban region profiling. Specifically, except that human mobility is encoded for inter-region relations, geographic neighborhood is introduced for capturing geographical contextual information while POI side information is adopted for representing intra-region information. Then, graphs are used to capture accessibility, vicinity, and functionality correlations among regions. An encoder-decoder multi-graph fusion module is further proposed to jointly learn comprehensive representations. Experiments on real-world datasets show that Region2Vec can be employed in three applications and outperforms all state-of-the-art baselines. Particularly, Region2Vec has better performance than previous studies in regions with high variance socioeconomic attributes.},
	address = {New York, NY, USA},
	author = {Luo, Yan and Chung, Fu-lai and Chen, Kai},
	booktitle = {Proceedings of the 31st ACM International Conference on Information \&amp; Knowledge Management},
	doi = {10.1145/3511808.3557720},
	isbn = {9781450392365},
	keywords = {urban computing, geographic information systems, data mining},
	location = {Atlanta, GA, USA},
	numpages = {5},
	pages = {4294–4298},
	publisher = {Association for Computing Machinery},
	series = {CIKM '22},
	title = {Urban Region Profiling via Multi-Graph Representation Learning},
	url = {https://doi.org/10.1145/3511808.3557720},
	year = {2022},
}

@inproceedings{10.1145/3514221.3517823,
	abstract = {Kernel Density Visualization (KDV) has been extensively used in a wide range of applications, including traffic accident hotspot detection, crime hotspot detection, disease outbreak detection, and ecological modeling. However, KDV is a computationally expensive operation, which is not scalable to large datasets (e.g., million-scale data points) and high resolution sizes (e.g., 1920 x 1080). To significantly improve the efficiency for generating KDV, we develop two efficient Sweep Line AlgorithMs (SLAM), which can theoretically reduce the time complexity for generating KDV. By incorporating the resolution-aware optimization (RAO) into SLAM, we can further achieve the lowest time complexity for generating KDV. Our extensive experiments on four large-scale real datasets (up to 4.33 million data points) show that all our methods can achieve one to two-order-of-magnitude speedup in many test cases and efficiently support KDV with exploratory operations (e.g., zooming and panning) compared with the state-of-the-art solutions.},
	address = {New York, NY, USA},
	author = {Chan, Tsz Nam and U, Leong Hou and Choi, Byron and Xu, Jianliang},
	booktitle = {Proceedings of the 2022 International Conference on Management of Data},
	doi = {10.1145/3514221.3517823},
	isbn = {9781450392495},
	keywords = {reduce the time complexity, kernel density visualization, hotspot detection, exploratory operations, SLAM},
	location = {Philadelphia, PA, USA},
	numpages = {15},
	pages = {2120–2134},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '22},
	title = {SLAM: Efficient Sweep Line Algorithms for Kernel Density Visualization},
	url = {https://doi.org/10.1145/3514221.3517823},
	year = {2022},
}

@inproceedings{10.1145/3514221.3522568,
	abstract = {Within the rapidly expanding Internet of Things (IoT), growing amounts of spatially referenced data are being generated. Due to the dynamic, decentralized, and heterogeneous nature of the IoT, spatial IoT data (SID) quality has attracted considerable attention in academia and industry. How to invent and use technologies for managing spatial data quality and exploiting low-quality spatial data are key challenges in the IoT. In this tutorial, we highlight the SID consumption requirements in applications and offer an overview of spatial data quality in the IoT setting. In addition, we review pertinent technologies for quality management and low-quality data exploitation, and we identify trends and future directions for quality-aware SID management and utilization. The tutorial aims to not only help researchers and practitioners to better comprehend SID quality challenges and solutions, but also offer insights that may enable innovative research and applications.},
	address = {New York, NY, USA},
	author = {Li, Huan and Tang, Bo and Lu, Hua and Cheema, Muhammad Aamir and Jensen, Christian S.},
	booktitle = {Proceedings of the 2022 International Conference on Management of Data},
	doi = {10.1145/3514221.3522568},
	isbn = {9781450392495},
	keywords = {quality management, internet of things, geo-sensory data},
	location = {Philadelphia, PA, USA},
	numpages = {9},
	pages = {2474–2482},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '22},
	title = {Spatial Data Quality in the IoT Era: Management and Exploitation},
	url = {https://doi.org/10.1145/3514221.3522568},
	year = {2022},
}

@inproceedings{10.1145/3529372.3533280,
	abstract = {We present a hybrid text and geospatial search application for hydrographic datasets built on the open-source Lucene search library. Our goal is to demonstrate that it is possible to build custom GIS applications by integrating existing open-source components and data sources, which contrasts with existing approaches based on monolithic platforms such as ArcGIS and QGIS. Lucene provides rich index structures and search capabilities for free text and geometries; the former has already been integrated and exposed via our group's Anserini and Pyserini IR toolkits. In this work, we extend these toolkits to include geospatial capabilities. Combining knowledge extracted from Wikidata with the HydroSHEDS dataset, our application enables text and geospatial search of rivers worldwide.},
	address = {New York, NY, USA},
	articleno = {36},
	author = {Yang, Matthew Y. R. and Yang, Siwen and Lin, Jimmy},
	booktitle = {Proceedings of the 22nd ACM/IEEE Joint Conference on Digital Libraries},
	doi = {10.1145/3529372.3533280},
	isbn = {9781450393454},
	keywords = {rivers, river basins, lucene, geospatial indexing},
	location = {Cologne, Germany},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	series = {JCDL '22},
	title = {Integration of text and geospatial search for hydrographic datasets using the lucene search library},
	url = {https://doi.org/10.1145/3529372.3533280},
	year = {2022},
}

@inproceedings{10.1145/3534678.3539108,
	abstract = {Multi-touch attribution (MTA), aiming to estimate the contribution of each advertisement touchpoint in conversion journeys, is essential for budget allocation and automatically advertising. Existing methods first train a model to predict the conversion probability of the advertisement journeys with historical data and calculate the attribution of each touchpoint by using the results counterfactual predictions. An assumption of these works is the conversion prediction model is unbiased. It can give accurate predictions on any randomly assigned journey, including both the factual and counterfactual ones. Nevertheless, this assumption does not always hold as the user preferences act as the common cause for both ad generation and user conversion, involving the confounding bias and leading to an out-of-distribution (OOD) problem in the counterfactual prediction. In this paper, we define the causal MTA task and propose CausalMTA to solve this problem. It systemically eliminates the confounding bias from both static and dynamic perspectives and learn an unbiased conversion prediction model using historical data. We also provide a theoretical analysis to prove the effectiveness of CausalMTA with sufficient ad journeys. Extensive experiments on both synthetic and real data in Alibaba advertising platform show that CausalMTA can not only achieve better prediction performance than the state-of-the-art method but also generate meaningful attribution credits across different advertising channels.},
	address = {New York, NY, USA},
	author = {Yao, Di and Gong, Chang and Zhang, Lei and Chen, Sheng and Bi, Jingping},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3534678.3539108},
	isbn = {9781450393850},
	keywords = {multi-touch attribution, counterfactual prediction, computational advertising},
	location = {Washington DC, USA},
	numpages = {11},
	pages = {4342–4352},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {CausalMTA: Eliminating the User Confounding Bias for Causal Multi-touch Attribution},
	url = {https://doi.org/10.1145/3534678.3539108},
	year = {2022},
}

@inproceedings{10.1145/3534678.3539358,
	abstract = {Computing trajectory similarities is a critical and fundamental task for various spatial-temporal applications, such as clustering, prediction, and anomaly detection. Traditional similarity metrics, i.e. DTW and Hausdorff, suffer from quadratic computation complexity, leading to their inability on large-scale data. To solve this problem, many trajectory representation learning techniques are proposed to approximate the metric space while reducing the complexity of similarity computation. Nevertheless, these works are designed based on RNN backend, resulting in a serious performance decline on long trajectories. In this paper, we propose a novel graph-based method, namely TrajGAT, to explicitly model the hierarchical spatial structure and improve the performance of long trajectory similarity computation. TrajGAT consists of two main modules, i.e. , graph construction and trajectory encoding. For graph construction, TrajGAT first employs PR quadtree to build the hierarchical structure of the whole spatial area, and then constructs a graph for each trajectory based on the original records and the leaf nodes of the quadtree. For trajectory encoding, we replace the self-attention in Transformer with graph attention and design an encoder to represent the generated graph trajectory. With these two modules, TrajGAT can capture the long-term dependencies of trajectories while reducing the GPU memory usage of Transformer. Our experiments on two real-life datasets show that TrajGAT not only improves the performance on long trajectories but also outperforms the state-of-the-art methods on mixture trajectories significantly.},
	address = {New York, NY, USA},
	author = {Yao, Di and Hu, Haonan and Du, Lun and Cong, Gao and Han, Shi and Bi, Jingping},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3534678.3539358},
	isbn = {9781450393850},
	keywords = {transformer, trajectory similarity computation, long-term dependency, graph attention network},
	location = {Washington DC, USA},
	numpages = {11},
	pages = {2275–2285},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {TrajGAT: A Graph-based Long-term Dependency Modeling Approach for Trajectory Similarity Computation},
	url = {https://doi.org/10.1145/3534678.3539358},
	year = {2022},
}

@inproceedings{10.1145/3534678.3539410,
	abstract = {Given raster imagery features and imperfect vector training labels with registration uncertainty, this paper studies a deep learning framework that can quantify and reduce the registration uncertainty of training labels as well as train neural network parameters simultaneously. The problem is important in broad applications such as streamline classification on Earth imagery or tissue segmentation on medical imagery, whereby annotating precise vector labels is expensive and time-consuming. However, the problem is challenging due to the gap between the vector representation of class labels and the raster representation of image features and the need for training neural networks with uncertain label locations. Existing research on uncertain training labels often focuses on uncertainty in label class semantics or characterizes label registration uncertainty at the pixel level (not contiguous vectors). To fill the gap, this paper proposes a novel learning framework that explicitly quantifies vector labels' registration uncertainty. We propose a registration-uncertainty-aware loss function and design an iterative uncertainty reduction algorithm by re-estimating the posterior of true vector label locations distribution based on a Gaussian process. Evaluations on real-world datasets in National Hydrography Dataset refinement show that the proposed approach significantly outperforms several baselines in the registration uncertainty estimations performance and classification performance.},
	address = {New York, NY, USA},
	author = {He, Wenchong and Jiang, Zhe and Kriby, Marcus and Xie, Yiqun and Jia, Xiaowei and Yan, Da and Zhou, Yang},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3534678.3539410},
	isbn = {9781450393850},
	keywords = {weakly supervised spatial deep learning, remote sensing, registration uncertainty, imperfect vector labels},
	location = {Washington DC, USA},
	numpages = {11},
	pages = {554–564},
	publisher = {Association for Computing Machinery},
	series = {KDD '22},
	title = {Quantifying and Reducing Registration Uncertainty of Spatial Vector Labels on Earth Imagery},
	url = {https://doi.org/10.1145/3534678.3539410},
	year = {2022},
}

@article{10.1145/3539660,
	abstract = {An important problem in terrain analysis is modeling how water flows across a terrain creating floods by forming channels and filling depressions. In this article, we study a number of flow-query-related problems: Given a terrain Σ, represented as a triangulated xy-monotone surface with n vertices, and a rain distribution R that may vary over time, determine how much water is flowing over a given vertex or edge as a function of time. We develop internal-memory as well as I/O-efficient algorithms for flow queries. This article contains four main algorithmic results:(i) An internal-memory algorithm for answering terrain-flow queries: Preprocess Σ into a linear-size data structure so given a rain distribution R, the flow-rate functions of all vertices and edges of Σ can be reported quickly. (ii) I/O-efficient algorithms for answering terrain-flow queries. (iii) An internal-memory algorithm for answering vertex-flow queries: Preprocess Σ into a linear-size data structure so given a rain distribution R, the flow-rate function of a vertex under the single-flow direction (SFD) model can be computed quickly.(iv) An efficient algorithm that, given a path 𝖯 in Σ and flow rate along 𝖯, computes the two-dimensional channel along which water flows.Additionally, we implement a version of the terrain-flow query and 2D channel algorithms and examine a number of queries on real terrains.},
	address = {New York, NY, USA},
	articleno = {3},
	author = {Arge, Lars and Lowe, Aaron and Svendsen, Svend C. and Agarwal, Pankaj K.},
	doi = {10.1145/3539660},
	issn = {2374-0353},
	issue_date = {March 2023},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Terrains, hydrological modeling, flood-risk analysis, river-network extraction},
	month = {jan},
	number = {1},
	numpages = {39},
	publisher = {Association for Computing Machinery},
	title = {1D and 2D Flow Routing on a Terrain},
	url = {https://doi.org/10.1145/3539660},
	volume = {9},
	year = {2023},
}

@inproceedings{10.1145/3543507.3583469,
	abstract = {Assigning semantically relevant, real-world locations to documents opens new possibilities to perform geographic information retrieval. We propose a novel approach to automatically determine the latitude-longitude coordinates of appropriate Wikipedia articles with high accuracy, leveraging both text and metadata in the corpus. By examining articles whose base-truth coordinates are known, we show that our method attains a substantial improvement over state of the art works. We subsequently demonstrate how our approach could yield two benefits: (1) detecting significant geolocation errors in Wikipedia; and (2) proposing approximated coordinates for hundreds of thousands of articles which are not traditionally considered to be locations (such as events, ideas or people), opening new possibilities for conceptual geographic retrievals over Wikipedia.},
	address = {New York, NY, USA},
	author = {Krause, Amir and Cohen, Sara},
	booktitle = {Proceedings of the ACM Web Conference 2023},
	doi = {10.1145/3543507.3583469},
	isbn = {9781450394161},
	keywords = {Geographic Information Retrieval, Geolocation, Wikipedia},
	location = {Austin, TX, USA},
	numpages = {11},
	pages = {3331–3341},
	publisher = {Association for Computing Machinery},
	series = {WWW '23},
	title = {Geographic Information Retrieval Using Wikipedia Articles},
	url = {https://doi.org/10.1145/3543507.3583469},
	year = {2023},
}

@inproceedings{10.1145/3543507.3583862,
	abstract = {Poverty maps are essential tools for governments and NGOs to track socioeconomic changes and adequately allocate infrastructure and services in places in need. Sensor and online crowd-sourced data combined with machine learning methods have provided a recent breakthrough in poverty map inference. However, these methods do not capture local wealth fluctuations, and are not optimized to produce accountable results that guarantee accurate predictions to all sub-populations. Here, we propose a pipeline of machine learning models to infer the mean and standard deviation of wealth across multiple geographically clustered populated places, and illustrate their performance in Sierra Leone and Uganda. These models leverage seven independent and freely available feature sources based on satellite images, and metadata collected via online crowd-sourcing and social media. Our models show that combined metadata features are the best predictors of wealth in rural areas, outperforming image-based models, which are the best for predicting the highest wealth quintiles. Our results recover the local mean and variation of wealth, and correctly capture the positive yet non-monotonous correlation between them. We further demonstrate the capabilities and limitations of model transfer across countries and the effects of data recency and other biases. Our methodology provides open tools to build towards more transparent and interpretable models to help governments and NGOs to make informed decisions based on data availability, urbanization level, and poverty thresholds.},
	address = {New York, NY, USA},
	author = {Esp\'{\i}n-Noboa, Lisette and Kert\'{e}sz, J\'{a}nos and Karsai, M\'{a}rton},
	booktitle = {Proceedings of the ACM Web Conference 2023},
	doi = {10.1145/3543507.3583862},
	isbn = {9781450394161},
	keywords = {deep learning, high-resolution spatial inference, machine learning, online crowd-sourced data, poverty maps, satellite images},
	location = {Austin, TX, USA},
	numpages = {12},
	pages = {4029–4040},
	publisher = {Association for Computing Machinery},
	series = {WWW '23},
	title = {Interpreting wealth distribution via poverty map inference using multimodal data},
	url = {https://doi.org/10.1145/3543507.3583862},
	year = {2023},
}

@article{10.1145/3543850,
	abstract = {A region ℛ is a dwell region for a moving object O if, given a threshold distance rq and duration τq, every point of ℛ remains within distance rq from O for at least time τq. Points within ℛ are likely to be of interest to O, so identification of dwell regions has applications such as monitoring and surveillance. We first present a logarithmic-time online algorithm to find dwell regions in an incoming stream of object positions. Our method maintains the upper and lower bounds for the radius of the smallest circle enclosing the object positions, thereby greatly reducing the number of trajectory points needed to evaluate the query. It approximates the radius of the smallest circle enclosing a given subtrajectory within an arbitrarily small user-defined factor and is also able to efficiently answer decision queries asking whether or not a dwell region exists. For the offline version of the dwell region problem, we first extend our online approach to develop the ρ-Index, which indexes subtrajectories using query radius ranges. We then refine this approach to obtain the τ-Index, which indexes subtrajectories using both query radius ranges and dwell durations. Our experiments using both real-world and synthetic datasets show that the online approach can scale up to hundreds of thousands of moving objects. For archived trajectories, our indexing approaches speed up queries by many orders of magnitude.},
	address = {New York, NY, USA},
	articleno = {9},
	author = {Uddin, Reaz and Mahin, Mehnaz Tabassum and Rajan, Payas and Ravishankar, Chinya V. and Tsotras, Vassilis J.},
	doi = {10.1145/3543850},
	issn = {2374-0353},
	issue_date = {June 2023},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Spatio-temporal databases, stay regions, smallest enclosing circle, regions of interest, trajectories},
	month = {apr},
	number = {2},
	numpages = {35},
	publisher = {Association for Computing Machinery},
	title = {Dwell Regions: Generalized Stay Regions for Streaming and Archival Trajectory Data},
	url = {https://doi.org/10.1145/3543850},
	volume = {9},
	year = {2023},
}

@inproceedings{10.1145/3543873.3587356,
	abstract = {Wikidata Atlas is an online system that allows users to explore Wikidata items on an interactive global map; for example, users can explore the global distribution of all lighthouses described by Wikidata. Designing such a system poses challenges in terms of scalability, where some classes have hundreds of thousands of instances; efficiency, where visualisations are generated live; freshness, where we want changes on Wikidata to be reflected as they happen in the system; and usability, where we aim for the system to be accessible for a broad audience. Herein we describe the design and implementation of the system in light of these challenges.},
	address = {New York, NY, USA},
	author = {Del Pino, Benjam\'{\i}n and Hogan, Aidan},
	booktitle = {Companion Proceedings of the ACM Web Conference 2023},
	doi = {10.1145/3543873.3587356},
	isbn = {9781450394192},
	keywords = {Wikidata, geographic data, user interfaces},
	location = {<conf-loc>, <city>Austin</city>, <state>TX</state>, <country>USA</country>, </conf-loc>},
	numpages = {4},
	pages = {238–241},
	publisher = {Association for Computing Machinery},
	series = {WWW '23 Companion},
	title = {Wikidata Atlas: Putting Wikidata on the Map},
	url = {https://doi.org/10.1145/3543873.3587356},
	year = {2023},
}

@inproceedings{10.1145/3545862.3545894,
	abstract = {His Majesty King Bhumibol Adulyadej of Thailand initiated more than 2,500 projects throughout his 70 year reign. The Royal Development Projects Board (RDPB) gathers all information of projects spread all over the country, from north to south. Records of various projects might be kept in many sources; however, there is no organizational collection of information that is spatial-based. This research aims to develop an online web spatial database, which manage information (i.e., project name, covered areas, responsibility agencies) systematically based on geographic locations. The research process was comprised of data requested from the office of the RDPB, organizing it in a format used for further geospatial web development, as well as locating the projects’ location and designing a map display. The process discovered that the geocoding function embedded in a geographic information system (GIS) software (i.e., ArcGIS or QGIS) could not identify the locations in Thailand; therefore, written programming was used to search for the location in Google, which worked faster. Since there are 2,738 projects, mapping should consider how quantitative data relates in a spatial form with map communication, aesthetics, and sufficient information. Online data could be accessed anywhere, anytime, and at any place, and users can select their interest area/project based on places and project types, which is more flexible compared to information in printed text formats. The outcome web-based will help supplement the existing information to make it clearer and easier to understand. Additionally, the virtual learning platform was developed to attract young generations, and ultimately to communicate and ensure the facts are distributed in more attractive and creative manner.},
	address = {New York, NY, USA},
	author = {Limlahapun, Ponthip and Jongkroy, Puntip},
	booktitle = {Proceedings of the 8th International Conference on Frontiers of Educational Technologies},
	doi = {10.1145/3545862.3545894},
	isbn = {9781450396400},
	keywords = {virtual learning platform, geospatial web-based system, geolocation},
	location = {Yokohama, Japan},
	numpages = {9},
	pages = {201–209},
	publisher = {Association for Computing Machinery},
	series = {ICFET '22},
	title = {Development web spatial database system of the Royal Projects under the Inspiration of King Rama 9th},
	url = {https://doi.org/10.1145/3545862.3545894},
	year = {2022},
}

@inproceedings{10.1145/3546632.3546890,
	abstract = {In order to visualize the spread of infectious diseases, this paper provides some references and suggestions for the prevention and control of infectious diseases. In this paper, the epidemic data, spatiotemporal individual trajectory data and individual contact network based on subway IC card and mobile phone signaling data are visualized and analyzed, and the classic infectious disease dynamics model SEIRS model has been expanded and improved to make it more realistic. On this basis, a web-side application is developed. The system uses B/S (Browser/Server) architecture for development. Through HTML, CSS, JavaScript integration library Echarts, Ajax and other technologies. Using this system, users can interact, explore and view the development of infectious diseases interactively. This paper provides a new solution and research ideas for the prevention and control of infectious diseases.},
	address = {New York, NY, USA},
	author = {Zhen, Zhen and Ma, Zhanwu and Wang, Yuhui},
	booktitle = {Proceedings of the 2022 International Conference on Computational Infrastructure and Urban Planning},
	doi = {10.1145/3546632.3546890},
	isbn = {9781450396363},
	keywords = {data visualization, data analysis B/S, Spatiotemporal contact network, SSM},
	location = {Nanchang, China},
	numpages = {5},
	pages = {119–123},
	publisher = {Association for Computing Machinery},
	series = {CIUP '22},
	title = {Design of a Visualization System for Infectious Disease Transmission and Control Based on Multi-source Spatio-Temporal Data},
	url = {https://doi.org/10.1145/3546632.3546890},
	year = {2022},
}

@inproceedings{10.1145/3549555.3549589,
	abstract = {In this paper, we study the problem of urban image geo-localization, where the aim is to estimate the real-world location in which an image was taken. Among the previous approaches to this task, we note three distinct categories: one only analyzes metadata; the other only analyzes the image content; and the third combines the two. However, most previous approaches require large annotated collections of images or their metadata. Instead of relying on large collections of images, we propose to use publicly available geographical (GIS) data, which contains information about urban objects in public spaces, as a backbone database to query images against. We argue that images can be effectively represented by the objects they contain, and that the spatial geometry of a scene—i.e., the positioning of these objects relative to each other—can function as a unique identifier for a particular physical location. Our experiments demonstrate the potential of using open GIS data for precise image geolocation estimation and serve as a baseline for future research in multimedia geo-localization.},
	address = {New York, NY, USA},
	author = {Glistrup, Mathias and Rudinac, Stevan and J\'{o}nsson, Bj\"{o}rn \TH{}\'{o}r},
	booktitle = {Proceedings of the 19th International Conference on Content-Based Multimedia Indexing},
	doi = {10.1145/3549555.3549589},
	isbn = {9781450397209},
	keywords = {urban multimedia data, multimedia retrieval, image geo-localization},
	location = {Graz, Austria},
	numpages = {7},
	pages = {50–56},
	publisher = {Association for Computing Machinery},
	series = {CBMI '22},
	title = {Urban Image Geo-Localization Using Open Data on Public Spaces},
	url = {https://doi.org/10.1145/3549555.3549589},
	year = {2022},
}

@article{10.1145/3554981,
	abstract = {With the wide use of Location-Based Social Networks (LBSNs), predicting user friendship from online social relations and offline trajectory data is of great value to improve the platform service quality and user satisfaction. Existing methods mainly focus on some hand-crafted features or graph embedding models based on the user-location bipartite graph, which cannot precisely capture the latent mobility similarity for the majority of users who have no explicit co-visit behaviors and also fail to balance the tradeoff between social features and mobility features for friendship prediction. In this regard, we propose a dual subgraph-based pairwise graph neural network (DSGNN) for friendship prediction in LBSNs, which extracts a pairwise social subgraph and a trajectory subgraph to model the social proximity and mobility similarity, respectively. Specifically, to overcome the co-visit data sparsity, we design an entropy-based random walk to construct a location graph that captures the high-level correlation between locations. Based on this, we characterize the pairwise mobility similarity from trajectory level instead of location level, which is modeled by a graph neural network (GNN) on a labeled trajectory subgraph composed of the two trajectories of the target user pair. Besides, we also utilize another GNN to extract social proximity based on social subgraph of the target user pair. Finally, we propose a gate layer to adaptively balance the fusion of the social and mobility features for friendship prediction. We conduct extensive experiments on the real-world datasets and demonstrate the superiority of our approach, which outperforms other state-of-the-art methods. In particular, the comparative experiments on the trajectory level mobility similarity further validate the effectiveness of the designed trajectory subgraph-based method, which can extract predictive mobility features.},
	address = {New York, NY, USA},
	articleno = {42},
	author = {Wei, Xuemei and Liu, Yezheng and Sun, Jianshan and Jiang, Yuanchun and Tang, Qifeng and Yuan, Kun},
	doi = {10.1145/3554981},
	issn = {1556-4681},
	issue_date = {April 2023},
	journal = {ACM Trans. Knowl. Discov. Data},
	keywords = {Location-based social network, friendship prediction, mobility, random walk, graph neural network},
	month = {feb},
	number = {3},
	numpages = {28},
	publisher = {Association for Computing Machinery},
	title = {Dual Subgraph-Based Graph Neural Network for Friendship Prediction in Location-Based Social Networks},
	url = {https://doi.org/10.1145/3554981},
	volume = {17},
	year = {2023},
}

@inproceedings{10.1145/3555041.3589401,
	abstract = {Geospatial analytics is an important field in many communities, including crime science, transportation science, epidemiology, ecology, and urban planning. However, with the rapid growth of big geospatial data, most of the commonly used geospatial analytic tools are not efficient (or even feasible) to support large-scale datasets. As such, domain experts have raised the concerns about the inefficiency issues for using these tools. In this tutorial, we aim to arouse the attention of database researchers for this important, emerging, database-related, and interdisciplinary topic, which consists of four parts. In the first part, we will discuss different problems and highlight the challenges for two types of geospatial analytic tools, which are (1) hotspot detection and (2) correlation analysis. In the second and third parts, we will specifically discuss two geospatial analytic tools, namely kernel density visualization (the representative hotspot detection method) and K-function (the representative correlation analysis method), respectively, and their variants. In the fourth part, we will highlight the future opportunities for this topic.},
	address = {New York, NY, USA},
	author = {Chan, Tsz Nam and U, Leong Hou and Choi, Byron and Xu, Jianliang and Cheng, Reynold},
	booktitle = {Companion of the 2023 International Conference on Management of Data},
	doi = {10.1145/3555041.3589401},
	isbn = {9781450395076},
	keywords = {GIS, efficient algorithm and software development, geospatial analytics, k-function, kernel density visualization},
	location = {Seattle, WA, USA},
	numpages = {9},
	pages = {21–29},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '23},
	title = {Large-scale Geospatial Analytics: Problems, Challenges, and Opportunities},
	url = {https://doi.org/10.1145/3555041.3589401},
	year = {2023},
}

@inproceedings{10.1145/3555041.3589711,
	abstract = {Network kernel density visualization (NKDV) is an important tool for many application domains, including criminology and transportation science. However, all existing software tools, e.g., SANET (a plug-in for QGIS and ArcGIS) and spNetwork (an R package), adopt the na\"{\i}ve implementation of NKDV, which does not scale to large-scale location datasets and high-resolution sizes. To overcome this issue, we develop the first python library, called PyNKDV, which adopts our complexity-reduced solution and its parallel implementation to significantly improve the efficiency for generating NKDV. Moreover, PyNKDV is also user friendly (with four lines of python code) and can support commonly used geospatial analytic systems (e.g., QGIS and ArcGIS). In this demonstration, we will use three large-scale location datasets (up to 7.71 million data points), provide different python scripts (in the Jupyter Notebook), and install existing software tools (i.e., SANET and spNetwork) for participants to (1) explore different functionalities of our PyNKDV library and (2) compare its practical efficiency with existing software tools.},
	address = {New York, NY, USA},
	author = {Chan, Tsz Nam and Zang, Rui and Ip, Pak Lon and U, Leong Hou and Xu, Jianliang},
	booktitle = {Companion of the 2023 International Conference on Management of Data},
	doi = {10.1145/3555041.3589711},
	isbn = {9781450395076},
	keywords = {geospatial analytic systems, nkdv, python library},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {99–102},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '23},
	title = {PyNKDV: An Efficient Network Kernel Density Visualization Library for Geospatial Analytic Systems},
	url = {https://doi.org/10.1145/3555041.3589711},
	year = {2023},
}

@inproceedings{10.1145/3555041.3589734,
	abstract = {This paper demonstrates GeoTorchAI, a spatiotemporal deep learning framework. In recent years, many neural network models have been proposed focusing on the applications of raster imagery and spatiotemporal non-imagery datasets. Implementing these models using existing deep learning frameworks, such as PyTorch and TensorFlow, requires nontrivial coding efforts from the developers because these models differ extensively from state-of-the-art models supported by existing deep learning frameworks. Moreover, existing deep learning frameworks lack the support for scalable data preprocessing, a mandatory step for converting spatiotemporal datasets into trainable tensors. GeoTorchAI enables machine learning practitioners to implement spatiotemporal deep learning models with minimum coding efforts on top of PyTorch. It provides state-of-the-art neural network models, ready-to-use benchmark datasets, and transformation operations for raster imagery and spatiotemporal non-imagery datasets. Besides deep learning, GeoTorchAI contains a data preprocessing module that allows preparing trainable spatiotemporal vector datasets and the transformation of raster images in a cluster computing setting.},
	address = {New York, NY, USA},
	author = {Chowdhury, Kanchan and Sarwat, Mohamed},
	booktitle = {Companion of the 2023 International Conference on Management of Data},
	doi = {10.1145/3555041.3589734},
	isbn = {9781450395076},
	keywords = {apache spark, satellite images, spatiotemporal deep learning},
	location = {Seattle, WA, USA},
	numpages = {4},
	pages = {195–198},
	publisher = {Association for Computing Machinery},
	series = {SIGMOD '23},
	title = {A Demonstration of GeoTorchAI: A Spatiotemporal Deep Learning Framework},
	url = {https://doi.org/10.1145/3555041.3589734},
	year = {2023},
}

@article{10.1145/3555310,
	abstract = {COVID-19 outbreak was declared a pandemic by the World Health Organization on March 11, 2020. To minimize casualties and the impact on the economy, various mitigation measures have being employed with the purpose to slow the spread of the infection, such as complete lockdown, social distancing, and random testing. The key contribution of this article is twofold. First, we present a novel extended spatially informed epidemic model, SIRTEM, Spatially Informed Rapid Testing for Epidemic Modeling and Response to COVID-19, that integrates a multi-modal testing strategy considering test accuracies. Our second contribution is an optimization model to provide a cost-effective testing strategy when multiple test types are available. The developed optimization model incorporates realistic spatially based constraints, such as testing capacity and hospital bed limitation as well.},
	address = {New York, NY, USA},
	articleno = {29},
	author = {Azad, Fahim Tasneema and Dodge, Robert W. and Varghese, Allen M. and Lee, Jaejin and Pedrielli, Giulia and Candan, K. Sel\c{c}uk and Chowell-Puente, Gerardo},
	doi = {10.1145/3555310},
	issn = {2374-0353},
	issue_date = {December 2022},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {multi-city mixing, multi-accuracy testing, COVID-19},
	month = {nov},
	number = {4},
	numpages = {43},
	publisher = {Association for Computing Machinery},
	title = {SIRTEM: Spatially Informed Rapid Testing for Epidemic Modeling and Response to COVID-19},
	url = {https://doi.org/10.1145/3555310},
	volume = {8},
	year = {2022},
}

@inproceedings{10.1145/3555776.3577832,
	abstract = {The massive use of personal location devices, the Internet of Mobile Things, and Location Based Social Networks, enables the collection of vast amounts of movement data. Such data can be enriched with several semantic dimensions (or aspects), i.e., contextual and heterogeneous information captured in the surrounding environment, leading to the creation of multiple aspect trajectories (MATs). In this work, we present how the MAT-Builder system can be used for the semantic enrichment processing of movement data while being agnostic to aspects and external semantic data sources. This is achieved by integrating MAT-Builder into a methodology which encompasses three design principles and a uniform representation formalism for enriched data based on the Resource Description Framework (RDF) format. An example scenario involving the generation and querying of a dataset of MATs gives a glimpse of the possibilities that our methodology can open up.},
	address = {New York, NY, USA},
	author = {Lettich, Francesco and Pugliese, Chiara and Renso, Chiara and Pinelli, Fabio},
	booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
	doi = {10.1145/3555776.3577832},
	isbn = {9781450395175},
	keywords = {trajectory enrichment, semantic enrichment, multiple aspect trajectory, knowledge graph},
	location = {Tallinn, Estonia},
	numpages = {3},
	pages = {515–517},
	publisher = {Association for Computing Machinery},
	series = {SAC '23},
	title = {A general methodology for building multiple aspect trajectories},
	url = {https://doi.org/10.1145/3555776.3577832},
	year = {2023},
}

@inproceedings{10.1145/3557738.3557948,
	abstract = {Previous research revealed spatial interactions between several public facilities and campus locations. Therefore, it is likely that a campus development in a location will encourage the growth of various supporting facilities in that location. Thus, the question arises: Do the campus's existence and various public facilities around it encourage changes in land use in the area? The objective of this study is to explore land-use changes in the areas around education facilities. This study used a Geographical Information System (GIS) to process and analyze data, including the projection system setup, digitizing, editing, and visualizing. The results of the land use analysis for several consecutive years show a tendency to increase the built-up area category and decrease the cultivated land category. These findings provide insight into the questions underlying this research. These findings provide insight into the questions underlying this research and encourage further research on how to determine the appropriate location for developing various supporting facilities by understanding the environmental transformation of the existing urban campus. The relationship between campus locations and neighborhood characteristics, such as demographics and income, also requires further research.},
	address = {New York, NY, USA},
	articleno = {68},
	author = {Widaningrum, Dyah Lestari},
	booktitle = {Proceedings of the 2022 International Conference on Engineering and Information Technology for Sustainable Industry},
	doi = {10.1145/3557738.3557948},
	isbn = {9781450397186},
	keywords = {Urban campus, Land-use, Geographical Information System, Facility location},
	location = {Alam Sutera, Tangerang, Indonesia},
	numpages = {6},
	publisher = {Association for Computing Machinery},
	series = {ICONETSI '22},
	title = {The effect of supporting facilities growth around the urban campus on land-use change},
	url = {https://doi.org/10.1145/3557738.3557948},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560936,
	abstract = {Spatial join is an important operation for combining spatial data. Parallelization is essential for improving spatial join performance. However, load imbalance due to data skew limits the scalability of parallel spatial join. There are many work sharing techniques to address this problem in a parallel environment. One of the techniques is to use data and space partitioning and then scheduling the partitions among threads/processes with the goal of minimizing workload differences across threads/processes. However, load imbalance still exists due to differences in join costs of different pairs of input geometries in the partitions.For the load imbalance problem, we have designed a work stealing spatial join system (WSSJ-DM) on a distributed memory environment. Work stealing is an approach for dynamic load balancing in which an idle processor steals computational tasks from other processors [5]. This is the first work that uses work stealing concept (instead of work sharing) to parallelize spatial join computation on a large compute cluster. We have evaluated the scalability of the system on shared and distributed memory. Our experimental evaluation shows that work stealing is an effective strategy. We compared WSSJ-DM with work sharing implementations of spatial join on a high performance computing environment using partitioned and un-partitioned datasets. Static and dynamic load balancing approaches were used for comparison. We study the effect of memory affinity in work stealing operations involved in spatial join on a multi-core processor.WSSJ-DM performed spatial join using ST_Intersection on Lakes (8.4M polygons) and Parks (10M polygons) in 30 seconds using 35 compute nodes on a cluster (1260 CPU cores). A work sharing Master-Worker implementation took 160 seconds in contrast.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Yang, Jie and Puri, Satish and Zhou, Hui},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560936},
	isbn = {9781450395298},
	keywords = {high performance computing, distributed computing, NUMA, MPI},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Fine-grained dynamic load balancing in spatial join by work stealing on distributed memory},
	url = {https://doi.org/10.1145/3557915.3560936},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560945,
	abstract = {Simulations of future urban form (road networks, land use type, building density and building type) are needed to provide greater clarity of future urban flooding dynamics. This paper proposes an interdisciplinary and novel geospatial approach involving advanced geosimulations, artificial intelligence algorithms and hydrodynamic modelling to assess how flood risk is exacerbated under different urban form scenarios. It is envisioned that the final output will have a pivotal impact on urban growth modelling research and enhance community-level knowledge and resilience to urban flooding.},
	address = {New York, NY, USA},
	articleno = {11},
	author = {Burke, Richard and Xing, Jin and Ford, Alistair and Dawson, Richard},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560945},
	isbn = {9781450395298},
	keywords = {urban form, simulation, hydrodynamic modelling, geospatial data, geosimulation, framework, flood risk, artificial intelligence},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {A geospatial modelling framework to assess flood risk under future scenarios of urban form (vision paper)},
	url = {https://doi.org/10.1145/3557915.3560945},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560961,
	abstract = {This paper is motivated by a practical problem: many U.S. states have public hearings on "communities of interest" as part of their redistricting process, but no state has as yet adopted a concrete method of spatializing and aggregating community maps in order to take them into account in the drawing of new boundaries for electoral districts. Below, we describe a year-long project that collected and synthesized thousands of community maps through partnerships with grassroots organizations and/or government offices. The submissions were then aggregated by geographical clustering with a modified Hausdorff distance; then, the text from the narrative submissions was classified with semantic labels so that short runs of a Markov chain could be used to form semantic sub-clusters. The resulting dataset is publicly available, including the raw data of submitted community maps as well as post-processed community clusters and a scoring system for measuring how well districting plans respect the clusters. We provide a discussion of the strengths and weaknesses of this methodology and conclude with proposed directions for future work.},
	address = {New York, NY, USA},
	articleno = {27},
	author = {Chambers, Erin and Duchin, Moon and Edmonds, Ranthony A. C. and Edwards, Parker and Matthews, J N and Pizzimenti, Anthony E. and Richardson, Chanel and Rule, Parker and Stern, Ari},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560961},
	isbn = {9781450395298},
	keywords = {semantic classification, regionalization, redistricting, geospatial data, clustering},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Aggregating community maps},
	url = {https://doi.org/10.1145/3557915.3560961},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560962,
	abstract = {Accurate and timely mapping of flood extent plays a crucial role in disaster management such as damage assessment and relief activities. In recent years, high-resolution optical imagery becomes increasingly available with the wide deployment of satellites and drones. However, analyzing such imagery data to extract flood extent poses unique challenges due to noises such as obstacles (e.g., tree canopies, clouds). In this paper, we propose an elevation-guided annotation tool for flood extent mapping, which allows annotators to provide the flooded/dry labels for just a few pixels to cover a large area where the labels of most other pixels are automatically inferred. The physical rule we use here to guide the automatic label inference is that if a location is flooded (resp. dry), then its adjacent locations with a lower (resp. higher) elevation must also be flooded (resp. dry). In this way, annotators just need to label the pixels that they are confident with, and the true labels of many ambiguous pixels such as tree-canopy ones can be automatically inferred. We demonstrate the usage of our annotation tool using high-resolution aerial imagery from National Oceanic and Atmospheric Administration (NOAA) National Geodetic Survey (NGS) together with the corresponding Digital Elevation Model (DEM) data. The annotated data can be used to train machine learning models for flood extent mapping, and we train U-Net models to infer the flood map for an unseen region and achieve a high accuracy. Our annotation tool is open-sourced at https://github.com/SaugatAdhikari/Flood-Annotation-Tool.},
	address = {New York, NY, USA},
	articleno = {28},
	author = {Adhikari, Saugat and Yan, Da and Sami, Mirza Tanzim and Khalil, Jalal and Yuan, Lyuheng and Joy, Bhadhan Roy and Jiang, Zhe and Sainju, Arpan Man},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560962},
	isbn = {9781450395298},
	keywords = {flood mapping, earth imagery, annotation, U-Net, DEM},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {An elevation-guided annotation tool for flood extent mapping on earth imagery (demo paper)},
	url = {https://doi.org/10.1145/3557915.3560962},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560963,
	abstract = {App-based ride-hailing services, such as Uber and Lyft, have become popular thanks to technology advancements including smartphones and 4G/5G network. However, little is known about to what degree their operations impact urban traffic since Transportation Network Companies seldom share their ride data due to business and user privacy reasons. Recently, transportation engineering researchers began to collect data in large cities trying to understand the transportation impacts of ride-hailing services, so as to assist transport planning and policy making. However, (1) there does not exist a general data collection approach applicable to any city, and (2) the studies were based on historical data and cannot project the future easily even though ride-hailing services are developing quickly.In this paper, we introduce our approach to building a digital twin of the transportation in the medium-sized city of Birmingham, Alabama. This digital twin is a transportation simulation model that incorporates transportation modes such as public transits and ride-hailing services, in addition to private vehicles that constitute the majority of Birmingham's traffic. With this digital twin, transportation engineers can flexibly analyze the impact of ride-hailing services under different scenarios, such as "if the number of Uber drivers doubles" which could happen in the near future. The digital twin may also enable new opportunities, such as being an environment for learning policies with reinforcement learning.To enable realistic transportation simulation, we propose a novel approach to collect Uber ride data that is easy to carry out in any city. This approach collects app screenshots about Uber rides from Uber drivers, and uses crowdsourcing to postprocess these screenshots to extract detailed ride information. We then fit a spatiotemporal distribution of Uber rides using the collected data via network kernel density estimation (KDE). The existing network KDE method is flawed in that the contributions of different data samples are not the same, so we propose a new formulation to solve this issue. The distribution combined with population statistics from census data enable the generation of realistic Uber rides for agent-based simulation. Our project is shared at https://github.com/jalal1/UberSim.},
	address = {New York, NY, USA},
	articleno = {29},
	author = {Khalil, Jalal and Yan, Da and Yuan, Lyuheng and Jafarzadehfadaki, Mostafa and Adhikari, Saugat and Sisiopiku, Virginia P. and Jiang, Zhe},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560963},
	isbn = {9781450395298},
	keywords = {simulation, ride-hailing, map matching, kernel density estimation},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Realistic urban traffic simulation with ride-hailing services: a revisit to network kernel density estimation (systems paper)},
	url = {https://doi.org/10.1145/3557915.3560963},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560964,
	abstract = {Recent advances in location-aware devices have resulted in an exponential increase in the trajectory data streams. A number of applications require real-time processing and analysis of massive moving objects' trajectories. For instance, route guidance in emergency evacuation, patients tracking, etc. Existing scalable trajectory management systems lack support for real-time processing, while the real-time systems do not natively support spatial trajectory processing. This work presents TStream, a real-time and scalable trajectory stream processing and analysis framework. TStream utilizes grid index to support efficient processing of continuous range, kNN and join queries.},
	address = {New York, NY, USA},
	articleno = {30},
	author = {Shaikh, Salman Ahmed and Kitagawa, Hiroyuki and Matono, Akiyoshi and Kim, Kyoung-sook},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560964},
	isbn = {9781450395298},
	keywords = {trajectory processing, spatial streams, real-time queries, TStream},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {TStream: a framework for real-time and scalable trajectory stream processing and analysis},
	url = {https://doi.org/10.1145/3557915.3560964},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560967,
	abstract = {While the electric vehicle (EV) industry is facing some challenges concerning its refueling, its rapid growth in popularity is increasing these difficulties. In this paper, we demonstrate the gravity of the problems that EVs may experience for charging,both now and in the near future, and show how establishing new charging stations can be challenging. We also present the challenges in optimizing the use of charging stations by EV users. Then, we envisage opportunities for the rise of alternative charging options, such as distributed generation, crowdsourced, wireless and mobile charging stations. Additionally, we explain directions on how route and charging stations' location planning can cater to optimizing the charging infrastructure.},
	address = {New York, NY, USA},
	articleno = {32},
	author = {Basharzad, Saeed Nasehi and Choudhury, Farhana M. and Tanin, Egemen and Andrew, Lachlan L. H. and Samet, Hanan and Sarvi, Majid},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560967},
	isbn = {9781450395298},
	keywords = {route planning, facility location problem, electric vehicles},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Electric vehicle charging: it is not as simple as charging a smartphone (vision paper)},
	url = {https://doi.org/10.1145/3557915.3560967},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560968,
	abstract = {The ever increasing amount of GPS-equipped vehicles provides in real-time valuable traffic information for the roads traversed by the moving vehicles. In this way, a set of sparse and time evolving traffic reports is generated for each road. These time series are a valuable asset in order to forecast the future traffic condition. In this paper we present a deep learning framework that encodes the sparse recent traffic information and forecasts the future traffic condition. Our framework consists of a recurrent part and a decoder. The recurrent part employs an attention mechanism that encodes the traffic reports that are available at a particular time window. The decoder is responsible to forecast the future traffic condition.},
	address = {New York, NY, USA},
	articleno = {33},
	author = {Zygouras, Nikolaos and Gunopulos, Dimitrios},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560968},
	isbn = {9781450395298},
	keywords = {travel time estimation, transformer, traffic forecasting, mining mobility data, deep learning, GPS trajectories},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {A novel framework for handling sparse data in traffic forecast},
	url = {https://doi.org/10.1145/3557915.3560968},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560978,
	abstract = {The recent proliferation of real-world human mobility datasets has catalyzed geospatial and transportation research in trajectory prediction, demand forecasting, travel time estimation, and anomaly detection. However, these datasets also enable, more broadly, a descriptive analysis of intricate systems of human mobility. We formally define patterns of life analysis as a natural, explainable extension of online unsupervised anomaly detection, where we not only monitor a data stream for anomalies but also explicitly extract normal patterns over time. To learn patterns of life, we adapt Grow When Required (GWR) episodic memory from research in computational biology and neurorobotics to a new domain of geospatial analysis. This biologically-inspired neural network, related to self-organizing maps (SOM), constructs a set of "memories" or prototype traffic patterns incrementally as it iterates over the GPS stream. It then compares each new observation to its prior experiences, inducing an online, unsupervised clustering and anomaly detection on the data. We mine patterns-of-interest from the Porto taxi dataset, including both major public holidays and newly-discovered transportation anomalies, such as festivals and concerts which, to our knowledge, have not been previously acknowledged or reported in prior work. We anticipate that the capability to incrementally learn normal and abnormal road transportation behavior will be useful in many domains, including smart cities, autonomous vehicles, and urban planning and management.},
	address = {New York, NY, USA},
	articleno = {43},
	author = {Tenzer, Mark and Rasheed, Zeeshan and Shafique, Khurram},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560978},
	isbn = {9781450395298},
	keywords = {self-organizing feature maps, patterns of life, geospatial analysis, biological neural networks, anomaly detection},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Learning citywide patterns of life from trajectory monitoring},
	url = {https://doi.org/10.1145/3557915.3560978},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560980,
	abstract = {A need to understand and predict vehicles' behavior underlies both public and private goals in the transportation domain, including urban planning and management, ride-sharing services, and intelligent transportation systems. Individuals' preferences and intended destinations vary throughout the day, week, and year: for example, bars are most popular in the evenings, and beaches are most popular in the summer. Despite this principle, we note that recent studies on a popular benchmark dataset from Porto, Portugal have found, at best, only marginal improvements in predictive performance from incorporating temporal information. We propose an approach based on hypernetworks, a variant of meta-learning ("learning to learn") in which a neural network learns to change its own weights in response to an input. In our case, the weights responsible for destination prediction vary with the metadata, in particular the time, of the input trajectory. The time-conditioned weights notably improve the model's error relative to ablation studies and comparable prior work, and we confirm our hypothesis that knowledge of time should improve prediction of a vehicle's intended destination.},
	address = {New York, NY, USA},
	articleno = {45},
	author = {Tenzer, Mark and Rasheed, Zeeshan and Shafique, Khurram and Vasconcelos, Nuno},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560980},
	isbn = {9781450395298},
	keywords = {transportation, temporal reasoning, hyper-network, destination prediction},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Meta-learning over time for destination prediction tasks},
	url = {https://doi.org/10.1145/3557915.3560980},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560982,
	abstract = {Many online trip planning and navigation software need to routinely solve the problem of deciding where to take stops during a journey for various services such as refueling (or EV charging), rest stops, food, etc. The goal is to minimize the overhead of these stops while ensuring that the traveller is not starved of any essential resource (such as fuel, rest, or food) during the journey. In this paper, we formally model this problem and call it the pit stop problem. We design algorithms for this problem under various settings: single vs multiple types of stops, and offline vs online optimization (i.e., in advance of or during the trip). Our algorithms achieve provable guarantees in terms of approximating the optimal solution. We then extensively evaluate our algorithms on real world data and demonstrate that they significantly outperform baseline solutions.},
	address = {New York, NY, USA},
	articleno = {47},
	author = {Gollapudi, Sreenivas and Kollias, Kostas and Panigrahi, Debmalya},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560982},
	isbn = {9781450395298},
	keywords = {online algorithms, electric vehicle routing},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {The pit stop problem: how to plan your next road trip},
	url = {https://doi.org/10.1145/3557915.3560982},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560987,
	abstract = {In this paper, we consider the problem of gradual road network simplification, where given an embedded road network the goal is to compute a fine-grained succession of simplifications with decreasing level-of-detail. This allows to render the network on any desired zoom level or with a user-defined number of segments. Previous work has established that this can be achieved based on a Contraction Hierarchies (CH) data structure. CH was originally developed as a graph preprocessing method to speed up shortest path planning in road networks. However, since it is inherently based on a graph simplification mechanism, it can also serve as a basis for rendering. But the existing method exhibits several shortcomings, for example, topological inconsistencies arise on many simplification levels and the preservation of the shape of routes and the overall network for coarser graph representations is insufficient. This severely impairs the navigability of the map. We significantly improve upon the existing method by modifying the CH construction process as well as the rendering algorithm.},
	address = {New York, NY, USA},
	articleno = {52},
	author = {Baur, Lukas and Funke, Stefan and Rupp, Tobias and Storandt, Sabine},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560987},
	isbn = {9781450395298},
	keywords = {shortest paths, map rendering, graph visualization},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Gradual road network simplification with shape and topology preservation},
	url = {https://doi.org/10.1145/3557915.3560987},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560989,
	abstract = {We present RODGEN, an interactive, graphical user interface for generating road networks that adopts the growth-based model. The first step in the generation process is to construct the backbone of the network by either choosing between a grid-based and a ring-based predefined topology or allowing the users to define a custom one. The backbone divides the space into a number of areas, called neighborhoods. The user can populate neighborhoods either by importing existing road networks or adding roads by hand. Besides generating road networks, our interface also provides a platform for analysis. For this purpose, we employ a general-purpose graph analytics library, which allows the users to compute graph statistics, perform connectivity analysis and execute basic routing tasks.},
	address = {New York, NY, USA},
	articleno = {54},
	author = {Martinez, Claudia Perez and Bouros, Panagiotis and Chondrogiannis, Theodoros},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560989},
	isbn = {9781450395298},
	keywords = {user interface, road networks, graphs, generator},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {RODGEN: an interactive interface for road network generation},
	url = {https://doi.org/10.1145/3557915.3560989},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560990,
	abstract = {We present PATHFINDERVIS, a tool which is able to efficiently retrieve and visualize massive trajectory data on up to continent-sized networks. PATHFINDERVIS is an extension of the existing PATHFINDER framework which was designed as a pure backend and lacked a visualization. PATHFINDERVIS features many prototypes of different approaches to efficiently transmit and visualize large trajectory data.},
	address = {New York, NY, USA},
	articleno = {55},
	author = {Baur, Lukas and Funke, Stefan and Rupp, Tobias},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560990},
	isbn = {9781450395298},
	keywords = {visualization, trajectories, big data},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {PATHFINDERVIS},
	url = {https://doi.org/10.1145/3557915.3560990},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560991,
	abstract = {An important data source for traffic analysis is GPS trajectory data. However, due to measurement inaccuracies, such data does not necessarily align well with data describing the road network. Hence, GPS data typically needs to be aligned with the road network before further analysis can take place; this process is called map matching. The challenges in map matching are exacerbated when trajectories are sparse (have a low measurement frequency); we then need to fill the gaps between the measurements with a realistic estimate of the actual route between two points. As vehicle movement is subject to physical constraints (such as acceleration and speed bounds), an estimated route should be physically consistent. We present a method that creates physically consistent estimated routes to perform map matching for sparse trajectories.},
	address = {New York, NY, USA},
	articleno = {56},
	author = {Custers, Bram and Meulemans, Wouter and Roeloffzen, Marcel and Speckmann, Bettina and Verbeek, Kevin},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560991},
	isbn = {9781450395298},
	keywords = {physical consistency, map matching, GPS trajectories},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Physically consistent map matching},
	url = {https://doi.org/10.1145/3557915.3560991},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560992,
	abstract = {The touring problem aims to find an "interesting" (round) trip of a given length. Here, what is considered interesting depends on the type of the desired route, e.g., a user may be looking for an off-road cycling trip or fast running route.This problem can be modeled as an optimization problem on a graph and has various algorithmic solutions. We provide a framework that allows for simple integration of different algorithms for the touring problem. In this demonstration we have included four algorithms based on greedy selection, shortest paths, local search, and an integer linear programming formulation. The generated tours can be explored in an easy-to-use web interface.},
	address = {New York, NY, USA},
	articleno = {57},
	author = {Buchin, Kevin and Hagedoorn, Mart and Li, Guangping},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560992},
	isbn = {9781450395298},
	keywords = {trip routing, touring problem, arc orienteering},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Tour4Me: a framework for customized tour planning algorithms},
	url = {https://doi.org/10.1145/3557915.3560992},
	year = {2022},
}

@inproceedings{10.1145/3557915.3560996,
	abstract = {Predicting the next visited location of an individual is a key problem in human mobility analysis, as it is required for the personalization and optimization of sustainable transport options. Here, we propose a transformer decoder-based neural network to predict the next location an individual will visit based on historical locations, time, and travel modes, which are behaviour dimensions often overlooked in previous work. In particular, the prediction of the next travel mode is designed as an auxiliary task to help guide the network's learning. For evaluation, we apply this approach to two large-scale and long-term GPS tracking datasets involving more than 600 individuals. Our experiments show that the proposed method significantly outperforms other state-of-the-art next location prediction methods by a large margin (8.05\% and 5.60\% relative increase in F1-score for the two datasets, respectively). We conduct an extensive ablation study that quantifies the influence of considering temporal features, travel mode information, and the auxiliary task on the prediction results. Moreover, we experimentally determine the performance upper bound when including the next mode prediction in our model. Finally, our analysis indicates that the performance of location prediction varies significantly with the chosen next travel mode by the individual. These results show potential for a more systematic consideration of additional dimensions of travel behaviour in human mobility prediction tasks. The source code of our model and experiments is available at https://github.com/mie-lab/location-mode-prediction.},
	address = {New York, NY, USA},
	articleno = {61},
	author = {Hong, Ye and Martin, Henry and Raubal, Martin},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3560996},
	isbn = {9781450395298},
	keywords = {travel behaviour, mobility, location prediction, deep learning},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {How do you go where? improving next location prediction by learning travel mode information using transformers},
	url = {https://doi.org/10.1145/3557915.3560996},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561002,
	abstract = {We introduce Zebra regression, a novel streaming regression task. A stream of periodically-created point particles travels along a finite, 1-dimensional trajectory, and we observe a stream of sporadic, noisy reports that estimate the location of some particle, but without exact information on which particle is being observed. We seek a streaming algorithm that maintains accurate estimates of the number and locations of the particles that are currently en route. Our motivation comes from locating "particles" that are public transit vehicles serving "frequency based" routes --- routes whose schedules announce an approximate frequency of departures, rather than explicit per-trip timings. We expect similar problems to also arise in other settings where observations are scarce and stochastic, and object identity is not readily available.In this setting, we delineate several practical sub-problems and demonstrate an algorithm capable of accurately inferring particle positions with up to 88\% of the performance accuracy of a hypothetical system with access to particle identities with each observation. The experiments are conducted on both self-synthesized settings and real-life data source of a public transit system of vehicles and their spatio-temporal records, where the inference made by the algorithm improves significantly over a state-of-the-art streaming algorithm for general trajectory clustering.},
	address = {New York, NY, USA},
	articleno = {67},
	author = {Wang, Yuyan and Buthpitiya, Senaka and Fabrikant, Alex},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561002},
	isbn = {9781450395298},
	keywords = {streaming regression, spatio-temporal data, regression, clustering},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Zebra regression: estimating motion of periodically-emitted particles from unlabeled sightings},
	url = {https://doi.org/10.1145/3557915.3561002},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561009,
	abstract = {Knowledge graphs are a rapidly growing paradigm and technology stack for integrating large-scale, heterogeneous data in an AI-ready form, i.e., combining data with the formal semantics required to understand it. However, toolchains that support data synthesis and knowledge discovery through information organization, search, filtering, and visualization have been developed at a pace lagging knowledge graph technology. In this paper, we present Knowledge Explorer, an open-source faceted search interface that provides environmentally intelligent services for interactively browsing and navigating KnowWhereGraph. Currently one of the largest open knowledge graphs, KnowWhereGraph contains over 12 billion statements with rich spatial and temporal information from more than 30 data layers. With an extensive collection of facets, Knowledge Explorer enables spatial, temporal, full-text, and expert search with dereferencing functionality to support "follow-your-nose" exploration, and it allows users to narrow their search by selecting facets. Given the size of the underlying graph and dependency on GeoSPARQL, we have improved query performance by implementing Elasticsearch indexing, spatial query generation, and caching. Knowledge Explorer is capable of retrieving information within seconds, answering a wide variety of competency questions posed by researchers, humanitarian relief organizations, and the broader public, thus helping better perform tasks such as cross-gazetteer place retrieval and disaster assessment from global to local geographic scales.},
	address = {New York, NY, USA},
	articleno = {73},
	author = {Liu, Zilong and Gu, Zhining and Thelen, Thomas and Estrecha, Seila Gonzalez and Zhu, Rui and Fisher, Colby K. and D'Onofrio, Anthony and Shimizu, Cogan and Janowicz, Krzysztof and Schildhauer, Mark and Stephen, Shirly and Rehberger, Dean and Li, Wenwen and Hitzler, Pascal},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561009},
	isbn = {9781450395298},
	keywords = {spatial query generation, knowledge graph, faceted search, environmental intelligence, KnowWhereGraph},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Knowledge explorer: exploring the 12-billion-statement KnowWhereGraph using faceted search (demo paper)},
	url = {https://doi.org/10.1145/3557915.3561009},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561011,
	abstract = {MP-regions is an NP-hard problem that groups spatial areas to produce a maximum number of regions by enforcing a user-defined constraint at the regional level. Existing approximate algorithms for MP-regions do not scale for large datasets due to their high computational cost. This paper introduces SMP; a scalable technique to support MP-regions on large datasets. SMP works on two stages. The first stage finds an initial solution through randomized search, and the second stage improves this solution through efficient heuristic search. SMP optimizes the region building efficiency and quality by tuning the randomized area selection to trade-off runtime with region homogeneity. The experimental evaluation shows the superiority of our technique to support an order of magnitude larger datasets efficiently compared to the state-of-the-art techniques while producing high-quality solutions.},
	address = {New York, NY, USA},
	articleno = {75},
	author = {Alrashid, Hussah and Liu, Yongyi and Magdy, Amr},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561011},
	isbn = {9781450395298},
	keywords = {spatial clustering, regionalization, max p-regions},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {SMP: scalable max-P regionalization},
	url = {https://doi.org/10.1145/3557915.3561011},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561015,
	abstract = {Location measurements from people and vehicles often have long temporal gaps between them. However, we would still like to reason about location behavior during these gaps. This paper presents a new method for filling these gaps that is both principled and data-driven. Unlike the most common method, linear interpolation, our method explicitly represents the location uncertainty in the gaps with probability. It also learns from actual mobility data. We introduce bridgelets, which are small, spatio-temporal, maximum entropy clouds that model spatial uncertainty over small gaps. Using actual trajectories, we combine bridgelets into probabilistic bridges that are specific to absolute start and end locations on the map. The resulting bridges give the probability of visiting certain in-between locations given only the start and end points. Using real trajectory data, we compare our maximum entropy bridges to a popular baseline to show how our approach is much more accurate.},
	address = {New York, NY, USA},
	articleno = {79},
	author = {Krumm, John},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561015},
	isbn = {9781450395298},
	keywords = {trajectory, interpolation, geospatial, bridge, GPS},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Maximum entropy bridgelets for trajectory completion},
	url = {https://doi.org/10.1145/3557915.3561015},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561017,
	abstract = {Nutritional security is a worldwide issue that hampers sustainable development. To achieve nutritional security in a sustainable manner, stakeholders and decision makers need to integrate and analyze comprehensive and heterogeneous data to evaluate the different pathways and tradeoffs of nutrition and sustainability. However, the integration of data is often challenging and may result in unreliable outputs that hinder sustainable land management and the implementation of sustainable agricultural practices that could substantially increase nutritional security in a local population. Hence, a simulation framework that integrates computational and spatially explicit methodologies is needed to promote strategic decision making by facilitating the evaluation of components and pathways that affect nutritional security. This paper develops the Sustainable Nutrition System (SNS) that overcomes the limitations of data integration and allows decision makers to easily create synthetic landscapes to test scenarios of land use change and crop production systems to increase nutritional security, while fostering sustainable agricultural practices in a given population.},
	address = {New York, NY, USA},
	articleno = {81},
	author = {Torres, Ang\'{e}lica Valencia and Tiwari, Chetan and Atkinson, Samuel F.},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561017},
	isbn = {9781450395298},
	keywords = {synthetic landscapes, simulation framework, decision support system, data integration},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {A simulation framework for evaluating strategies for sustainable nutritional security},
	url = {https://doi.org/10.1145/3557915.3561017},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561019,
	abstract = {Hierarchical graph-based approaches provide valuable abstractions for studying emerging topological structures and favour a better understanding of mobility patterns and similarities. This research introduces a hierarchical similarity measure that applies a series of distances toward a hierarchical graph-based model. This similarity measure is derived from different categories of distances and is experimented with evolving hierarchical graphs extracted from a European maritime mobility dataset.},
	address = {New York, NY, USA},
	articleno = {83},
	author = {Elayam, Maryam Maslek and Ray, Cyril and Claramunt, Christophe},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561019},
	isbn = {9781450395298},
	keywords = {similarity measure, maritime transportation network, knowledge extraction, dynamic hierarchical graph},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Towards a hierarchical similarity measure for studying dynamic hierarchical graphs},
	url = {https://doi.org/10.1145/3557915.3561019},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561020,
	abstract = {In this paper, we present a system to localize traffic signs with high accuracy in real-time at the edge using 3D reconstruction of an environment. We use Structure-from-Motion (SfM) based 3D reconstruction with only a small number of tracked feature points in video frames. The 3D model obtained from SfM alone has an arbitrary scale and orientation, which is not suitable for localizing traffic signs absolutely. Therefore, we use the GNSS locations of the images to scale and orient the 3D model with the real world. Next, we compute the latitude-longitude location of a traffic sign using the mapping between the traffic sign bounding box and corresponding 3D points in the 3D model. We also present the design of other relevant system components such as vehicle location, timing, data synchronization, and sensor calibration, which are required to achieve high accuracy localization. We evaluated our system in city, suburb, and highway scenes on meticulously annotated ground truth datasets. According to evaluation, our system achieves the median localization accuracy of 2.76 meters with high localization success rate and runs on average 15\texttimes{} faster than a comparable baseline system.},
	address = {New York, NY, USA},
	articleno = {84},
	author = {Musa, ABM},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561020},
	isbn = {9781450395298},
	keywords = {traffic sign, mapping, localization, 3D reconstruction},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Multi-view traffic sign localization with high absolute accuracy in real-time at the edge},
	url = {https://doi.org/10.1145/3557915.3561020},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561031,
	abstract = {Sub-meter accurate vehicle localization is often the ultimate goal in the automotive industry as well as fleet management today and the Global Navigation Satellite System (GNSS) is one of the most practical ways to achieve this goal. In addition to precise navigation and real-time accurate vehicle location tracking, the high-accuracy location data enables many geospatial and mapping applications. Traditionally, high-accuracy GNSS localization solutions were highly expensive. However, we can now achieve high accuracy localization by combining GNSS and Inertial Measurement Unit (IMU) along with Real-Time Kinematic (RTK) correction at a high availability with low cost. In this paper, we present the evaluation and experimental results of such a solution that we are deploying in a large vehicle fleet. Our experiments show that sub-meter localization accuracy can be achieved in most scenarios. Additionally, we present various geospatial applications of the high-accuracy GPS trajectories, some of which were previously impossible with the data collected from low-accuracy consumer-grade sensors. Finally, we share a dataset with multiple GPS trajectories containing GNSS data from both our low-cost and ground-truth GNSS systems. We believe this dataset will enable further research on various applications of high-accuracy location data as well as GNSS characteristics in different areas.},
	address = {New York, NY, USA},
	articleno = {95},
	author = {Musa, ABM and Baker, Chris and Eftelioglu, Emre and Chowdhury, Amber Roy},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561031},
	isbn = {9781450395298},
	keywords = {trajectory, spatiotemporal, location, GNSS},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {High-accuracy GNSS localization with low-cost},
	url = {https://doi.org/10.1145/3557915.3561031},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561033,
	abstract = {Given trajectories with gaps, we investigate methods to tighten spatial bounds on areas (e.g., nodes in a spatial network) where possible rendezvous activity could have occurred. The problem is important for reducing manual effort to post-process possible rendezvous areas using satellite imagery and has many societal applications to improve public safety, security, and health. The problem of rendezvous detection is challenging due to the difficulty of interpreting missing data within a trajectory gap and the very high cost of detecting gaps in such a large volume of location data. Most recent literature presents formal models, namely space-time prism, to track an object's rendezvous patterns within trajectory gaps on a spatial network. However, the bounds derived from the space-time prism are rather loose, resulting in unnecessarily extensive postprocessing manual effort. To address these limitations, we propose a Time Slicing-based Gap-Aware Rendezvous Detection (TGARD) algorithm to tighten the spatial bounds in spatial networks. We propose a Dual Convergence TGARD (DC-TGARD) algorithm to improve computational efficiency using a bi-directional pruning approach. Theoretical results show the proposed spatial bounds on the area of possible rendezvous are tighter than that from related work (space-time prism). Experimental results on synthetic and real-world spatial networks (e.g., road networks) show that the proposed DC-TGARD is more scalable than the TGARD algorithm.},
	address = {New York, NY, USA},
	articleno = {97},
	author = {Sharma, Arun and Gupta, Jayant and Ghosh, Subhankar},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561033},
	isbn = {9781450395298},
	keywords = {trajectory data mining, spatio-temporal data analysis, spatial modeling and reasoning},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Towards a tighter bound on possible-rendezvous areas: preliminary results},
	url = {https://doi.org/10.1145/3557915.3561033},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561037,
	abstract = {In this paper, we develop a set of normalized spatial temporal properties from which to assess road performance and detect abnormal traffic stream environments. Data is provided by non-instrumented retail vehicle telemetry data, which captures timestamps, GPS locations, and velocity provided every 3 seconds. Open Street Map (OSM) roadways are subdivided into 500-meter segments. Vehicles traveling across those segments are crowd-sensed every 15 minutes. The demonstration compares two traffic streams over the same segments and time periods exactly two weeks apart and illustrates the effectiveness of the approach. The novelty includes methods to create comparable spatial temporal frames and properties by transformations of telemetry data into driver experiences. The outcome of our approach generates a space time grid of cells from which to detect, describe, and diagnose traffic environments.},
	address = {New York, NY, USA},
	articleno = {101},
	author = {Gordon, Richard and Grimm, Donald K and Bai, Fan},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561037},
	isbn = {9781450395298},
	keywords = {traffic streams, traffic measurement, road performance, driving environments},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Crowd-sensing driving environments using headway dynamics (demo paper)},
	url = {https://doi.org/10.1145/3557915.3561037},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561038,
	abstract = {Modern data analytics applications prefer to use column-storage formats due to their improved storage efficiency through encoding and compression. Parquet is the most popular file format for column data storage that provides several of these benefits out of the box. However, geospatial data is not readily supported by Parquet. This paper introduces Spatial Parquet, a Parquet extension that efficiently supports geospatial data. Spatial Parquet inherits all the advantages of Parquet for non-spatial data, such as rich data types, compression, and column/row filtering. Additionally, it adds three new features to accommodate geospatial data. First, it introduces a geospatial data type that can encode all standard spatial geometries in a column format compatible with Parquet. Second, it adds a new lossless and efficient encoding method, termed FP-delta, that is customized to efficiently store geospatial coordinates stored in floating-point format. Third, it adds a light-weight spatial index that allows the reader to skip non-relevant parts of the file for increased read efficiency. Experiments on large-scale real data showed that Spatial Parquet can reduce the data size by a factor of three even without compression. Compression can further reduce the storage size. Additionally, Spatial Parquet can reduce the reading time by two orders of magnitude when the light-weight index is applied. This initial prototype can open new research directions to further improve geospatial data storage in column format.},
	address = {New York, NY, USA},
	articleno = {102},
	author = {Saeedan, Majid and Eldawy, Ahmed},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561038},
	isbn = {9781450395298},
	keywords = {parquet, geospatial, datasets, column store},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Spatial parquet: a column file format for geospatial data lakes},
	url = {https://doi.org/10.1145/3557915.3561038},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561042,
	abstract = {With the widespread use of mobile and sensing devices, there has been an explosion of high velocity, transient data having spatial and temporal characteristics. Interactive analysis at such scale and speed require support for highly efficient query processing backend frameworks coupled with lightweight yet powerful frontend interfaces. While existing in-memory distributed stream processing frameworks are perfect candidates for scalable big data processing, spatio-temporal systems in this domain are mostly dominated by specify-once-apply-continuously query model. Any modification in query state requires query restart limiting system responsiveness and producing outdated or in the worst case erroneous results. Furthermore, most of the contemporary spatio-temporal big data systems are designed to operate in a single execution environment limiting their applicability to users accustomed to other similar frameworks with different APIs. In this paper, we demon-strate SPEAR-Board; an interactive web-based interface integrated with cross-platform stream processing engine; SPEAR, capable of seamlessly handling spatio-temporal query state changes in real-time. We demonstrate working of SPEAR-Board with respect to spatio-temporal Range and Nearest Neighbor queries backed by Apache Spark and Apache Flink deployed over cloud resources.},
	address = {New York, NY, USA},
	articleno = {105},
	author = {Baig, Furqan and Nalluri, Pradeep and Kong, Jun and Wang, Fusheng},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561042},
	isbn = {9781450395298},
	keywords = {stream-processing, spatio-temporal-stream, spatio-temporal, spatial-stream, real-time-spatio-temporal, distributed-stream},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {SPEAR-board: cross-platform interactive spatio-temporal big data analytics},
	url = {https://doi.org/10.1145/3557915.3561042},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561470,
	abstract = {Spatio-temporal information is used for driving a plethora of intelligent transportation, smart-city and crowd-sensing applications. Data is now a valuable production factor and data marketplaces have appeared to help individuals and enterprises bring it to market and the ever-growing demand. Such marketplaces are able to combine data from different sources to meet the requirements of different applications. In this paper we study the problem of estimating the relative value of spatio-temporal datasets combined in marketplaces for predicting transportation demand and travel time in metropolitan areas. Using large datasets of taxi rides from Chicago, Porto and New York we show that simplistic but popular approaches for estimating the relative value of data, such as splitting it equally among the data sources, more complex ones based on volume or the "leave-one-out" heuristic, are inaccurate. Instead, more complex notions of value from economics and game-theory, such as the Shapley value, need to be employed if one wishes to capture the complex effects of mixing different datasets on the accuracy of forecasting algorithms. This does not seem to be a coincidental observation related to a particular use case but rather a general trend across different use cases with different objective functions.},
	address = {New York, NY, USA},
	articleno = {23},
	author = {Azcoitia, Santiago Andr\'{e}s and Paraschiv, Marius and Laoutaris, Nikolaos},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561470},
	isbn = {9781450395298},
	keywords = {data marketplaces, shapley value, spatio-temporal data, value of information},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Computing the relative value of spatio-temporal data in data marketplaces},
	url = {https://doi.org/10.1145/3557915.3561470},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561475,
	abstract = {Socio-demographic information is usually only accessible at relatively coarse spatial resolutions. However, its availability at thinner granularities is of substantial interest for several stakeholders, since it enhances the formulation of informed hypotheses on the distribution of population indicators. Spatial disaggregation methods aim to compute these fine-grained estimates, often using regression algorithms that employ ancillary data to re-distribute the aggregated information. However, since disaggregation tasks are ill-posed, and given that examples of disaggregated data at the target geospatial resolution are seldom available, model training is particularly challenging. We propose to address this problem through a self-supervision framework that iteratively refines initial estimates from seminal disaggregation heuristics. Specifically, we propose to co-train two different models, using the results from one model to train/refine the other. By doing so, we are able to explore complementary views from the data. We assessed the use of co-training with a fast regressor based on random forests that takes individual raster cells as input, together with a more expressive model, based on a fully-convolutional neural network, that takes raster patches as input. We also compared co-training against the use of self-training with a single model. In experiments involving the disaggregation of a socio-demographic variable collected for Continental Portugal, the results show that our co-training approach outperforms alternative disaggregation approaches, including methods based on self-training or co-training with two similar fully-convolutional models. Co-training is effective at exploring the characteristics of both regression algorithms, leading to a consistent improvement in different types of error metrics.},
	address = {New York, NY, USA},
	articleno = {91},
	author = {Monteiro, Jo\~{a}o and Martins, Bruno and Costa, Miguel and Pires, Jo\~{a}o M.},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561475},
	isbn = {9781450395298},
	keywords = {co-training, convolutional neural networks, dasymetric disaggregation, deep learning, encoder-decoder neural networks, geospatial data disaggregation, self-supervised learning},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {A co-training approach for spatial data disaggregation},
	url = {https://doi.org/10.1145/3557915.3561475},
	year = {2022},
}

@inproceedings{10.1145/3557915.3561479,
	abstract = {The recent waves of COVID-19 highlighted the importance of understanding and quantifying spatiotemporal interactions to infer, model, and predict disease spread in real time. In this demonstration paper, we present a robust infrastructure for interactive exploration of interregional and international spatiotemporal interactions via time-lagged correlations of increases in COVID-19 incidence. This infrastructure consists of: (i) an operational data store (ODS) coupled with automated scripts for downloading, cleaning, and processing data from heterogeneous sources; (ii) a server application handling on-demand analyses of the database data through a RESTful API; and (iii) a web application providing the interactive dashboard to explore various correlation and geostatistical metrics of the integrated data in spacetime. The environment allows users to study focal spatiotemporal trends and the potential of regions to export and import the virus. Moreover, the application has the potential to reveal the effect of the national border to mitigate the interaction, particularly the spread of the virus. The infrastructure serves COVID-19 data from Germany, Poland, and Czechia, with the possibility of extension to other regions and topics. The dashboard is under active development and accessible on www.where2test.de/correlation.},
	address = {New York, NY, USA},
	articleno = {99},
	author = {Mertel, Adam and Abdussalam, Wildan and Vysko\v{c}il, Ji\v{r}\'{\i} and Calabrese, Justin M.},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3561479},
	isbn = {9781450395298},
	keywords = {GIS infrastructure, regional interaction, spatial epidemiology, spatiotemporal data analysis, spatiotemporal exploration, visual analytics, web cartography},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {Infrastructure for spatiotemporal exploration of interregional and international interaction of epidemiological data (DEMO PAPER)},
	url = {https://doi.org/10.1145/3557915.3561479},
	year = {2022},
}

@inproceedings{10.1145/3557915.3565986,
	abstract = {We design a novel unsupervised approach to delineate building footprints on large-scale LiDAR point clouds. By computing an α-shape on low-height points, we delineate the building bottoms on the ground. We then use the terrain ruggedness index and vector ruggedness measurement on the entire points to find flat surface areas. Finally, valid building footprints are filtered by checking flat surfaces in the detected bottom areas. Compared to the Artificial Intelligence (AI)-assisted mapping results from Microsoft Building Footprints, the accuracy of the proposed method is 17\% higher in the test areas. The simple and effective pipeline makes the proposed method easy to use and suitable for a wider range of applications.},
	address = {New York, NY, USA},
	articleno = {120},
	author = {Xu, Xin},
	booktitle = {Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3557915.3565986},
	isbn = {9781450395298},
	keywords = {α-shape, LiDAR, building footprint, unsupervised learning},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '22},
	title = {An unsupervised building footprints delineation approach for large-scale LiDAR point clouds},
	url = {https://doi.org/10.1145/3557915.3565986},
	year = {2022},
}

@inproceedings{10.1145/3557916.3567820,
	abstract = {Understanding complex human mobility in the current digital era is very difficult, especially in highly populated metropolitan areas. The benefits of knowing urban mobility come in many ways and these include, but are not limited to, informing regional planners to better manage urban transportation, providing policy makers with provide reasonable strategies to reduce greenhouse gas (GHG) emissions, and assisting city officials to promote sustainable infrastructural plans, among many others. In this study, we explored the relationship between air quality data - particulate matter (PM) data (PM10 and PM2.5) and carbon monoxide (CO) - and the human mobility data in South Korea. This relationship is further implemented to an agent-based model (ABM) to replicate population mobility reflected based on these datasets in order to discover and simulate air quality with respect to human mobility.},
	address = {New York, NY, USA},
	author = {Borjigin, Sachraa G. and Park, Jongmin and Baik, Jongjin and Batterman, Stuart A.},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on Advances in Resilient and Intelligent Cities},
	doi = {10.1145/3557916.3567820},
	isbn = {9781450395304},
	keywords = {spatial analysis, human mobility, air quality, agent-based modeling},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {20–23},
	publisher = {Association for Computing Machinery},
	series = {ARIC '22},
	title = {Influence of air quality on human mobility: a case study using PM \&amp; CO data},
	url = {https://doi.org/10.1145/3557916.3567820},
	year = {2022},
}

@inproceedings{10.1145/3557917.3567615,
	abstract = {The last decade witnesses a fast development in geospatial application of artificial intelligence (GeoAI). However, due to the misalignment with wider computer science progresses, the geospatial community, for a long time, keeps working with powerful and over-sophisticated tools and software, whose functionality goes far beyond the actual basic need of GeoAI tasks. This fact, to a certain extent, hinders our steps towards establishing future sustainable and replicable GeoAI models. In this paper, we aim to address this challenge by introducing an efficient big data framework based on the modern HDF5 technology, called AtlasHDF, in which we designed lossless data mappings (immediate mapping and analysis-ready mapping) from OpenStreetMap (OSM) vector data into a single HDF5 data container to facilitate fast and flexible GeoAI applications learnt from OSM data. Since the HDF5 is included as a default dependency in most GeoAI and high performance computing (HPC) environments, the proposed AtlasHDF provides a cross-platformm and single-techonology solution of handling heterogeneous big geodata for GeoAI. As a case study, we conducted a comparative analysis of the AtlasHDF framework with three commonly-used data formats (i.e., PBF, Shapefile and GeoPackage) using the latest OSM data from the city of Berlin (Germany), then elaborated on the advantages of each data format w.r.t file size, querying, rending, dependency, data extendability. Given a wide range of GeoAI tasks that can potentially benefit from our framework, our future work will focus on extending the framework to heterogeneous big geodata (vector and raster) to support seamless and fast data integration without any geospatial software dependency until the training stage of GeoAI. A reference implementation of the framework developed in this paper is provided to the public at: https://github.com/tumbgd/hdf4water.},
	address = {New York, NY, USA},
	author = {Werner, Martin and Li, Hao},
	booktitle = {Proceedings of the 10th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3557917.3567615},
	isbn = {9781450395311},
	keywords = {immediate mapping, hierarchical data format, big data, OpenStreetMap, GeoAI},
	location = {Seattle, Washington},
	numpages = {7},
	pages = {1–7},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '22},
	title = {AtlasHDF: an efficient big data framework for GeoAI},
	url = {https://doi.org/10.1145/3557917.3567615},
	year = {2022},
}

@inproceedings{10.1145/3557917.3567617,
	abstract = {Accurate and rich representation of roads in a map is critical for safe and efficient navigation experience. Often, open source road data is incomplete and manually adding roads is labor intensive and consequently expensive. In this paper, we propose RING-Net, an approach for Road INference from GPS trajectories using a deep image segmentation Network. Previous work on road inference is either focused on satellite images or GPS trajectories, but they are not compatible with each other when there is a lack of high quality data from either of the source types. Even though it is primarily focused on using GPS trajectories as its input, RING-Net architecture is flexible enough to be used with multiple data sources with minimal effort. More specifically, RING-Net converts raw GPS trajectories into multi-band raster images with trip related features, and infers roads with high precision. Experiments on public data show that Ring-Net can be used to improve the completeness of a road network. Our approach is promising to bring us one step closer to fully automated map updates.},
	address = {New York, NY, USA},
	author = {Eftelioglu, Emre and Garg, Ravi and Kango, Vaibhav and Gohil, Chintan and Chowdhury, Amber Roy},
	booktitle = {Proceedings of the 10th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3557917.3567617},
	isbn = {9781450395311},
	keywords = {segmentation, road inference, deep learning, GPS trajectories},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {17–26},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '22},
	title = {RING-Net: road inference from GPS trajectories using a deep segmentation network},
	url = {https://doi.org/10.1145/3557917.3567617},
	year = {2022},
}

@inproceedings{10.1145/3557917.3567619,
	abstract = {Contrasting spatial co-location pattern discovery aims to find subsets of spatial features whose prevalences are substantially different in two spatial domains. This problem is important for generating hypotheses in many spatial applications, including oncology, regional economics, ecology, and epidemiology. In oncology, for example, this problem is important in developing immune-checkpoint inhibitor therapy for cancer treatment. This problem is challenging due to the large number of potential patterns that are exponentially related to the number of input spatial features. Traditional methods of co-location pattern detection require multiple runs, making computationally expensive and do not scale to large datasets. To address these limitations, we propose a Contrasting Spatial Co-location Discovery (CSCD) framework and contribute two filter-refine algorithms that exploit a novel interest measure; the participation index distribution difference (PIDD). Experiments on multiple cancer datasets (e.g., MxIF) show that the proposed algorithm yields substantial computational time savings compared with a baseline algorithm. A real-world case study demonstrates that the proposed work discovers patterns that are missed by the related work and have the potential to inspire new scientific discovery.},
	address = {New York, NY, USA},
	author = {Li, Yan and Farhadloo, Majid and Krishnan, Santhoshi and Xie, Yiqun and Frankel, Timothy L and Shekhar, Shashi and Rao, Arvind},
	booktitle = {Proceedings of the 10th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3557917.3567619},
	isbn = {9781450395311},
	keywords = {participation index distribution difference (PIDD), contrasting spatial co-location},
	location = {Seattle, Washington},
	numpages = {11},
	pages = {36–46},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '22},
	title = {CSCD: towards spatially resolving the heterogeneous landscape of MxIF oncology data},
	url = {https://doi.org/10.1145/3557917.3567619},
	year = {2022},
}

@inproceedings{10.1145/3557918.3565863,
	abstract = {Geographic phenomena are considered complex due to the heterogeneous nature of spatial dependencies. It is impossible to specify a universal law described in statistical or physical languages that can perfectly characterize a real-world geographic process and explain how it forms certain observed patterns. Traditional spatial analytics based on strict statistical principles, strong assumptions, or classic computation workflows are facing great challenges and opportunities when embracing the explosive growth of geospatial data and recent technical innovations. Here, we highlight the promises of Intelligent Spatial Analytics (ISA), a new set of spatial analytical approaches based on spatially explicit deep neural networks with more flexible data representation, modules for complex spatial dependence, weaker model prior assumptions, and hence the enhanced ability to predict/explain unknowns. Three essential topics in spatial analysis, i.e., geostatistics, spatial econometrics, and flow analytics are elaborated as examples in the vision of ISA. We also discuss challenging issues of ISA as an invitation to explore deeper linkages between machine/deep learning and spatial analysis at the frontier of Geospatial Artificial Intelligence.},
	address = {New York, NY, USA},
	author = {Zhu, Di and Gao, Song and Cao, Guofeng},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3557918.3565863},
	isbn = {9781450395328},
	keywords = {spatial analysis, neural networks, intelligent spatial analytics, GeoAI, GIS},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {4},
	pages = {10–13},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '22},
	title = {Towards the intelligent era of spatial analysis and modeling},
	url = {https://doi.org/10.1145/3557918.3565863},
	year = {2022},
}

@inproceedings{10.1145/3557918.3565865,
	abstract = {Recent years brought advancements in using neural networks for representation learning of various language or visual phenomena. New methods freed data scientists from hand-crafting features for common tasks. Similarly, problems that require considering the spatial variable can benefit from pretrained map region representations instead of manually creating feature tables that one needs to prepare to solve a task. However, very few methods for map area representation exist, especially with respect to road network characteristics. In this paper, we propose a method for generating microregions' embeddings with respect to their road infrastructure characteristics. We base our representations on OpenStreetMap road networks in a selection of cities and use the H3 spatial index to allow reproducible and scalable representation learning. We obtained vector representations that detect how similar map hexagons are in the road networks they contain. Additionally, we observe that embeddings yield a latent space with meaningful arithmetic operations. Finally, clustering methods allowed us to draft a high-level typology of obtained representations. We are confident that this contribution will aid data scientists working on infrastructure-related prediction tasks with spatial variables.},
	address = {New York, NY, USA},
	author = {Le\'{s}niara, Kacper and Szyma\'{n}ski, Piotr},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3557918.3565865},
	isbn = {9781450395328},
	keywords = {spatial representation learning, road network embeddings, embedding, clustering, OpenStreetMap embeddings},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {12},
	pages = {18–29},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '22},
	title = {highway2vec: representing OpenStreetMap microregions with respect to their road network characteristics},
	url = {https://doi.org/10.1145/3557918.3565865},
	year = {2022},
}

@inproceedings{10.1145/3557918.3565870,
	abstract = {Density-based clustering methods are frequently used to define spatial clusters and outliers (noise) for location-only data. Different algorithms for solving this problem emerged over the past few decades, with their main difference being the numerical representation of the spatial density. A problem not addressed by conventional density-based clustering methods is defining alternate spatial cluster maps at statistically significant spatial scales. This problem differs from conventional clustering, as the goal of finding alternate clusters is to define different spatial cluster maps for all statistically significant spatial scales. Knowledge of distinct spatial scales pertinent to clustering is important for understanding various scales underlying the data. In addition, alternate clusters with different spatial scales can inform decisions that require to be made at different spatial granularity. In this paper, we introduce a statistical test that uses Kullback-Leibler (KL) divergence loss between different spatial density profiles to identify all statistically significant spatial scales at which clustering occurs. The proposed method defines different clustering maps that reflect different scales at which spatial clusters occur. We define the divergence on a 1-D representation of cluster density, the reachability profile, to cluster spatial units with varying spatial scales. We illustrate the use of multiple spatial clustering at different scales by comparing the proposed method to the state-of-the-art for defining a single map of multiscale clusters, HDBScan. We conclude the paper by applying the proposed method to physical and human geography problems, area of interest delineation, and wildfire cluster modeling, respectively.},
	address = {New York, NY, USA},
	author = {Aydin, Orhun and Osorio-Murillo, Carlos and Huang, Cheng-Chia},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3557918.3565870},
	isbn = {9781450395328},
	keywords = {spatial scale, spatial clustering, multi-scale, KL divergence},
	location = {<conf-loc>, <city>Seattle</city>, <state>Washington</state>, </conf-loc>},
	numpages = {10},
	pages = {66–75},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '22},
	title = {Density-based cluster detection at multiple spatial scales via kullback-leibler divergence of reachability profiles},
	url = {https://doi.org/10.1145/3557918.3565870},
	year = {2022},
}

@inproceedings{10.1145/3557919.3565813,
	abstract = {In the field of the 3D reconstruction of cities in the past there is a raising interest in the creation of models that are not just geometrical, but also informative, semantic and georeferenced. Despite the advancements that were done in the historical reconstruction of architecture and archaeology, the solutions designed for larger scale models are still very limited. On the other hand, research on the digitisation of current-day cities provides useful instruments. In particular, CityJSON - a JSON encoding of CityGML - represents an easy-to-use and lightweight solution for storing 3D models of cities that are geolocated, semantic and that contain additional information in the form of attributes. This contribution proposes (1) to extend the schema to the needs of a historical representation; and (2) to incorporate the newly created model in a continuous flow pipeline, in which the geometry is dynamically updated each time an attribute is changed, as a means to foster collaboration.},
	address = {New York, NY, USA},
	author = {Vaienti, Beatrice and Guhennec, Paul and di Lenardo, Isabella},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3557919.3565813},
	isbn = {9781450395335},
	keywords = {version controlling, uncertainty visualisation, historical validation, historical modelling, digitisation, architectural reconstruction, 4D cities},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {20–23},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '22},
	title = {A data structure for scientific models of historical cities: extending the CityJSON format},
	url = {https://doi.org/10.1145/3557919.3565813},
	year = {2022},
}

@inproceedings{10.1145/3557922.3567481,
	abstract = {Building numbers shown on building outlines of a map are important information for guiding delivery associates to the correct building of a package's recipient. Intuitively, the more labeled buildings are present in our map, the less likely to misplace an order in addition to other benefits such as delivery efficiency as drivers get better visual cues about building positions. Although there are free and collaborative projects for creating geographic database of the world, such as the OpenStreetMap (OSM) [2] which also supplies building outlines along with their building numbers, many building outlines still remain unlabeled in many U.S. regions and other countries. Hence, we are interested in developing models that can automatically add building numbers with ≥ 99\% precision to unlabeled buildings across geographies with low to medium building number coverage. In this paper, we describe a ML model which in offline results showed 2\% to 12\% increase in building number coverage in some US regions compared to that of the OSM. The proposed model can also be applied to improve the building number coverage of other countries after fine-tuning to those new regions.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Ma, Puyang and Garg, Ravi and Moustafa, Mohamed},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Spatial Big Data and AI for Industrial Applications},
	doi = {10.1145/3557922.3567481},
	isbn = {9781450395359},
	keywords = {map features, machine learning, geospatial data, GPS points},
	location = {Seattle, Washington},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {GeoIndustry '22},
	title = {BinoML: supervised ranking for automatic building labeling},
	url = {https://doi.org/10.1145/3557922.3567481},
	year = {2022},
}

@inproceedings{10.1145/3557989.3566157,
	abstract = {Mesa is an open-source agent-based modeling (ABM) framework implemented in the Python programming language, allowing users to build and visualize agent-based models. It has been used in a diverse range of application areas over the years ranging from biology to workforce dynamics. However, there has been no direct support for integrating geographical data from geographical information systems (GIS) into models created with Mesa. Users have had to rely on their own implementations to meet such needs. In this paper we present Mesa-Geo, a GIS extension for Mesa, which allows users to import, manipulate, visualise and export geographical data for ABM. We introduce the main components and functionalities of Mesa-Geo, followed by example applications utilizing geographical data which demonstrates Mesa-Geo's core functionalities and features common to agent-based models. Finally, we conclude with a discussion and outlook on future directions for Mesa-Geo.},
	address = {New York, NY, USA},
	author = {Wang, Boyu and Hess, Vincent and Crooks, Andrew},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
	doi = {10.1145/3557989.3566157},
	isbn = {9781450395373},
	keywords = {geographic information systems (GIS), complex systems, agent-based modeling (ABM), Python},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {1–10},
	publisher = {Association for Computing Machinery},
	series = {GeoSim '22},
	title = {Mesa-Geo: A GIS Extension for the Mesa Agent-Based Modeling Framework in Python},
	url = {https://doi.org/10.1145/3557989.3566157},
	year = {2022},
}

@inproceedings{10.1145/3557989.3566158,
	abstract = {Given a set S of spatial feature-types, its feature-instances, a study area, and a neighbor relationship, the goal is to find pairs &lt;a region (rg), a subset C of S&gt; such that C is a statistically significant regional colocation pattern in region rg. For example Caribou Coffee and Starbucks are significantly co-located in Minneapolis but not in Dallas at present. This problem has applications in a wide variety of domains including ecology, economics, and sociology. The problem is computationally challenging due to the exponential number of regional colocation patterns and candidate regions. The current literature on regional colocation pattern detection has not addressed statistical significance which can result in spurious (chance) pattern instances. In this paper, we propose a novel technique for mining statistically significant regional colocation patterns. Our approach determines regions based on geographically defined boundaries (e.g., counties) unlike previous works which employed clustering, or regular polygons to enumerate candidate regions. To reduce spurious patterns, we perform a statistical significance test by modeling the observed data points with multiple Monte Carlo simulations within the corresponding regions. Using Safegraph POI dataset, this paper provides a case study on retail establishments in Minnesota for validation of proposed ideas. The paper also provides a detailed interpretation of discovered patterns using game theory and regional economics.},
	address = {New York, NY, USA},
	author = {Ghosh, Subhankar and Gupta, Jayant and Sharma, Arun and An, Shuai and Shekhar, Shashi},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
	doi = {10.1145/3557989.3566158},
	isbn = {9781450395373},
	keywords = {statistical significance, spatial heterogeneity, regional colocation pattern, neighborhood graph, game theory},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {11–20},
	publisher = {Association for Computing Machinery},
	series = {GeoSim '22},
	title = {Towards geographically robust statistically significant regional colocation pattern detection},
	url = {https://doi.org/10.1145/3557989.3566158},
	year = {2022},
}

@inproceedings{10.1145/3557989.3566159,
	abstract = {Siting transportation infrastructures such highway and highspeed railway connects cities and benefits the coordinate development in urban agglomerations. Previous infrastructure siting models rely on the expert experience or spatial-support decision using spatial data describing current conditions. The future development after the infrastructure construction is less considered. This study presents an alternative spatial cooperative simulation-based infrastructure siting model. The spatial cooperative scenario simulation model is developed to simulate the land, population, and economy variations and assess the benefits of the infrastructure construction. The experiment in the high-speed railway in Guangdong-Hongkong-Macao Greater Bay Area, China demonstrates the effectiveness of the presented model. The results provide valuable insights into the spatial-support siting decision for the new high-speed railway in the urban agglomerations.},
	address = {New York, NY, USA},
	author = {Li, Mingxiao and Gao, Wei and Tu, Wei and Yue, Jun and Huang, Zhengdong and Li, Qingquan},
	booktitle = {Proceedings of the 5th ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
	doi = {10.1145/3557989.3566159},
	isbn = {9781450395373},
	keywords = {spatial simulation, infrastructure siting, high-speed railway, greater bay area},
	location = {Seattle, Washington},
	numpages = {4},
	pages = {21–24},
	publisher = {Association for Computing Machinery},
	series = {GeoSim '22},
	title = {Spatial cooperative scenario simulation for infrastructure siting in Guangdong-HongKong-Macao Greater Bay Area},
	url = {https://doi.org/10.1145/3557989.3566159},
	year = {2022},
}

@inproceedings{10.1145/3557990.3567586,
	abstract = {Everyday our living city produces a tremendous amount of spatial-temporal data, involved with multiple sources from the individual scale to the city scale. Undoubtedly, such massive urban data can be explored for a better city and better life, as what the urban computing community has been dedicating in recent years. Nevertheless, existing studies are still facing the challenges of data fusion for the urban data as well as the knowledge distillation for specific applications. Moreover, there is a lack of full-featured and user-friendly platform for both researchers and developers in urban computing scenario. Therefore, in this paper, we present an urban knowledge graph (UrbanKG) system to incorporate knowledge graph with urban computing. Specifically, the system introduces a complete scheme to construct knowledge graph for urban data fusion from Data layer to Construction layer. The system further develops the multiple layers of Storage, Algorithm, Operation and Applications, which achieve the knowledge distillation and support various functions to the users. We perform three representative and practical use cases and demonstrate the system capability of boosting performance in various downstream applications, indicating a promising research direction of knowledge-driven urban computing.},
	address = {New York, NY, USA},
	author = {Liu, Yu and Ding, Jingtao and Li, Yong},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geospatial Knowledge Graphs},
	doi = {10.1145/3557990.3567586},
	isbn = {9781450395380},
	keywords = {urban computing, spatial-temporal data, knowledge graph},
	location = {Seattle, Washington},
	numpages = {5},
	pages = {3–7},
	publisher = {Association for Computing Machinery},
	series = {GeoKG '22},
	title = {Developing knowledge graph based system for urban computing},
	url = {https://doi.org/10.1145/3557990.3567586},
	year = {2022},
}

@inproceedings{10.1145/3557990.3567588,
	abstract = {This paper highlights the challenges of representing uncertain geospatial information in knowledge graphs. We propose to use Real Estate advertisements since professionals use a lot of vernacular and vague places in order to promote a house to their target audience. Then, we suggest to model local place names using fuzzy set theory. Finally, we discuss how to build a knowledge graph that represents extracted geospatial objects and their uncertainty.},
	address = {New York, NY, USA},
	author = {Cadorel, Lucie and Tettamanzi, Andrea G. B. and Gandon, Fabien},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geospatial Knowledge Graphs},
	doi = {10.1145/3557990.3567588},
	isbn = {9781450395380},
	keywords = {uncertainty, knowledge graph, fuzzy sets},
	location = {Seattle, Washington},
	numpages = {2},
	pages = {1–2},
	publisher = {Association for Computing Machinery},
	series = {GeoKG '22},
	title = {Towards a representation of uncertain geospatial information in knowledge graphs},
	url = {https://doi.org/10.1145/3557990.3567588},
	year = {2022},
}

@inproceedings{10.1145/3557990.3567590,
	abstract = {In an automated map making process, map features like lane-markings, traffic-signs, poles, stop-lines and similar other features are extracted using deep learning methods from various sources of imagery or sensor data. These sources come with their own positional errors due to which the map features extracted from these sources are always misaligned with respect to each other, making the conflation of map features a difficult task. We propose a novel method to find map feature correspondences between 2 sets of map feature datasets obtained from different sources by first converting them into a heterogeneous geospatial graph and then doing node representation learning using a graph neural network that can generate vector embeddings that encode information of morphology, attributes, and absolute and relative positions of the map feature with respect to its neighbours along with aggregated information from its neighbours. This process can be employed to generate embeddings of map feature nodes, which are amicable to identifying spatially similar and corresponding map feature nodes across disparate sources with varying degree of similarity scores. When applied aptly, these map feature correspondences between two sources can be used as anchor points to perform spatial alignment with linear or nonlinear transforms, leading to a better conflation.},
	address = {New York, NY, USA},
	author = {Soni, Abhilshit and Boddhu, Sanjay},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geospatial Knowledge Graphs},
	doi = {10.1145/3557990.3567590},
	isbn = {9781450395380},
	keywords = {map fusion, geospatial graph, conflation, alignment},
	location = {Seattle, Washington},
	numpages = {10},
	pages = {7–16},
	publisher = {Association for Computing Machinery},
	series = {GeoKG '22},
	title = {Finding map feature correspondences in heterogeneous geospatial datasets},
	url = {https://doi.org/10.1145/3557990.3567590},
	year = {2022},
}

@inproceedings{10.1145/3557991.3567784,
	abstract = {Classifying trip modalities, i.e. driving, walking, etc., from GPS trajectories is one of the fundamental tasks for urban mobility analytics. It can be used for efficient route planning, human activity recognition, and public transportation design where understanding the time and location of transitioning to different modalities may provide additional insights. Informally, given a GPS trajectory consisting of temporally ordered GPS locations, trip modality/activity classification aims to assign trip modes to each GPS point. It is a challenging task due to the associated noise with the GPS data, the lack of knowledge about the underlying road network as well as the driving traffic conditions which may affect the trip behavior (e.g. driving slower than walking speed at rush hour traffic). Despite its widespread applications, the existing methods are either dependent on multi-sensor data (such as GPS, IMU, Camera, etc.) or use heuristic-based filtering to classify modalities of the trajectory datasets. Moreover, they consider limited number of transitions per trip making them inadequate for more frequent activity changes. In this paper, we propose a novel deep neural network architecture, Frequent Activity Classification Network FACNet, leveraging a bi-directional LSTM network and a custom Attention module to infer modality of GPS points in a trajectory with frequent modality changes. Our supervised learning approach depends only on the GPS trace without any additional inputs, making it applicable to a wide variety of modality related problems. Experiments confirm the superiority of our method compared to the related work as well as heuristic approaches. Finally, we provide access to a set of anonymized GPS trajectories that is made available to the broader research community to provide opportunities to further improve the existing research on the topic.},
	address = {New York, NY, USA},
	articleno = {6},
	author = {Eftelioglu, Emre and Wolff, Gil and Nimmagadda, Sai Krishna Tejaswi and Kumar, Vishal and Chowdhury, Amber Roy},
	booktitle = {Proceedings of the 15th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3557991.3567784},
	isbn = {9781450395397},
	keywords = {urban mobility, neural networks, geospatial data, GPS trajectories},
	location = {Seattle, Washington},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '22},
	title = {Deep classification of frequently-changing activities from GPS trajectories},
	url = {https://doi.org/10.1145/3557991.3567784},
	year = {2022},
}

@inproceedings{10.1145/3557991.3567797,
	abstract = {In this paper we propose a method called START for automatically extracting journeys from truck GPS traces. Each journey represents a movement of a truck from a task place (e.g., a cargo station) to another task place, possibly with multiple stops in between for non-task purposes (resting, eating, refueling, etc.). The START method begins with detecting stops. Then it classifies each detected stop as a task stop or a non-task stop. For stop classification, START utilizes a novel feature called route-detour and combines it with a set of temporal features and place category features. The method further clusters detected stops and applies a majority voting to consolidate the classification of the member stops in each cluster. We evaluate START using real-world data and compare it with a baseline method that uses only temporal and place category features. The results show that START achieves a better stop classification accuracy than the baseline method. Finally, we demonstrate the utility of START via using the journeys extracted by START to build and evaluate an ML-based ETA prediction model.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Xu, Bo and Gupta, Rohit and Hashisho, Basel and K\"{o}hn, Reinhard and van de Hoef, Sebastian},
	booktitle = {Proceedings of the 15th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3557991.3567797},
	isbn = {9781450395397},
	keywords = {stop classification, routing, GPS traces, GIS, ETA},
	location = {Seattle, Washington},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '22},
	title = {Extracting journeys from truck GPS traces},
	url = {https://doi.org/10.1145/3557991.3567797},
	year = {2022},
}

@inproceedings{10.1145/3557991.3567802,
	abstract = {Reducing emissions of greenhouse gases has become a major challenge for the next decades. The transportation sector is responsible for about a quarter of all the CO2 in most developed countries. This study uses a large set of trajectory data (272.289 trajectories, built from 75.178.775 GPS points) to analyze and quantify, on a road segment level, the impacts of driving at a steady speed on the energy consumption of electrical vehicles. The results show that drivers should strive to maintain a steady speed for as much as possible as it can reduce the consumption by up to 42\% while increasing the travel time by just 10\%.},
	address = {New York, NY, USA},
	articleno = {12},
	author = {David, Rodrigo Sasse and Zim\'{a}nyi, Esteban and Torp, Kristian and Sakr, Mahmoud},
	booktitle = {Proceedings of the 15th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
	doi = {10.1145/3557991.3567802},
	isbn = {9781450395397},
	keywords = {trajectories, steady speed, energy consumption, electrical vehicles, eco-driving, OBD, GPS, EVs},
	location = {Seattle, Washington},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {IWCTS '22},
	title = {Speed and energy consumption for electrical vehicles},
	url = {https://doi.org/10.1145/3557991.3567802},
	year = {2022},
}

@inproceedings{10.1145/3557992.3565989,
	abstract = {Often, global and regional topics on Twitter across multiple thematic areas, such as disasters, politics, protests, entertainment, epidemics, literature, travel, culture, weather, etc., witness an unprecedented level of exchange of conversations. An issue with those conversations is that a user can be at location A and participate in a public discourse specific to location B, which we refer to as the Location A/B problem. Location profiling of users solely based on locations mentioned in their tweets leads to ineffective location-based recommendations. The problem is deemed solved if location candidates could be categorized as either origin locations (Location As) or non-origin locations (Location Bs); however, real-world tweets are much more complex, and currently, no public datasets are available for training such classifiers. To the best of our knowledge, this study yields the first steps in addressing the Location A/B problem on Twitter. We propose a theoretical framework that utilizes the existing literature on location inference to categorize location candidates as either origin locations or non-origin locations. We envision that: (i) the framework provides the grounds for designing models that aim to solve the Location A/B problem, and (ii) the location profiling of users based on origin locations leads to improved geotargeted recommendations.},
	address = {New York, NY, USA},
	articleno = {4},
	author = {Lamsal, Rabindra and Harwood, Aaron and Read, Maria Rodriguez},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3557992.3565989},
	isbn = {9781450395403},
	keywords = {spatial indicators, recommender system, origin locations, location extraction, geotag tweets, Twitter analytics},
	location = {Seattle, Washington},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '22},
	title = {Addressing the location A/B problem on Twitter: the next generation location inference research},
	url = {https://doi.org/10.1145/3557992.3565989},
	year = {2022},
}

@inproceedings{10.1145/3557992.3565990,
	abstract = {Twitter is a popular social networking service where people send short messages called tweets. Tweets contain metadata such as language, hashtags, geotags, and time of creation. We focus on the geotags of tweets. A Geo-tag is georeferenced information that indicates the geographical origin of a tweet. Geotagged tweets provide an excellent opportunity to understand the underlying user behavior. We propose a preference-aware route recommendation method relying on over one billion geotagged tweets. The method can recommend routes based on user preference by extracting a subset of one billion geotagged tweets according to user preference and using that subset to generate a cost function for route discovery. The proposed method assumes that areas with a high density of geotagged tweets are areas of high interest. In other words, if the density of geotagged tweets with user preference is superimposed on the cost of the route search, the users' preference can be considered when recommending a route. We highlight a nighttime route recommendation mechanism for a case study of our method. We hypothesize that geotagged tweets sent out at night indicate human activity at night. In other words, areas with a high density of geo-tagged tweets are considered to be areas that are vibrant at night. In addition, it is empirically clear that nighttime vibrant is also based on brightness. Therefore, we utilize nighttime tweets and nighttime light data to recommend routes. We extract a subset by calculating nighttime from tweet metadata. Tweets data are divided into grids and used to calculate a vibrant grid from a weighted tweets grid and a nighttime lights grid. Edge is weighted from vibrant cell values and road network edge lengths to recommend a vibrant route based on weighted road network edges. We experimented in Shinjuku, Tokyo, Japan, between two stations. As a result, based on the objective evaluation, we recommended a vibrant route.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Yamashita, Osei and Yokoyama, Shohei},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3557992.3565990},
	isbn = {9781450395403},
	keywords = {social data, route recommendation, nighttime lights, dijkstra's algorithm, Twitter},
	location = {Seattle, Washington},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '22},
	title = {Preference aware route recommendation using one billion geotagged tweets},
	url = {https://doi.org/10.1145/3557992.3565990},
	year = {2022},
}

@inproceedings{10.1145/3557992.3565994,
	abstract = {Geocoding a spatial description is challenging since vernacular place names and vague spatial expressions give uncertainty and ambiguity to the description. Usually, digital gazetteers are used to match geospatial objects to their boundaries. However, gazetteers do not contain all places. Therefore, a number of studies have proposed to enrich gazetteers by estimating and representing the vernacular places. Nevertheless, only a few approaches have taken into account vague spatial expressions such as "nearby", and have represented geospatial objects as sharp boundaries. In this work, we present an automatic workflow to retrieve a location approximation of vague spatial description. We propose a model to estimate a fuzzy representation of each mentioned geospatial information and spatial expressions. Then, we perform information fusion to find a location approximation of a property. Lastly, we demonstrate our proposed method by applying it to the case of French Real Estate advertisements with two real-world datasets in Nice and Paris. Real Estate advertisements allow us to deal with uncertain geospatial objects since avague and exaggerated property location's description is usually provided. Our results show that our proposed method is promising and able to correctly approximate a location from uncertain spatial descriptions.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Cadorel, Lucie and Overal, Denis and Tettamanzi, Andrea G. B.},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3557992.3565994},
	isbn = {9781450395403},
	keywords = {uncertainty, spatial relationships, natural language, geocoding, fuzzy sets},
	location = {Seattle, Washington},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '22},
	title = {Fuzzy representation of vague spatial descriptions in real estate advertisements},
	url = {https://doi.org/10.1145/3557992.3565994},
	year = {2022},
}

@inproceedings{10.1145/3563359.3597443,
	abstract = {The ‘15-minute city’ paradigm is an urban model based on the concept of ‘hyper-proximity’: citizens should be able to access fundamental services and facilities (such as schools, shops, parks, doctors, and markets) within 15-20 minutes on foot, by bicycle or by public transport. Compliance with the ‘15-minute city’ paradigm is supposed to reduce pollution and social inequalities. It is supposed to bring the psychological fragility of the citizen back to the center of the urban redevelopment debate. Although the concept has gained great attention and interest from policymakers and urban designers, we still lack tools that can help to validate, on a data-driven basis, the assumption that hyper-proximity is eventually correlated with lower urban segregation, which is one of the driving forces that lead to social inequalities. We aim to define a data-driven methodology to analyze the urban areas where services should be accessible within 15 minutes; network analysis is exploited to estimate services proximity as well as the connectivity of different urban areas with each other, in order to gather signals of the general resilience or exposure to urban segregation. We also aim to compute a set of city-agnostic metrics that will include user-specified parameters and personalized weights for each Point of Interest’s category. United-and-Close is the resulting Web platform designed to be accessible to citizens, policy and decision-makers, and investors, but also for researchers involved in disciplines such as urban informatics that need support to better assess the 15-minute paradigm and its actual impact on our cities.},
	address = {New York, NY, USA},
	author = {Lai, Mirko and Vilella, Salvatore and Cena, Federica and Patti, Viviana and Ruffo, Giancarlo Francesco},
	booktitle = {Adjunct Proceedings of the 31st ACM Conference on User Modeling, Adaptation and Personalization},
	doi = {10.1145/3563359.3597443},
	isbn = {9781450398916},
	keywords = {15-minute City paradigm, accessibility, inclusive urban design},
	location = {Limassol, Cyprus},
	numpages = {6},
	pages = {115–120},
	publisher = {Association for Computing Machinery},
	series = {UMAP '23 Adjunct},
	title = {United-and-Close: An interactive visual platform for assessing urban segregation within the 15-minutes paradigm},
	url = {https://doi.org/10.1145/3563359.3597443},
	year = {2023},
}

@article{10.1145/3567421,
	abstract = {Generating alternative routes in road networks is an application of significant interest for online navigation systems. A high quality set of diverse alternate routes offers two functionalities - (a) support multiple (unknown) preferences that the user may have; and (b) robust changes in network conditions. We formulate a new quantification of the latter in this paper, and propose a novel method to produce alternative routes based on concepts from electrical flows and their decompositions. Our method is fundamentally different from the main techniques that produce alternative routes in road networks, which are the penalty and the plateau methods, with the former providing high quality results but being too slow for practical use and the latter being fast but suffering in terms of quality. We evaluate our method against the penalty and plateau methods, showing that it is as fast as the plateau method while also recovering much of the headroom towards the quality of the penalty method. The metrics we use to evaluate performance include the stretch (the average cost of the routes), the diversity, and the robustness (the connectivity between the origin and destination) of the induced set of routes.},
	address = {New York, NY, USA},
	articleno = {24},
	author = {Sinop, Ali Kemal and Fawcett, Lisa and Gollapudi, Sreenivas and Kollias, Kostas},
	doi = {10.1145/3567421},
	issn = {2374-0353},
	issue_date = {December 2023},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Routing, road networks, algorithms, electrical flows, alternates},
	month = {nov},
	number = {4},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {Robust Routing Using Electrical Flows},
	url = {https://doi.org/10.1145/3567421},
	volume = {9},
	year = {2023},
}

@article{10.1145/3568669,
	abstract = {Infectious diseases are transmitted between human hosts when in close contact over space and time. Recently, an unprecedented amount of spatial and spatiotemporal data have been made available that can be used to improve our understanding of the spread of COVID-19 and other infectious diseases. This understanding will be paramount to prepare for future pandemics through spatial algorithms and systems to collect, capture, curate and analyze complex, multi-scale human movement data to solve problems such as infectious diseases prediction, contact tracing, and risk assessment. In exploring and deepening the conversation around this topic, the five articles included in the second volume of this special issue employ diverse theoretical perspectives, methodologies, and frameworks, including but not limited to close contact modeling, infectious diseases spread prediction, mobility analysis, effective testing and intervention strategies. Rather than focusing on a narrow set of problems, these articles provide a glimpse into the diverse possibilities of leveraging spatial and spatiotemporal data for pandemic preparedness.},
	address = {New York, NY, USA},
	articleno = {25e},
	author = {Z\"{u}fle, Andreas and Gao, Song and Anderson, Taylor},
	doi = {10.1145/3568669},
	issn = {2374-0353},
	issue_date = {December 2022},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Infectious diseases, pandemic preparedness, spatiotemporal systems, geographical information systems, health informatics},
	month = {nov},
	number = {4},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	title = {Introduction to the Special Issue on Understanding the Spread of COVID-19, Part 2},
	url = {https://doi.org/10.1145/3568669},
	volume = {8},
	year = {2022},
}

@article{10.1145/3568670,
	abstract = {Infectious diseases are transmitted between human hosts when in close contact over space and time. Recently, an unprecedented amount of spatial and spatiotemporal data have been made available that can be used to improve our understanding of the spread of COVID-19 and other infectious diseases. This understanding will be paramount to prepare for future pandemics through spatial algorithms and systems to collect, capture, curate, and analyze complex, multi-scale human movement data to solve problems such as infectious diseases prediction, contact tracing, and risk assessment. In exploring and deepening the conversation around this topic, the eight articles included in the first volume of this special issue employ diverse theoretical perspectives, methodologies, and frameworks, including but not limited to infectious diseases simulation, risk prediction, response policy design, mobility analysis, and case diagnosis. Rather than focusing on a narrow set of problems, these articles provide a glimpse into the diverse possibilities of leveraging spatial and spatiotemporal data for pandemic preparedness.},
	address = {New York, NY, USA},
	articleno = {17e},
	author = {Z\"{u}fle, Andreas and Anderson, Taylor and Gao, Song},
	doi = {10.1145/3568670},
	issn = {2374-0353},
	issue_date = {September 2022},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {health informatics, geographical information systems, spatiotemporal systems, pandemic preparedness, Infectious diseases},
	month = {oct},
	number = {3},
	numpages = {5},
	publisher = {Association for Computing Machinery},
	title = {Introduction to the Special Issue on Understanding the Spread of COVID-19, Part 1},
	url = {https://doi.org/10.1145/3568670},
	volume = {8},
	year = {2022},
}

@inproceedings{10.1145/3569951.3597548,
	abstract = {In this paper, we summarize the research efforts undertaken by the Louisiana Coastal Protection and Restoration Authority (CPRA) during their 2023 Coastal Master Plan planning period, and the tools created by the Pittsburgh Supercomputing Center (PSC) to support them in their efforts. Specifically, we introduce the Master Plan Data Viewer and explore how creating interactive accompaniments for print material is a key element in the community’s goal of data democratization. This proof of concept focused on creating an easy-to-use geospatial data portal that would reduce barriers to entry, tell the story of the master plan, and inform Louisiana residents of the potential changes to their state’s coast.},
	address = {New York, NY, USA},
	author = {Yoder, Matt and Puerto, Juan},
	booktitle = {Practice and Experience in Advanced Research Computing},
	doi = {10.1145/3569951.3597548},
	isbn = {9781450399852},
	keywords = {GIS, HPC, data visualization, geospatial, portal, raster, vector},
	location = {Portland, OR, USA},
	numpages = {4},
	pages = {269–272},
	publisher = {Association for Computing Machinery},
	series = {PEARC '23},
	title = {Visualizing the Future of Louisiana’s Coastline},
	url = {https://doi.org/10.1145/3569951.3597548},
	year = {2023},
}

@article{10.1145/3571741,
	abstract = {Cities are very complex systems. Representing urban regions are essential for exploring, understanding, and predicting properties and features of cities. The enrichment of multi-modal urban big data has provided opportunities for researchers to enhance urban region embedding. However, existing works failed to develop an integrated pipeline that fully utilizes effective and informative data sources within geographic units. In this article, we regard a geo-tile as a geographic unit and propose a multi-modal and multi-stage representation learning framework, namely Geo-Tile2Vec, for urban analytics, especially for urban region properties identification. Specifically, in the early stage, geo-tile embeddings are firstly inferred through dynamic mobility events which are combinations of point-of-interest (POI) data and trajectory data by a Word2Vec-like model and metric learning. Then, in the latter stage, we use static street-level imagery to further enrich the embedding information by metric learning. Lastly, the framework learns distributed geo-tile embeddings for the given multi-modal data. We conduct experiments on real-world urban datasets. Four downstream tasks, i.e., main POI category classification task, main land use category classification task, restaurant average price regression task, and firm number regression task, are adopted for validating the effectiveness of the proposed framework in representing geo-tiles. Our proposed framework can significantly improve the performances of all downstream tasks. In addition, we also demonstrate that geo-tiles with similar urban region properties are geometrically closer in the vector space.},
	address = {New York, NY, USA},
	articleno = {10},
	author = {Luo, Yan and Leong, Chak-Tou and Jiao, Shuhai and Chung, Fu-Lai and Li, Wenjie and Liu, Guoping},
	doi = {10.1145/3571741},
	issn = {2374-0353},
	issue_date = {June 2023},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Multi-modal learning, representative learning, urban computing, unsupervised learning},
	month = {apr},
	number = {2},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {Geo-Tile2Vec: A Multi-Modal and Multi-Stage Embedding Framework for Urban Analytics},
	url = {https://doi.org/10.1145/3571741},
	volume = {9},
	year = {2023},
}

@inproceedings{10.1145/3577117.3577143,
	abstract = {With the rapid expansion of the scale of urbanization and the rapid economic and social development of coastal areas, the use changes of the coastline and coastal areas of the Leizhou Peninsula is increasing. Employing envi5.1 and arcgis10.2 tools, the normalized difference water index (NDWI) was used to process the image, and the water and land separation was carried out according to the threshold segmentation method. The data of the coastline in 2001, 2008, 2015 and 2020 were extracted from the interpretation of the remote sensing image maps, and the results of the automatic computer interpretation were examined using the fixed-sample visual interpretation method to analyze the coastline dynamics of the Leizhou Peninsula in the past 20 years. The results showed that: (1) From 2001 to 2020, the total length of the coastline of Leizhou Peninsula showed an increasing trend, a total increase of 95.98km, and annual variation was 4.80km; (2) From 2001 to 2008, the coastline increased the most, with an increase of 44.25km; from 2008 to 2015, the increase of the coastline was smaller, with an increase of 18.69km; from 2015 to 2020, the coastline increased by 33.04km; (3) The areas with large changes in coastline length were Xuwen county, Leizhou city, and Potou district, which increased by 38.29km, 34.21km, and 18.86km respectively; (4) The areas with the most complex coastline changes were mainly concentrated in Potou district, Xuwen county, Leizhou city and Suixi county for economic development, tourism development, marine aquaculture and other areas. In this regard, it was proposed to strengthen the basic dynamic information monitoring of the coastline, carry out the repair and survey of the coastline in a timely manner, grasp the utilization information of the coastline, and implement the task of protecting the coastline resources.},
	address = {New York, NY, USA},
	author = {Wu, Mingfa and Li, Fengyi and Liu, Yaoyao and Huang, Yao and Zhou, Yiting and Pan, Xianjun},
	booktitle = {Proceedings of the 6th International Conference on Advances in Image Processing},
	doi = {10.1145/3577117.3577143},
	isbn = {9781450397155},
	keywords = {China, Coastline change, GIS, Leizhou peninsula, NDWI, Remote sensing image},
	location = {Zhanjiang, China},
	numpages = {9},
	pages = {144–152},
	publisher = {Association for Computing Machinery},
	series = {ICAIP '22},
	title = {Remote Sensing Information Extraction and Dynamic Change Analysis of Leizhou Peninsula Coastline},
	url = {https://doi.org/10.1145/3577117.3577143},
	year = {2023},
}

@inproceedings{10.1145/3579028.3609009,
	abstract = {In our research laboratory, we have been working on developing a software product line (SPL) specifically tailored for generating web-based geographic information systems (GIS). In addition, we have also designed a domain specific language (DSL) to make configuring our products as easy and flexible as possible. Over time, we have utilized this product line to create small GIS products, aiming to simplify the process of publishing and sharing geographic data. The steps involved in generating and deploying this kind of products are consistently repeated, so they can be easily automated. Doing so, we further reduce the time to market for this set of simple products, and minimize the complexity associated with the entire process.This article introduces GIS-Publisher, a tool that allows users to easily generate web applications from a directory containing a collection of shapefiles (a popular format for storing geographic data). These web applications can be also automatically deployed on their preferred machine, whether it is locally, remotely (via SSH), or even on an AWS instance. Moreover, the tool also supports the definition of custom styles for each shapefile, granting users full control over the visual representation of their geographic data.},
	address = {New York, NY, USA},
	author = {de Castro, David and Corti\~{n}as, Alejandro and Lamas, Victor and Luaces, Miguel R.},
	booktitle = {Proceedings of the 27th ACM International Systems and Software Product Line Conference - Volume B},
	doi = {10.1145/3579028.3609009},
	isbn = {9798400700927},
	keywords = {automatic deployment, geograhpic data, software product lines, tool},
	location = {Tokyo, Japan},
	numpages = {5},
	pages = {20–24},
	publisher = {Association for Computing Machinery},
	series = {SPLC '23},
	title = {GIS-Publisher: From a Geographic Data Set to a Deployed Product with One Command},
	url = {https://doi.org/10.1145/3579028.3609009},
	year = {2023},
}

@inproceedings{10.1145/3580305.3599538,
	abstract = {The prosperity of crowdsourcing geospatial data provides increasing opportunities to understand our cities. In particular, OpenStreetMap (OSM) has become a prominent vault of geospatial data on the Web. In this context, learning urban region representations from OSM data, which is unexplored in previous work, could be profitable for various downstream tasks. In this work, we utilize OSM buildings (footprints) complemented with points of interest (POIs) to learn region representations, as buildings' shapes, spatial distributions, and properties have tight linkages to different urban functions. However, appealing as it seems, urban buildings often exhibit complex patterns to form dense or sparse areas, which brings significant challenges for unsupervised feature extraction. To address the challenges, we propose RegionDCL1, an unsupervised framework to deeply mine urban buildings. In a nutshell, we leverage random points generated by Poisson Disk Sampling to tackle data-sparse areas and utilize triplet loss with a novel adaptive margin to preserve inter-region correlations. Furthermore, we train our model with group-level and region-level contrastive learning, making it adaptive to varying region partitions. Extensive experiments in two global cities demonstrate that RegionDCL consistently outperforms the state-of-the-art counterparts across different region partitions, and outputs effective representations for inferring urban land use and population density.},
	address = {New York, NY, USA},
	author = {Li, Yi and Huang, Weiming and Cong, Gao and Wang, Hao and Wang, Zheng},
	booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3580305.3599538},
	isbn = {9798400701030},
	keywords = {geospatial data mining, openstreetmap, representation learning, urban regions},
	location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
	numpages = {11},
	pages = {1363–1373},
	publisher = {Association for Computing Machinery},
	series = {KDD '23},
	title = {Urban Region Representation Learning with OpenStreetMap Building Footprints},
	url = {https://doi.org/10.1145/3580305.3599538},
	year = {2023},
}

@inproceedings{10.1145/3580305.3599581,
	abstract = {Urbanization's rapid progress has led to many big cities, which have modernized many people's lives but also engendered big challenges, such as air pollution, increased energy consumption and traffic congestion. Tackling these challenges were nearly impossible years ago given the complex and dynamic settings of cities. Nowadays, sensing technologies and large-scale computing infrastructures have produced a variety of big data in urban spaces, e.g., human mobility, air quality, traffic patterns, and geographical data. Motivated by the opportunities of building more intelligent cities, we came up with a vision of urban computing, which aims to unlock the power of knowledge from big and heterogeneous data collected in urban spaces and apply this powerful information to solve major issues our cities face today.},
	address = {New York, NY, USA},
	author = {Meng, Chuishi and Li, Yanhua and Zheng, Yu and Ye, Jieping and Yang, Qiang and Yu, Philip S. and Wolfson, Ouri},
	booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	doi = {10.1145/3580305.3599581},
	isbn = {9798400701030},
	keywords = {spatio-temporal data mining, urban computing},
	location = {<conf-loc>, <city>Long Beach</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
	numpages = {2},
	pages = {5874–5875},
	publisher = {Association for Computing Machinery},
	series = {KDD '23},
	title = {The 12th International Workshop on Urban Computing},
	url = {https://doi.org/10.1145/3580305.3599581},
	year = {2023},
}

@article{10.1145/3582561,
	abstract = {In intelligent logistics systems, predicting the Estimated Time of Pick-up Arrival (ETPA) of packages is a crucial task, which aims to predict the courier’s arrival time to all the unpicked-up packages at any time. Accurate prediction of ETPA can help systems alleviate customers’ waiting anxiety and improve their experience. We identify three main challenges of this problem. First, unlike the travel time estimation problem in other fields like ride-hailing, the ETPA task is distinctively a multi-destination and path-free prediction problem. Second, an intuitive idea for solving ETPA is to predict the pick-up route and then the time in two stages. However, it is difficult to accurately and efficiently predict couriers’ future routes in the route prediction step since their behaviors are affected by multiple complex factors. Third, furthermore, in the time prediction step, the requirement for providing a courier’s all unpicked-up packages’ ETPA at once in real time makes the problem even more challenging. To tackle the preceding challenges, we propose RankETPA, which integrates the route inference into the ETPA prediction. First, a learning-based pick-up route predictor is designed to learn the route-ranking strategies of couriers from their massive spatial-temporal behaviors. Then, a spatial-temporal attention-based arrival time predictor is designed for real-time ETPA inference via capturing the spatial-temporal correlations between the unpicked-up packages. Extensive experiments on two real-world datasets and a synthetic dataset demonstrate that RankETPA achieves significant performance improvement against the baseline models.},
	address = {New York, NY, USA},
	articleno = {50},
	author = {Wen, Haomin and Lin, Youfang and Wu, Fan and Wan, Huaiyu and Sun, Zhongxiang and Cai, Tianyue and Liu, Hongyu and Guo, Shengnan and Zheng, Jianbin and Song, Chao and Wu, Lixia},
	doi = {10.1145/3582561},
	issn = {2157-6904},
	issue_date = {June 2023},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {Trajectory, Deep Neural Networks, package pick-up arrival time prediction},
	month = {apr},
	number = {3},
	numpages = {22},
	publisher = {Association for Computing Machinery},
	title = {Enough Waiting for the Couriers: Learning to Estimate Package Pick-up Arrival Time from Couriers’ Spatial-Temporal Behaviors},
	url = {https://doi.org/10.1145/3582561},
	volume = {14},
	year = {2023},
}

@inproceedings{10.1145/3583780.3615008,
	abstract = {With the proliferation of wireless and mobile devices, Spatial Crowdsourcing (SC) attracts increasing attention, where task assignment plays a critically important role. However, recent task assignment solutions in SC often assume that data is stored in a central station while ignoring the issue of privacy leakage. To enable decentralized training and privacy protection, we propose a federated task assignment framework with personalized location-preference learning, which performs efficient task assignment while keeping the data decentralized and private in each platform center (e.g., a delivery center of an SC company). The framework consists of two phases: personalized federated location-preference learning and task assignment. Specifically, in the first phase, we design a personalized location-preference learning model for each platform center by simultaneously considering the location information and data heterogeneity across platform centers. Based on workers' location preference, the task assignment phase aims to achieve effective and efficient task assignment by means of the Kuhn-Munkres (KM) algorithm and the newly proposed conditional degree-reduction algorithm. Extensive experiments on real-world data show the effectiveness of the proposed framework.},
	address = {New York, NY, USA},
	author = {Zhong, Xiaolong and Miao, Hao and Qiu, Dazhuo and Zhao, Yan and Zheng, Kai},
	booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
	doi = {10.1145/3583780.3615008},
	isbn = {9798400701245},
	keywords = {federated learning, preference, spatial crowdsourcing, task assignment},
	location = {<conf-loc>, <city>Birmingham</city>, <country>United Kingdom</country>, </conf-loc>},
	numpages = {10},
	pages = {3534–3543},
	publisher = {Association for Computing Machinery},
	series = {CIKM '23},
	title = {Personalized Location-Preference Learning for Federated Task Assignment in Spatial Crowdsourcing},
	url = {https://doi.org/10.1145/3583780.3615008},
	year = {2023},
}

@article{10.1145/3586077,
	abstract = {Archaeological recording is intended to preserve as much information as possible about the finds. However, once the pieces are removed from the site, there is information regarding the original positioning of these pieces that may be lost or not accurately recorded and can be relevant for further studies. This spatial arrangement can also be crucial for subsequent piece restoration or to understand certain aspects of ancient cultures.In this article, we describe a software prototype and a methodology to virtually reconstruct an archaeological site for posterity, once it has been excavated. The system is implemented with a client-server architecture. In the server, a spatial database stores and manages the three-dimensional (3D) models of the finds, as well as several 3D site ground surface models acquired at different times during the excavation process. On the client side, a graphical interface allows the user to manipulate the find models to re-create and virtually reconstruct the original spatial arrangement of the archaeological site. Topological relationships among the finds are stored in the database to provide further spatial analysis. The result is an integrated information system that goes beyond 3D visualization, making the site last for posterity after its excavation and allowing further spatial analysis.},
	address = {New York, NY, USA},
	articleno = {44},
	author = {Calzado-Mart\'{\i}nez, Alberto and Garc\'{\i}a-Fern\'{a}ndez, \'{A}ngel-Luis and Ortega-Alvarado, Lidia M. and Feito-Higueruela, Francisco-Ram\'{o}n},
	doi = {10.1145/3586077},
	issn = {1556-4673},
	issue_date = {September 2023},
	journal = {J. Comput. Cult. Herit.},
	keywords = {Virtual reconstruction, 3D interaction, archaeological recording},
	month = {aug},
	number = {3},
	numpages = {23},
	publisher = {Association for Computing Machinery},
	title = {Integrated Information System for 3D Interactive Reconstruction of an Archaeological Site},
	url = {https://doi.org/10.1145/3586077},
	volume = {16},
	year = {2023},
}

@article{10.1145/3587426,
	abstract = {We study the problem of sub-trajectory nearest-neighbor queries on polygonal curves under the continuous Fr\'{e}chet distance. Given an n vertex trajectory P and an m vertex query trajectory Q, we seek to report a vertex-aligned sub-trajectory P′ of P that is closest to Q, i.e., P′ must start and end on contiguous vertices of P. Since in real data P typically contains a very large number of vertices, we focus on answering queries, without restrictions on P or Q, using only precomputed structures of 𝒪(n) size.We use three baseline algorithms from straightforward extensions of known work; however, they have impractical performance on realistic inputs. Therefore, we propose a new Hierarchical Simplification Tree (HST) data structure and an adaptive clustering-based query algorithm that efficiently explores relevant parts of P. The core of our query methods is a novel greedy-backtracking algorithm that solves the Fr\'{e}chet decision problem using 𝒪(n+m) space and 𝒪O(nm) time in the worst case.Experiments on real and synthetic data show that our heuristic effectively prunes the search space and greatly reduces computations compared to baseline approaches.},
	address = {New York, NY, USA},
	articleno = {14},
	author = {Gudmundsson, Joachim and Pfeifer, John and Seybold, Martin P.},
	doi = {10.1145/3587426},
	issn = {2374-0353},
	issue_date = {June 2023},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Fr\'{e}chet distance, nearest sub-trajectory, greedy decision algorithm, Hierarchical Simplification Tree, metric pruning},
	month = {may},
	number = {2},
	numpages = {24},
	publisher = {Association for Computing Machinery},
	title = {On Practical Nearest Sub-Trajectory Queries under the Fr\'{e}chet Distance},
	url = {https://doi.org/10.1145/3587426},
	volume = {9},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625580,
	abstract = {The mission of resilience of Ukrainian cities calls for international collaboration with the scientific community to increase the quality of information by identifying and integrating information from various news and social media sources. Linked Data technology can be used to unify, enrich, and integrate data from multiple sources. In our work, we focus on datasets about damaging events in Ukraine due to Russia's invasion since February 2022. We convert two selected datasets to Linked Data and enrich them with additional geospatial information. Following that, we present an algorithm for the detection of identical events from different datasets. Our pipeline makes it easy to convert and enrich datasets to integrated Linked Data. The resulting dataset consists of 10K reported events covering damage to hospitals, schools, roads, residential buildings, etc. Finally, we demonstrate in use cases how our dataset can be applied to different scenarios for resilience purposes.},
	address = {New York, NY, USA},
	articleno = {36},
	author = {Attar, Manar and Wang, Shuai and Siebes, Ronald and Kultorp, Eirik},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625580},
	isbn = {9798400701689},
	keywords = {Ukraine resilience, linked geospatial data, data integration, linked open data},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Converting and Enriching Geo-annotated Event Data: Integrating Information for Ukraine Resilience},
	url = {https://doi.org/10.1145/3589132.3625580},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625583,
	abstract = {Dense urban areas like Hamburg strive to increase the attractiveness of cycling to overcome mobility-related problems such as space limitations, air pollution, and noise levels. To address this issue, smart mobility solutions can encourage more people to choose cycling. Green Light Optimal Speed Advisory for bikes (bike-GLOSA) can reduce the number of stops at red lights, allow smoother traveling, and convey a digital advantage to cyclists. However, the city-wide implementation of bike-GLOSA introduces new challenges, including the need for automated lane matching. Humans may employ spatial reasoning to determine the most logical sequence of lanes and associated traffic lights across intersection topologies for a given route. In this paper, we aim to replicate this spatial reasoning using Geospatial Artificial Intelligence (GeoAI). The proposed Machine Learning (ML) model not only overcomes limitations associated with location- and camera-based lane matching approaches. It also outperforms a previous route-based approach by 8\%, with an F1 score of 92\% on our test dataset. We critically examine our approach and real-world results to identify potential limitations and avenues for future research.},
	address = {New York, NY, USA},
	articleno = {40},
	author = {Matthes, Philipp and Jeschor, Daniel and Springer, Thomas},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625583},
	isbn = {9798400701689},
	keywords = {spatial pervasive computing, spatial modeling and reasoning, smart cities, intelligent transportation, geospatial AI},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {GeoAI-Powered Lane Matching for Bike Routes in GLOSA Apps},
	url = {https://doi.org/10.1145/3589132.3625583},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625587,
	abstract = {The proliferation of tracking sensors in today's devices has led to the generation of high-frequency, high-volume streams of mobility data capturing the movements of various objects. These movement data can be enriched with semantic contextual information, such as activities, events, user preferences, and more, generating semantically enriched trajectories. Creating and managing these types of trajectories presents challenges due to the massive data volume and the heterogeneous, complex semantic dimensions. To address these issues, we introduce a novel approach, MAT-Sum, which uses a location-centric enrichment perspective to summarize massive volumes of mobility data while preserving essential semantic information. Our approach enriches geographical areas with semantic aspects to provide the underlying context for trajectories, enabling effective data reduction through trajectory summarization. In the experimental evaluation, we show that MAT-Sum effectively minimizes trajectory volume while retaining a good level of semantic quality, thus presenting a viable solution to the relevant issue of managing massive mobility data.},
	address = {New York, NY, USA},
	articleno = {44},
	author = {Pugliese, Chiara and Lettich, Francesco and Pinelli, Fabio and Renso, Chiara},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625587},
	isbn = {9798400701689},
	keywords = {semantic enrichment, summarized semantic trajectory, multiple aspect trajectory, semantic trajectory},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Summarizing Trajectories Using Semantically Enriched Geographical Context},
	url = {https://doi.org/10.1145/3589132.3625587},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625588,
	abstract = {Thanks to advances in the urban big data, the bike sharing, especially station-based bike sharing, has emerged as the important first-/last-mile connectivities in many smart cities. Bike station network (BSN) expansion recommendation, i.e., recommending placement locations of new stations, is essential for satisfying local mobility demands, enhancing the BSN service quality, and may significantly affect the resource fairness and accessibility of different communities in the neighborhood. Furthermore, the dynamic and complex urban mobility environments make the station placement highly challenging to satisfy the mobility needs.To ease and facilitate the urban planning with awareness of mobility equity, we have designed and proposed CGIRL, a novel equity-aware Cross-Graph Interactive Reinforcement Learning approach for BSN expansion recommendation. Specifically, we have designed a novel reward function within our actor-critic reinforcement learning approach, jointly accounting for the local mobility, bike resource distribution equity, and accessibility of different socioeconomic groups to the expanded stations. To capture the policy of station decisions from the BSN deployment, we integrate the location graph's mobility and equity correlations across the city regions as a graph network, and design a novel cross-graph interaction network with embedding attention and sequential dependency that adaptively captures and interactively differentiates the correlations within station placement. Our extensive experimental studies upon a total of 393 (111 new) bike stations from New York City (NYC), Washington D.C. (DC), and Chicago have validated the effectiveness of CGIRL in the equity-aware BSN expansion recommendation.},
	address = {New York, NY, USA},
	articleno = {45},
	author = {Yang, Xi and He, Suining and Tabatabaie, Mahan},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625588},
	isbn = {9798400701689},
	keywords = {cross-graph interactive reinforcement learning, spatio-temporal graph attention, bike sharing network expansion, equity-aware recommendation},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Equity-Aware Cross-Graph Interactive Reinforcement Learning for Bike Station Network Expansion},
	url = {https://doi.org/10.1145/3589132.3625588},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625590,
	abstract = {Human mobility research has significantly benefited from recent advances in machine learning, as have numerous other industries. Aided by the ever-increasing availability of geospatial and mobility data, machine learning models have enabled large-scale systems for simulating city-wide macro and micro mobility behaviors, urban planning, transportation management, and disaster relief optimization. However, while many fields have invested significant effort in solving the model transferability and generalization problem, the inability of machine learning-based human mobility models to generalize to new locations has come to be implicitly accepted in most geospatial research. In this vision paper, we focus on this geospatial generalization problem, its root causes, and how it is restricting the applications of otherwise-promising research. Most importantly, we argue for several data- and modeling-driven innovations which could help remedy this problem, spanning mega-scale simulations, large foundation models, and multi-task, transfer, and meta-learning. We also spotlight a handful of promising ideas which have recently emerged from the community. We hope that these proposals take root and help develop more capable, flexible, and generalizable models in research and industry.},
	address = {New York, NY, USA},
	articleno = {47},
	author = {Tenzer, Mark and Rasheed, Zeeshan and Shafique, Khurram},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625590},
	isbn = {9798400701689},
	keywords = {domain adaptation, transfer learning, generalization, geospatial data, mobility},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {The Geospatial Generalization Problem: When Mobility Isn't Mobile},
	url = {https://doi.org/10.1145/3589132.3625590},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625591,
	abstract = {Deep learning for Earth imagery plays an increasingly important role in geoscience applications such as agriculture, ecology, and natural disaster management. Still, progress is often hindered by the limited training labels. Given Earth imagery with limited training labels, a base deep neural network model, and a spatial knowledge base with label constraints, our problem is to infer the full labels while training the neural network. The problem is challenging due to the sparse and noisy input labels, spatial uncertainty within the label inference process, and high computational costs associated with a large number of sample locations. Existing works on neuro-symbolic models focus on integrating symbolic logic into neural networks (e.g., loss function, model architecture, and training label augmentation), but these methods do not fully address the challenges of spatial data (e.g., spatial uncertainty, the trade-off between spatial granularity and computational costs). To bridge this gap, we propose a novel Spatial Knowledge-Infused Hierarchical Learning (SKI-HL) framework that iteratively infers sample labels within a multi-resolution hierarchy. Our framework consists of a module to selectively infer labels in different resolutions based on spatial uncertainty and a module to train neural network parameters with uncertainty-aware multi-instance learning. Extensive experiments on real-world flood mapping datasets show that the proposed model outperforms several baseline methods. The code is available at https://github.com/ZelinXu2000/SKI-HL.},
	address = {New York, NY, USA},
	articleno = {48},
	author = {Xu, Zelin and Xiao, Tingsong and He, Wenchong and Wang, Yu and Jiang, Zhe},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625591},
	isbn = {9798400701689},
	keywords = {spatial data mining, neural-symbolic system, knowledge-infused learning},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Spatial Knowledge-Infused Hierarchical Learning: An Application in Flood Mapping on Earth Imagery},
	url = {https://doi.org/10.1145/3589132.3625591},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625598,
	abstract = {The recent advance of adapting pre-trained task-agnostic artificial intelligence (AI) models leads to great successes in downstream tasks via fine-tuning, or low-resource (i.e., few-shot and zero-shot) learning. However, when adapting such pre-trained AI models to geographical applications, it is still challenging to find the "sweet spot" of the model's generalizability and specializability (e.g., geographic generalizability v.s. spatial heterogeneity). For instance, a building detection task may require vision models with different parameters across different geographic areas of the world. In this paper, we rethink this interesting topic, namely Geographical Generalizability of GeoAI models, with a case study of detecting OpenStreetMap (OSM) missing buildings across different countries in sub-Saharan Africa. We consider a real-world scenario, in which we first train a Single-Shot Multibox Detection (SSD) base model for OSM missing building detection in Kakola, Tanzania, where a previous humanitarian mapping project of OSM was organized to map all possible buildings. Then we extrapolate this base model using Few-Shot Transfer Learning (FSTL) to a set of areas in the proximity of the test area in Cameroon. Here, we develop a Geographical Weighted Model Ensemble (GWME) method to improve Geographical Generalizability of GeoAI models. Moreover, we compare four unsupervised model ensemble weighting strategies: 1) Average weighting, 2) Image similarity weighting, 3) Geographical distance weighting, and 4) Self-attention-based weighting. Experiments show promising results of the proposed GWME method, which implicitly generates model weights from their location embedding and image feature embedding in an unsupervised manner. More specifically, the self-attention-based model ensemble achieves the highest performance. The results shed inspiring light on improving the generalizability and replicability of GeoAI models across geographic areas. Data and code are available at https://github.com/tum-bgd/GWME.},
	address = {New York, NY, USA},
	articleno = {55},
	author = {Li, Hao and Wang, Jiapan and Zollner, Johann Maximilian and Mai, Gengchen and Lao, Ni and Werner, Martin},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625598},
	isbn = {9798400701689},
	keywords = {self-attention, humanitarian mapping, model ensemble, vision transformer, OpenStreetMap, GeoAI},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Rethink Geographical Generalizability with Unsupervised Self-Attention Model Ensemble: A Case Study of OpenStreetMap Missing Building Detection in Africa},
	url = {https://doi.org/10.1145/3589132.3625598},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625599,
	abstract = {Predicting traffic accident hotspots is crucial for ensuring public safety, improving transport planning, and reducing transportation costs. Traditional deep learning models, such as Transformers and LSTMs, have been successful in this field but fail to integrate critical attributes essential for accurate prediction. To address these limitations, we propose utilizing a Temporal Convolutional Network (TCN), which efficiently learns spatial, temporal, and other external factors integral to accident hotspot prediction. Our proposed TCN architecture 1 demonstrate superior performance over state-of-the-art methods, offering valuable insights for proactive accident mitigation.},
	address = {New York, NY, USA},
	articleno = {56},
	author = {Yeddula, Sai Deepthi and Jiang, Chen and Hui, Bo and Ku, Wei-Shinn},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625599},
	isbn = {9798400701689},
	keywords = {spatio-temporal data, temporal convolutional networks, traffic accident hotspot prediction},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Traffic Accident Hotspot Prediction Using Temporal Convolutional Networks: A Spatio-Temporal Approach},
	url = {https://doi.org/10.1145/3589132.3625599},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625608,
	abstract = {The process of regionalization involves clustering a set of spatial areas into spatially contiguous regions. Given the NP-hard nature of regionalization problems, all existing algorithms yield approximate solutions. To ascertain the quality of these approximations, it is crucial for domain experts to obtain statistically significant evidence on optimizing the objective function, in comparison to a random reference distribution derived from all potential sample solutions. In this paper, we propose a novel spatial regionalization problem, denoted as SISR (Statistical Inference for Spatial Regionalization), which generates random sample solutions with a predetermined region cardinality. The driving motivation behind SISR is to conduct statistical inference on any given regionalization scheme. To address SISR, we present a parallel technique named PRRP (P-Regionalization through Recursive Partitioning). PRRP operates over three phases: the region growing phase constructs initial regions with a predefined cardinality, while the region merging and region splitting phases ensure the spatial contiguity of unassigned areas, allowing for the growth of subsequent regions with predefined cardinalites. An extensive evaluation shows the effectiveness of PRRP using various real datasets.},
	address = {New York, NY, USA},
	articleno = {65},
	author = {Alrashid, Hussah and Magdy, Amr and Rey, Sergio},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625608},
	isbn = {9798400701689},
	keywords = {spatial clustering, regionalization, statistical inference},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Statistical Inference for Spatial Regionalization},
	url = {https://doi.org/10.1145/3589132.3625608},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625609,
	abstract = {Nearest neighbor (NN) queries are ubiquitous in spatial databases, but have been studied mainly for point data. Inspired by recent work on indexing non-point objects for range queries, we propose a secondary partitioning scheme for space-partitioning indices, tailored to NN search. Our scheme classifies the contents of each primary partition into 16 secondary partitions, considering the begin and end of objects with respect to the spatial extent of the primary partition. Based on this, we design algorithms for both incremental NN and k-NN search that avoid duplicate results and skip unnecessary computations. We compare our scheme to the state-of-the-art indexing and find that it has a significant performance advantage.},
	address = {New York, NY, USA},
	articleno = {33},
	author = {Michalopoulos, Achilleas and Tsitsigkos, Dimitrios and Bouros, Panagiotis and Mamoulis, Nikos and Terrovitis, Manolis},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625609},
	isbn = {9798400701689},
	keywords = {non-point data, distance queries, spatial indexing},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Efficient Nearest Neighbor Queries on Non-point Data},
	url = {https://doi.org/10.1145/3589132.3625609},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625613,
	abstract = {High-definition (HD) maps play an essential role in autonomous driving. However, producing HD map needs huge amount of manual annotations and is thus labor intensive and costly, which limits the widespread use of HD map. To improve the productivity and reduce cost, extensive studies have explored automation of HD map production. Existing studies primarily focus on constructing vectorized map elements from vehicle sensing images and point clouds. However, the follow-up procedure of extracting traffic semantics based on vectorized map elements is lack of study, though it is laborious and costly as well. In this paper, we focus on the automation of inferring traffic light controls for HD map production. To be specific, we aim at associating traffic lights with their controlled roads based on vectorized map data. This problem is not trivial in that: 1) the placement of traffic light contrastive to road varies considerably from scene to scene; 2) even if the placement of traffic lights and roads are similar, the road network layout has great influence on the traffic light controls. To tackle the above challenges, we propose a Heterogeneous Interaction model with Stacked Transformers (HIST) that learns representation from vectorized map elements and encodes contextual information via heterogeneous interactions among different types of map elements. We conduct extensive experiments in major cities of China to validate the efficacy of HIST. Results show HIST achieves accuracy ranging from 96.03\% to 98.85\% in different cities. We further deploy HIST on the HD map production line at AMAP. By incorporating a rule-based confidence system, the whole system achieves the performance with accuracy &gt; 99.9\% and automation rate &gt; 85\%, which meets the industrial level quality requirement and saves vast amount of human labor.},
	address = {New York, NY, USA},
	articleno = {59},
	author = {Liu, Zhicheng and Liao, Yitian and Sun, Zan and Guan, Huankang and Jiang, Danning and Li, Yong},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625613},
	isbn = {9798400701689},
	keywords = {transformer, traffic light association, HD map production},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Which Traffic Light Should You Look at? Automatically Associating Traffic Lights with Roads in High-Definition Map (Industrial Paper)},
	url = {https://doi.org/10.1145/3589132.3625613},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625615,
	abstract = {We present a fire risk management system which takes input data from various sources (e.g., meteorological data, satellite indicators for vegetation, historical burned areas), produces a harmonized spatio-temporal data cube to compute fire risk and enables semantic querying to assist fire risk management. The distinguishing implementation features of the system is the use of data cubes, machine learning algorithms and, most importantly, geospatial ontology-based data access technologies. The system has been implemented in the European project DeepCube for the geographic area of Greece and can be used operationally to assist authorities to determine fire risk during the summer fire season.},
	address = {New York, NY, USA},
	articleno = {34},
	author = {Bilidas, Dimitris and Mantas, Anastasios and Yfantis, Filippos and Stamoulis, George and Koubarakis, Manolis and Kondylatos, Spyros and Prapas, Ioannis and Papoutsis, Ioannis},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625615},
	isbn = {9798400701689},
	keywords = {fire risk, machine learning, ontology based data access, data cubes},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Fire Risk Management using Data Cubes, Machine Learning and OBDA systems},
	url = {https://doi.org/10.1145/3589132.3625615},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625619,
	abstract = {Research on trajectory data mining relies on appropriate datasets, including Gps-based geolocations, check-in data to points of interest (Pois), and synthetic datasets. Even though some data are accessible, the majority of mobility datasets are typically discovered through ad-hoc searches and lack comprehensive documentation of their generation process or source to reproduce curated or customized versions of them. At the same time, there has been a growing interest in a new type of mobility data, describing trajectories as sequences of higher-order geometric elements like hexagons that offer several benefits: (i) reduced sparsity and analysis at different granularity levels, (ii) compatibility with popular machine learning architectures, (iii) improved generalization and reduced overfitting, and (iv) efficient visualization. To this end, we present Point2Hex, a method and tool for generating higher-order mobility flow datasets from raw trajectory data. We used Point2Hex to create higherorder versions of seven popular mobility datasets typically employed in trajectory-related technical problems and downstream tasks, such as trajectory prediction, classification, clustering, imputation, and anomaly detection, to name a few. To promote reuse and encourage reproducibility, we provide the source code and documentation of Point2Hex, as well as the generated higher-order mobility flow datasets in publicly accessible repositories.},
	address = {New York, NY, USA},
	articleno = {69},
	author = {Faraji, Ali and Li, Jing and Alix, Gian and Alsaeed, Mahmoud and Yanin, Nina and Nadiri, Amirhossein and Papagelis, Manos},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625619},
	isbn = {9798400701689},
	keywords = {generator, higher-order mobility flow datasets, trajectory datasets},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Point2Hex: Higher-order Mobility Flow Data and Resources},
	url = {https://doi.org/10.1145/3589132.3625619},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625637,
	abstract = {Traffic assignment (TA) is crucial in optimizing transportation systems and consists in efficiently assigning routes to a collection of trips. Existing TA algorithms often do not adequately consider realtime traffic conditions, resulting in inefficient route assignments. This paper introduces Metis, a coordinated, one-shot TA algorithm that combines alternative routing with edge penalization and informed route scoring. We conduct experiments in several cities to evaluate the performance of Metis against state-of-the-art one-shot methods. Compared to the best baseline, Metis significantly reduces CO2 emissions by 18\% in Milan, 28\% in Florence, and 46\% in Rome, improving trip distribution considerably while still having low computational time. Our study proposes Metis as a promising solution for optimizing TA and urban transportation systems.},
	address = {New York, NY, USA},
	articleno = {87},
	author = {Cornacchia, Giuliano and Nanni, Mirco and Pappalardo, Luca},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625637},
	isbn = {9798400701689},
	keywords = {urban sustainability, CO2 emissions, path diversification, route planning, alternative routing, traffic assignment},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {One-Shot Traffic Assignment with Forward-Looking Penalization},
	url = {https://doi.org/10.1145/3589132.3625637},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625638,
	abstract = {The compression of satellite imagery remains an important research area as hundreds of terabytes of images are collected every day, which drives up storage and bandwidth costs. Although progress has been made in increasing the resolution of these satellite images, many downstream tasks are only interested in small regions of any given image. These areas of interest vary by task but, once known, can be used to optimize how information within the image is encoded. Whereas standard image encoding methods, even those optimized for remote sensing, work on the whole image equally, there are emerging methods that can be guided by saliency maps to focus on important areas. In this work we show how imagery preprocessing techniques driven by saliency maps can be used with traditional lossy compression coding standards to create variable rate image compression within a single large satellite image. Specifically, we use variable sized smoothing kernels that map to different quantized saliency levels to process imagery pixels in order to optimize downstream compression and encoding schemes.},
	address = {New York, NY, USA},
	articleno = {88},
	author = {Downes, Justin and Saltwick, Sam and Chen, Anthony},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625638},
	isbn = {9798400701689},
	keywords = {compression, satellite imagery, image processing},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Saliency Driven Imagery Preprocessing for Efficient Compression - Industrial Paper},
	url = {https://doi.org/10.1145/3589132.3625638},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625645,
	abstract = {Accurate short-term precipitation prediction at a high spatial resolution is crucial for effective urban water management, flooding warning, and mitigation. However, conventional numerical weather models usually face the challenge of systematic errors and spatiotemporal biases due to an inadequate understanding of many processes and unrealistic parameterizations. In recent years, deep learning techniques have gained popularity as a tool in precipitation forecasting and risk pre-warning. To support deep learning for precipitation forecasting and flooding warning, this paper introduces a large-scale multimodal Geo dataset. This dataset incorporates spatially connected features and real-world climate data, enabling the prediction of extreme precipitations. The dataset comprises Multi-Radar/Multi-Sensor System (MRMS), High-Resolution Rapid Refresh (HRRR), Geostationary Satellite Server (GOES) data, and local hydrological data from the United States Geological Survey (USGS), providing a diverse array of information sources. The compiling of multi-source data within the proposed multimodal Geo scope can improve prediction accuracy over uni-modal data and shows high accuracy in predicting heavy rain when integrating Transformer, which offers the opportunity for more efficient urban water management and improved disaster response strategies. By providing a comprehensive view of environmental conditions, this dataset enables a deeper understanding of precipitation patterns, facilitating effective mitigation efforts.},
	address = {New York, NY, USA},
	articleno = {95},
	author = {Jiang, Chen and Wang, Wenlu and Pan, Naiqing and Ku, Wei-Shinn},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625645},
	isbn = {9798400701689},
	keywords = {transformer, deep learning, precipitation, short-term forecast, spatial-temporal analysis},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {A Multimodal Geo Dataset for High-resolution Precipitation Forecasting},
	url = {https://doi.org/10.1145/3589132.3625645},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625646,
	abstract = {The widespread of geotagged data combined with modern map services allows for the accurate attachment of data to spatial networks. Applying statistical analysis, such as hotspot detection, over spatial networks is very important for precise quantification and patterns analysis, which empowers effective decision-making in various important applications. Existing hotspot detection algorithms on spatial networks either lack statistical evidence on detected hotspots, such as clustering, or they provide statistical evidence at a prohibitive computational overhead. In this paper, we propose efficient algorithms for detecting hotspots based on the network local K-function for predefined and unknown hotspot radii. The network local K-function is a widely adopted statistical approach for network pattern analysis that enables the understanding of the density and distribution of activities and events in the spatial network. However, its practical application has been limited due to the inefficiency of existing algorithms, particularly for large-sized networks. Extensive experimental evaluation using real and synthetic datasets shows that our algorithms are up to 28 times faster than the state-of-the-art algorithms in computing hotspots with a predefined radius and up to more than four orders of magnitude faster in identifying hotspots without a predefined radius.},
	address = {New York, NY, USA},
	articleno = {96},
	author = {Liu, Yongyi and Kang, Yunfan and Mahmood, Ahmed and Magdy, Amr},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625646},
	isbn = {9798400701689},
	keywords = {spatial network, K-function, hotspot detection},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Scalable Evaluation of Local K-Function for Radius-Accurate Hotspot Detection in Spatial Networks},
	url = {https://doi.org/10.1145/3589132.3625646},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625656,
	abstract = {Measuring spatial accessibility to healthcare resources and facilities has long been an important problem in public health. For example, during disease outbreaks, sharing spatial accessibility data such as individual travel distances to health facilities is vital to policy making and designing effective interventions. However, sharing these data may raise privacy concerns, as information about individual data contributors (e.g., health status and residential address) may be disclosed. In this work, we investigate those unintended information leakage in spatial accessibility analysis. Specifically, we are interested in understanding whether sharing data for spatial accessibility computations may disclose individual participation (i.e., membership inference) and personal identifiable information (i.e., address inference). Furthermore, we propose two provably private algorithms that mitigate those privacy risks. The evaluation is conducted with real population and healthcare facilities data from Mecklenburg county, NC and Nashville, TN. Compared to state-of-the-art privacy practices, our methods effectively reduce the risks of membership and address disclosure, while providing useful data for spatial accessibility analysis.},
	address = {New York, NY, USA},
	articleno = {106},
	author = {Fan, Liyue and Bonomi, Luca},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625656},
	isbn = {9798400701689},
	keywords = {health informatics, spatial accessibility, privacy},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {11},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Hide Your Distance: Privacy Risks and Protection in Spatial Accessibility Analysis},
	url = {https://doi.org/10.1145/3589132.3625656},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625657,
	abstract = {Accessing realistic human movements (aka trajectories) is essential for many application domains, such as urban planning, transportation, and public health. However, due to privacy and commercial concerns, real-world trajectories are not readily available, giving rise to an important research area of generating synthetic but realistic trajectories. Inspired by the success of deep neural networks (DNN), data-driven methods learn the underlying human decision-making mechanisms and generate synthetic trajectories by directly fitting real-world data. However, these DNN-based approaches do not exploit people's moving behaviors (e.g., work commute, shopping purpose), significantly influencing human decisions during the generation process. This paper proposes MBP-GAIL, a novel framework based on generative adversarial imitation learning that synthesizes realistic trajectories that preserve moving behavior patterns in real data. MBP-GAIL models temporal dependencies by Recurrent Neural Networks (RNN) and combines the stochastic constraints from moving behavior patterns and spatial constraints in the learning process. Through comprehensive experiments, we demonstrate that MBP-GAIL outperforms state-of-the-art methods and can better support decision making in trajectory simulations.},
	address = {New York, NY, USA},
	articleno = {107},
	author = {Lin, Haowen and Shaham, Sina and Chiang, Yao-Yi and Shahabi, Cyrus},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625657},
	isbn = {9798400701689},
	keywords = {GAIL, trajectory generation},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Generating Realistic and Representative Trajectories with Mobility Behavior Clustering},
	url = {https://doi.org/10.1145/3589132.3625657},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625658,
	abstract = {Bavaria Buildings is a large, analysis-ready dataset providing openly available co-registered 40cm aerial imagery of Upper Bavaria paired with building footprint information. The Bavaria Buildings dataset (BBD) contains 18205 orthophotos of 2500 \texttimes{} 2500 pixels, where each pixel covers 40cm \texttimes{} 40cm in space (Digitales Orthophoto 40cm - DOP40). The dataset has been pre-processed and co-registered and also provides a set of 5.5 million image tiles of 250 \texttimes{} 250 pixels ready for deep learning and image analysis tasks. For each image tile, we provide two segmentation masks; one based on the official building footprints (Hausumringe) data as published by the Free State of Bavaria and one based on a historic OpenStreetMap (OSM) extract dating to 2021. The dataset is ready for essential analysis tasks, such as detection, segmentation, instance extraction, footprint geometry extraction, multimodal localization, and multimodal data quality assessment of buildings in Bavaria. We plan to update the dataset with each major re-publication of the upstream data sources to foster change detection research in the future. The BBD is available at https://doi.org/10.14459/2023mp1709451.},
	address = {New York, NY, USA},
	articleno = {108},
	author = {Werner, Martin and Li, Hao and Zollner, Johann Maximilian and Teuscher, Balthasar and Deuser, Fabian},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625658},
	isbn = {9798400701689},
	keywords = {geospatial artificial intelligence, data quality, OpenStreetMap, building detection, very high resolution, orthophoto, dataset},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Bavaria Buildings - A Novel Dataset for Building Footprint Extraction, Instance Segmentation, and Data Quality Estimation},
	url = {https://doi.org/10.1145/3589132.3625658},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625659,
	abstract = {Locating undiscovered deposits of critical minerals requires accurate geological data. However, most of the 100,000 historical geological maps of the United States Geological Survey (USGS) are in raster format. This hinders critical mineral assessment. We target the problem of extracting geological features represented as polygons from raster maps. We exploit the polygon metadata that provides information on the geological features, such as the map keys indicating how the polygon features are represented, to extract the features. We present a metadata-driven machine-learning approach that encodes the raster map and map key into a series of bitmaps and uses a convolutional model to learn to recognize the polygon features. We evaluated our approach on USGS geological maps; our approach achieves a median F1 score of 0.809 and outperforms state-of-the-art methods by 4.52\%.},
	address = {New York, NY, USA},
	articleno = {109},
	author = {Lin, Fandel and Knoblock, Craig A. and Shbita, Basel and Vu, Binh and Li, Zekun and Chiang, Yao-Yi},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625659},
	isbn = {9798400701689},
	keywords = {image processing, polygon extraction, raster map},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Exploiting Polygon Metadata to Understand Raster Maps - Accurate Polygonal Feature Extraction},
	url = {https://doi.org/10.1145/3589132.3625659},
	year = {2023},
}

@inproceedings{10.1145/3589132.3625660,
	abstract = {Routing long trips of electric vehicles (EVs) is in growing demand and a non-trivial task as charging stops have to be planned along the way. Developing and testing realistic EV routing algorithms is challenging as multiple factors have to be considered such as traffic conditions, charging station availability, and car properties relevant to energy-consumption modeling. Moreover, testing and evaluating such algorithms requires realistic data and tools to simulate energy consumption and charging. This paper demonstrates a web-based testbed system for EV routing algorithms. Users can input start and end points on the map, set the car properties that influence the energy consumption, and adjust the charging station availability to see how the results change. The system visualizes a set of proposed routes as well as a number of alternative routes considered (but discarded) by the routing algorithm. Details of each leg of each route can be interactively explored. The highly configurable system allows the algorithm developers to ask what-if and why-not questions.},
	address = {New York, NY, USA},
	articleno = {71},
	author = {Makulavi\v{c}ius, Nojus and Brilingaitundefined, Agnundefined and Bukauskas, Linas and \v{C}ivilis, Alminas and Krinickij, Virgilijus and \v{S}altenis, Simonas},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3625660},
	isbn = {9798400701689},
	keywords = {testbed, spatio-temporal query, GIS, visualization, testing, electric vehicle routing},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {E-TRI: E-Vehicle Testbed Routing Infrastructure},
	url = {https://doi.org/10.1145/3589132.3625660},
	year = {2023},
}

@inproceedings{10.1145/3589132.3628363,
	abstract = {The proliferation of motion sensors has significantly contributed to the availability of mobility data. An important line of research focuses on augmenting these datasets with diverse semantic information, referred to as aspects, thereby yielding multiple aspect trajectories (MATs). However, a notable gap in the existing literature pertains to the absence of methodologies for obtaining MATs and the scarcity of real-world datasets. To address this gap, we introduce MAT-Builder, an innovative system designed to facilitate the customization of semantic enrichment of trajectories through the use of arbitrary aspects and external data sources. Notably, the richness of information endowed by MAT-Builder may introduce challenges in terms of data management and storage. Consequently, we propose MAT-Sum, an approach tailored to summarize trajectories while preserving their semantic information.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Pugliese, Chiara},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3628363},
	isbn = {9798400701689},
	keywords = {semantic enrichment, summarized semantic trajectory, multiple aspect trajectory, semantic trajectory},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Semantic-aware building and summarization of multiple aspect trajectories},
	url = {https://doi.org/10.1145/3589132.3628363},
	year = {2023},
}

@inproceedings{10.1145/3589132.3628370,
	abstract = {Harmful Algal Blooms (HABs) present significant environmental and public health threats. Recent machine learning-based HABs monitoring methods often rely solely on unimodal data, e.g., satellite imagery, overlooking crucial environmental factors such as temperature. Moreover, existing multi-modal approaches grapple with real-time applicability and generalizability challenges due to the use of ensemble methodologies and hard-coded geolocation clusters. Addressing these gaps, this paper presents a novel deep learning model using a single-model-based multi-task framework. This framework is designed to segment water bodies and predict HABs severity levels concurrently, enabling the model to focus on areas of interest, thereby enhancing prediction accuracy. Our model integrates multimodal inputs, i.e., satellite imagery, elevation data, temperature readings, and geolocation details, via a dual-branch architecture: the Satellite-Elevation (SE) branch and the Temperature-Geolocation (TG) branch. Satellite and elevation data in the SE branch, being spatially coherent, assist in water area detection and feature extraction. Meanwhile, the TG branch, using sequential temperature data and geolocation information, captures temporal algal growth patterns and adjusts for temperature variations influenced by regional climatic differences, ensuring the model's adaptability across different geographic regions. Additionally, we propose a geometric multimodal focal loss to further enhance representation learning. On the Tick-Tick Bloom (TTB) dataset, our approach outperforms the SOTA methods by 15.65\%.},
	address = {New York, NY, USA},
	articleno = {9},
	author = {Zhao, Fei and Zhang, Chengcui},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3628370},
	isbn = {9798400701689},
	keywords = {harmful algal blooms, deep learning, computer vision, geolocation},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Deep Learning for HABs Prediction with Multimodal Fusion},
	url = {https://doi.org/10.1145/3589132.3628370},
	year = {2023},
}

@inproceedings{10.1145/3589132.3628371,
	abstract = {This paper introduces innovative approaches for extracting geographic objects from scanned map images. Overcoming the challenges associated with labor-intensive data labeling and inaccurate extraction results, we present two data labeling methods and an accurate-enhancing extraction method. The experiment in this paper shows that our approaches outperform the baselines.},
	address = {New York, NY, USA},
	articleno = {10},
	author = {Duan, Weiwei},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3628371},
	isbn = {9798400701689},
	keywords = {geospatial information sciences, map processing, object detection, transformer, variational autoencoder, generative models},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {2},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Efficient and Accurate Object Extraction from Scanned Maps by Leveraging External Data and Learning Representative Context},
	url = {https://doi.org/10.1145/3589132.3628371},
	year = {2023},
}

@inproceedings{10.1145/3589132.3629970,
	abstract = {Rising summer temperatures in Greenland have accelerated the formation of supraglacial lakes. Since these lakes play a significant role in ice sheet dynamics and bed lubrication, their continuous monitoring in a warming Arctic is becoming essential. The 31st ACM SIGSPATIAL competition (GISCUP 2023) aims to automate the detection of these lakes using satellite imagery. In this paper, we present two solutions to this problem based on image segmentation techniques: a DeepLabv3+ model that ranked first, and a U-Net-based approach that ranked fourth. We provide details about our implementations and explain the rationale behind our choices and the challenges we faced. Our results contribute to the understanding of supraglacial lake fluctuations and offer a valuable tool for ongoing environmental monitoring.},
	address = {New York, NY, USA},
	articleno = {15},
	author = {Walther, Simon and Cseres, Leonard and Marquis, R\'{e}my and Chapuis, Bertil and Perez-Uribe, Andres},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3629970},
	isbn = {9798400701689},
	keywords = {detection, segmentation, computer vision, satellite imagery},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Diving into Supraglacial Lakes Detection: a Deep Semantic Segmentation Approach},
	url = {https://doi.org/10.1145/3589132.3629970},
	year = {2023},
}

@inproceedings{10.1145/3589132.3629971,
	abstract = {The ACM SIGSPATIAL Cup 2023 proposed the challenge to identify and map supraglacial lakes in Greenland in satellite imagery. The peculiarities of supraglacial lakes pose a hard problem for semantic segmentation and object detection tasks because the definition of a lake is ill-fitted to the inner workings of such approaches. For example, lakes are often covered by ice and snow and narrow streams can connect distinct lakes, which is not directly translatable to the semantic segmentation of water. It is also not well-posed for object detection, especially the identity relation - what is a lake, what is not (yet) a lake, and what are two lakes is challenging. In this context, we worked on adapting semantic segmentation using the Segment Anything Model and instance segmentation using Mask R-CNN to the setting. The latter ended up superior in our own evaluation and even got ranked second among all participants. We are proud that our approach has led to competitive performance. The source code is available from https://github.com/tum-bgd/GISCup23.},
	address = {New York, NY, USA},
	articleno = {16},
	author = {Luo, Xuanshu and Walther, Paul and Mansour, Wejdene and Teuscher, Balthasar and Zollner, Johann Maximilian and Li, Hao and Werner, Martin},
	booktitle = {Proceedings of the 31st ACM International Conference on Advances in Geographic Information Systems},
	doi = {10.1145/3589132.3629971},
	isbn = {9798400701689},
	keywords = {segment anything model, mask R-CNN, computer vision, satellite imagery, image segmentation, supraglacial lakes},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	publisher = {Association for Computing Machinery},
	series = {SIGSPATIAL '23},
	title = {Exploring GeoAI Methods for Supraglacial Lake Mapping on Greenland Ice Sheet},
	url = {https://doi.org/10.1145/3589132.3629971},
	year = {2023},
}

@article{10.1145/3589321,
	abstract = {The acquisition of accurate rainfall distribution in space is an important task in hydrological analysis and natural disaster pre-warning. However, it is impossible to install rain gauges on every corner. Spatial interpolation is a common way to infer rainfall distribution based on available raingauge data. However, the existing works rely on some unrealistic pre-settings to capture spatial correlations, which limits their performance in real scenarios. To tackle this issue, we propose the SSIN, which is a novel data-driven self-supervised learning framework for rainfall spatial interpolation by mining latent spatial patterns from historical observation data. Inspired by the Cloze task and BERT, we fully consider the characteristics of spatial interpolation and design the SpaFormer model based on the Transformer architecture as the core of SSIN. Our main idea is: by constructing rich self-supervision signals via random masking, SpaFormer can learn informative embeddings for raw data and then adaptively model spatial correlations based on rainfall spatial context. Extensive experiments on two real-world raingauge datasets show that our method outperforms the state-of-the-art solutions. In addition, we take traffic spatial interpolation as another use case to further explore the performance of our method, and SpaFormer achieves the best performance on one large real-world traffic dataset, which further confirms the effectiveness and generality of our method.},
	address = {New York, NY, USA},
	articleno = {176},
	author = {Li, Jia and Shen, Yanyan and Chen, Lei and Ng, Charles Wang Wai},
	doi = {10.1145/3589321},
	issue_date = {June 2023},
	journal = {Proc. ACM Manag. Data},
	keywords = {self-supervised learning, spatial interpolation, transformer},
	month = {jun},
	number = {2},
	numpages = {21},
	publisher = {Association for Computing Machinery},
	title = {SSIN: Self-Supervised Learning for Rainfall Spatial Interpolation},
	url = {https://doi.org/10.1145/3589321},
	volume = {1},
	year = {2023},
}

@article{10.1145/3591361,
	abstract = {Travel time estimation (TTE) is a crucial task in intelligent transportation systems, which has been widely used in navigation and route planning. In recent years, several deep learning frameworks have been proposed to capture the dynamic features of road segments or intersections for travel time estimation. However, most existing works do not consider the joint features of the intersections and road segments. Moreover, most deep neural networks for TTE are designed based on empirical knowledge. Since the independent and joint features of intersections and road segments commonly vary with different datasets, the empirical deterministic neural architectures have limited adaptability to different scenarios. To tackle the above problems, we propose a novel automated deep learning framework, namely Automated Spatio-Temporal Dual Graph Convolutional Networks (Auto-STDGCN), for travel time estimation. Specifically, we propose to construct the node-wise graph and edge-wise graph to characterize the spatio-temporal features of intersections and road segments, respectively. In order to capture the joint spatio-temporal correlations of the dual graphs, a hierarchical neural architecture search approach is introduced, whose search space is composed of internal and external search space. In the internal search space, spatial graph convolution and temporal convolution operations are adopted to capture the respective spatio-temporal correlations of the dual graphs. Further, we design the external search space including the node-wise and edge-wise graph convolution operations from the internal architecture search to capture the interaction patterns between the intersections and road segments. We evaluate our proposed model Auto-STDGCN on three real-world datasets, which demonstrates that our model is significantly superior to the state-of-the-art methods. In addition, we also conduct case studies to visualize and explain the neural architectures learned by our model.},
	address = {New York, NY, USA},
	articleno = {64},
	author = {Jin, Guangyin and Yan, Huan and Li, Fuxian and Li, Yong and Huang, Jincai},
	doi = {10.1145/3591361},
	issn = {2157-6904},
	issue_date = {August 2023},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {Travel time estimation, spatio-temporal correlations, graph neural networks, road modeling, neural architecture search},
	month = {jun},
	number = {4},
	numpages = {23},
	publisher = {Association for Computing Machinery},
	title = {Dual Graph Convolution Architecture Search for Travel Time Estimation},
	url = {https://doi.org/10.1145/3591361},
	volume = {14},
	year = {2023},
}

@inproceedings{10.1145/3603163.3609080,
	abstract = {We present News in Time and Space (NiTS), a virtual reality application for visualization, filtering and interaction with geo-referenced events based on GDELT. It can be used both via VR glasses and as a desktop solution for shared use by multiple users with Ubiq. The aim of NiTS is to provide overviews of global events and trends in order to create a resource for their monitoring and analysis.},
	address = {New York, NY, USA},
	articleno = {7},
	author = {Gagel, Julian and Hustedt, Jasper and L\"{u}ttig, Timo and Berg, Theresa and Abrami, Giuseppe and Mehler, Alexander},
	booktitle = {Proceedings of the 34th ACM Conference on Hypertext and Social Media},
	doi = {10.1145/3603163.3609080},
	isbn = {9798400702327},
	keywords = {geographic information systems, human data interaction, spatial computing, virtual hypertext, virtual reality, virtual reality simulation},
	location = {Rome, Italy},
	numpages = {3},
	publisher = {Association for Computing Machinery},
	series = {HT '23},
	title = {News in Time and Space: Global Event Exploration in Virtual Reality},
	url = {https://doi.org/10.1145/3603163.3609080},
	year = {2023},
}

@inproceedings{10.1145/3603555.3608537,
	abstract = {A change towards sustainability is crucial also in the transportation sector. However, humans’ need for mobility is increasing. Therefore, a shift towards sustainable transportation is necessary. Besides changes in reliability and availability, transportation can be made more attractive by better understanding travelers’ needs. This paper presents first results of a study with 23 participants that traveled by public transport. We continuously recorded travel satisfaction, stress, event, and cardiological data. We plotted the collected data averaged in 0.0008 degree squares over the traveled map to detect points of collective positive or negative travel experience in public transport. Preliminary results show that bothersome people are a major cause for negative experience at a busy changing point. This adds to existing research, as most previous studies have concentrated on time- and cost aspects of public transport. Our method is a powerful tool to analyze points at which a transportation system needs to change to take the traveler’s needs into account.},
	address = {New York, NY, USA},
	author = {Bosch, Esther and Luther, Ricarda and Ihme, Klas},
	booktitle = {Proceedings of Mensch Und Computer 2023},
	doi = {10.1145/3603555.3608537},
	isbn = {9798400707711},
	keywords = {affective computing, cardiological data, geospatial data, public transport, travel experience},
	location = {<conf-loc>, <city>Rapperswil</city>, <country>Switzerland</country>, </conf-loc>},
	numpages = {5},
	pages = {417–421},
	publisher = {Association for Computing Machinery},
	series = {MuC '23},
	title = {Travel Experience in Public Transport: A Geospatial Analysis by Experience Mapping},
	url = {https://doi.org/10.1145/3603555.3608537},
	year = {2023},
}

@article{10.1145/3604246,
	abstract = {Navigation assistance systems have become integral to our daily routines, helping us to find our way through unfamiliar environments. However, their use may come at a price, as empirical evidence suggests a potentially harmful impact of these systems on our spatial abilities, including the acquisition of spatial knowledge. This could be remedied by giving users more freedom and involving them in the decision-making process. Therefore, we present a navigation system that combines augmented reality and Beeline Navigation (BeeAR). Here, the location of the destination is overlaid with a digital landmark and permanently displayed to the user via a visual, translucent AR display (without a map). Since the digital content is integrated into the real world, no mapping between the device and reality is required, potentially lowering the workload. Making one's own decisions along the route is expected to increase engagement with the environment, leading to increased acquisition of spatial knowledge. We compare BeeAR with findings from a previous study comparing Free Choice Navigation (FCN) and Turn-by-Turn (TBT) navigation conducted along the same routes on the outskirts of Vienna, Austria. Although BeeAR and FCN do not provide users with a map, BeeAR users could better retrace the walked route and remembered more points of interest along the route than FCN users. Participants of all three navigation conditions achieved a high configuration similarity between drawn points of interest and their true locations, albeit only one navigation condition included a map.},
	address = {New York, NY, USA},
	articleno = {199},
	author = {Mazurkiewicz, Bartosz and Galv\~{a}o, Marcelo de Lima and Giannopoulos, Ioannis},
	doi = {10.1145/3604246},
	issue_date = {September 2023},
	journal = {Proc. ACM Hum.-Comput. Interact.},
	keywords = {augmented reality, beeline navigation, spatial knowledge acquisition},
	month = {sep},
	number = {MHCI},
	numpages = {17},
	publisher = {Association for Computing Machinery},
	title = {BeeAR: Augmented Reality Beeline Navigation for Spatial Knowledge Acquisition},
	url = {https://doi.org/10.1145/3604246},
	volume = {7},
	year = {2023},
}

@article{10.1145/3604941,
	abstract = {Understanding human mobility has become an important aspect of location-based services in tasks such as personalized recommendation and individual moving pattern recognition, enabled by the large volumes of data from geo-tagged social media (GTSM). Prior studies mainly focus on analyzing human historical footprints collected by GTSM and assuming the veracity of the data, which need not hold when some users are not willing to share their real footprints due to privacy concerns—thereby affecting reliability/authenticity. In this study, we address the problem of Inferring Real Mobility (IRMo) of users, from their unreliable historical traces. Tackling IRMo is a non-trivial task due to the: (1) sparsity of check-in data; (2) suspicious counterfeit check-in behaviors; and (3) unobserved dependencies in human trajectories. To address these issues, we develop a novel Graph-enhanced Attention model called IRMoGA, which attempts to capture underlying mobility patterns and check-in correlations by exploiting the unreliable spatio-temporal data. Specifically, we incorporate the attention mechanism (rather than solely relying on traditional recursive models) to understand the regularity of human mobility, while employing a graph neural network to understand the mutual interactions from human historical check-ins and leveraging prior knowledge to alleviate the inferring bias. Our experiments conducted on four real-world datasets demonstrate the superior performance of IRMoGA over several state-of-the-art baselines, e.g., up to 39.16\% improvement regarding the Recall score on Foursquare.},
	address = {New York, NY, USA},
	articleno = {12},
	author = {Gao, Qiang and Fu, Hongzhu and Zhang, Kunpeng and Trajcevski, Goce and Teng, Xu and Zhou, Fan},
	doi = {10.1145/3604941},
	issn = {2157-6904},
	issue_date = {February 2024},
	journal = {ACM Trans. Intell. Syst. Technol.},
	keywords = {self-attention, graph learning, human mobility, POI graph, Fake check-ins},
	month = {jan},
	number = {1},
	numpages = {25},
	publisher = {Association for Computing Machinery},
	title = {Inferring Real Mobility in Presence of Fake Check-ins Data},
	url = {https://doi.org/10.1145/3604941},
	volume = {15},
	year = {2024},
}

@inproceedings{10.1145/3607834.3616570,
	abstract = {Satellite imagery, UAV imagery, and geo-referenced underwater photo transects (from the surface) are different methods used in marine monitoring and benthic habitat mapping applications to collect observations at different spatial scales. There are however challenges in linking them all together to provide fine-grained mapping and analysis for underwater, benthic habitats with complex geometric and ecological properties. We propose a novel framework called SkySea that offers users access to aligned observational data at multiple spatial scales. SkySea can integrate satellite images (e.g., from SENTINEL-2 at 10m resolution), UAV images (&lt;5cm ground sampling distance), detailed underwater images, 3D reconstruction of the seafloor/benthos from underwater images, and make the data available through a commonly used user interface, such as QGIS. Initial evaluation indicates that the spatial overlay achieves sub-meter-level accuracy, while the underwater 3D reconstruction reaches an average relative error of less than 10\% for size estimation with reference objects. We believe that this is a novel and innovative framework to achieve a seamless connection across an enormous gap of scales from satellite images, regional UAV images, local underwater images and local 3D reconstruction of the underwater environment, for benthic habitat mapping. It enables marine biologists to perform survey planning, species mapping, and model validation tasks in an integrated pipeline.},
	address = {New York, NY, USA},
	author = {Do, Brendan and Liu, Jiajun and Wang, Ziwei and Kusy, Brano and Merz, Torsten and Steven, Andy and Carlin, Geoffrey and Crosswell, Joseph and Li, Yang and Mortimer, Nicholas and Nayyeri, Fereshteh and Vanderklift, Mat and Wilson, Mark},
	booktitle = {Proceedings of the 2023 Workshop on UAVs in Multimedia: Capturing the World from a New Perspective},
	doi = {10.1145/3607834.3616570},
	isbn = {9798400702860},
	keywords = {3d reconstruction, benthic habitat mapping, image alignment},
	location = {<conf-loc>, <city>Ottawa ON</city>, <country>Canada</country>, </conf-loc>},
	numpages = {3},
	pages = {69–71},
	publisher = {Association for Computing Machinery},
	series = {UAVM '23},
	title = {SkySea: Connecting Satellite, UAV and Underwater Imagery for Benthic Habitat Mapping},
	url = {https://doi.org/10.1145/3607834.3616570},
	year = {2023},
}

@inproceedings{10.1145/3609956.3609963,
	abstract = {Highways play a crucial role in transportation services as they facilitate long-distance traveling and allow driving at an almost constant speed, thus resulting in lower fuel consumption and emissions. Many existing highway systems were designed before practical computational tools had been developed. Furthermore, most existing approaches to evaluating highways focus on analyzing mobility data rather than studying the design of the highway system. To address this gap in existing research, in this paper, we study the problem of evaluating the efficacy of the design of real-world highway systems. To this end, we propose two novel measures for the efficacy of highway systems, along with algorithms to compute them. In addition, we present a first-cut heuristic algorithm that aims at computing a highway system that optimizes our proposed measures. In our experiments, we demonstrate the potential of our methods in measuring the efficacy of real-world highway systems. We also evaluate the performance of our heuristic algorithm in computing a rough design of an efficient highway system.},
	address = {New York, NY, USA},
	author = {Chondrogiannis, Theodoros and Grossniklaus, Michael},
	booktitle = {Proceedings of the 18th International Symposium on Spatial and Temporal Data},
	doi = {10.1145/3609956.3609963},
	isbn = {9798400708992},
	keywords = {GIS, highway systems, transportation network analysis},
	location = {<conf-loc>, <city>Calgary</city>, <state>AB</state>, <country>Canada</country>, </conf-loc>},
	numpages = {10},
	pages = {1–10},
	publisher = {Association for Computing Machinery},
	series = {SSTD '23},
	title = {Highway Systems: How Good are They, Really?},
	url = {https://doi.org/10.1145/3609956.3609963},
	year = {2023},
}

@inproceedings{10.1145/3609956.3609970,
	abstract = {Human mobility influences our society and vice versa. During the COVID-19 pandemic, non-pharmaceutical intervention that alters activity-based mobility such as work-from-home greatly impacted human mobility patterns. Many studies on developing mitigation strategies have employed or implemented their own mobility intervention within their model assumption. For fair evaluation between intervention strategies across models, it is significant to set up compatible experimental environments. However, it is difficult to apply the identical intervention to different kinds of models and compare their effectiveness because each model might have different assumptions, capabilities, and implementations. Even if one can apply intervention to heterogeneous models, it may produce undesirable artifacts due to difference of models and integration with intervention. Therefore, minimizing undesirable artifacts and facilitating intervention experiments across heterogeneous models are substantial. Taking this into account, this paper investigates a design of activity-based mobility intervention (ABMI). We define ABMI together with related concepts and develop an extensible data model and schema of ABMI based on the 5W1H method that can be used in different models. As a case study, we apply the ABMI model to a micro-simulation to demonstrate the usability of the proposed model. We expect that standardized ABMI and interfaces may help to streamline development and experiments of intervention strategies across heterogeneous models.},
	address = {New York, NY, USA},
	author = {Kim, Joon-Seok and Thakur, Gautam Malviya and Christopher, Steven Carter},
	booktitle = {Proceedings of the 18th International Symposium on Spatial and Temporal Data},
	doi = {10.1145/3609956.3609970},
	isbn = {9798400708992},
	keywords = {activity, activity-based mobility, human mobility, intervention, micro-simulation, modeling and simulation, trajectory},
	location = {<conf-loc>, <city>Calgary</city>, <state>AB</state>, <country>Canada</country>, </conf-loc>},
	numpages = {10},
	pages = {131–140},
	publisher = {Association for Computing Machinery},
	series = {SSTD '23},
	title = {A Design of Activity-Based Mobility Intervention},
	url = {https://doi.org/10.1145/3609956.3609970},
	year = {2023},
}

@article{10.1145/3611011,
	abstract = {Regionalization techniques group spatial areas into a set of homogeneous regions to analyze and draw conclusions about spatial phenomena. A recent regionalization problem, called MP-regions, groups spatial areas to produce a maximum number of regions by enforcing a user-defined constraint at the regional level. The MP-regions problem is NP-hard. Existing approximate algorithms for MP-regions do not scale for large datasets due to their high computational cost and inherently centralized approaches to process data. This article introduces a parallel scalable regionalization framework (PAGE) to support MP-regions on large datasets. The proposed framework works in two stages. The first stage finds an initial solution through randomized search, and the second stage improves this solution through efficient heuristic search. To build an initial solution efficiently, we extend traditional spatial partitioning techniques to enable parallelized region building without violating the spatial constraints. Furthermore, we optimize the region building efficiency and quality by tuning the randomized area selection to trade off runtime with region homogeneity. The experimental evaluation shows the superiority of our framework to support an order of magnitude larger datasets efficiently compared to the state-of-the-art techniques while producing high-quality solutions.},
	address = {New York, NY, USA},
	articleno = {21},
	author = {Alrashid, Hussah and Liu, Yongyi and Magdy, Amr},
	doi = {10.1145/3611011},
	issn = {2374-0353},
	issue_date = {September 2023},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {Max-p regions, max-p regionalization, spatial clustering, regionalization, spatial regionalization},
	month = {sep},
	number = {3},
	numpages = {26},
	publisher = {Association for Computing Machinery},
	title = {PAGE: Parallel Scalable Regionalization Framework},
	url = {https://doi.org/10.1145/3611011},
	volume = {9},
	year = {2023},
}

@inproceedings{10.1145/3611314.3616066,
	abstract = {This paper presents concepts and approaches towards a climate and energy oriented digital twin for public buildings. The sustainable, resource-efficient operation of these buildings, such as schools and education centers, and the monitoring, control, and optimization of their climate, air, and energy performance pose multiple challenges, in particular to cope with the consequences of climate change and changes in the energy economy. In our approach, we consider buildings in which a network of heterogeneous sensors in each spatial unit records key properties such as temperature, humidity, and CO2 concentration, as well as energy consumption and solar energy production. The continuously collected sensor data forms a spatio-temporal data space, which is used by the digital twin as a basis for AI-based analyses and simulations. The transfer of time-series data in near real time can be done by different databases. Analysis techniques focusing on time-series data allow for targeted access to the information and support the identification of exceptional events, recurring patterns, and the comparison of energy and climate-related performance. A prototype of an energy- and climate-oriented digital twin is currently being implemented in a government project in Andalusia, Spain, covering about 430 public buildings.},
	address = {New York, NY, USA},
	articleno = {31},
	author = {Doellner, Juergen and Merino Cordoba, Salvador and Guzman Navarro, Francisco and Martinez, Javier and De Dios Lara, Juan and Guzman, Rafael},
	booktitle = {Proceedings of the 28th International ACM Conference on 3D Web Technology},
	doi = {10.1145/3611314.3616066},
	isbn = {9798400703249},
	keywords = {BIM, Data Visualization, Digital Twins, Domotics, Energy Efficiency, GIS, Sensor Data, Smart Building},
	location = {San Sebastian, Spain},
	numpages = {9},
	publisher = {Association for Computing Machinery},
	series = {Web3D '23},
	title = {Towards Concepts for Climate and Energy-Oriented Digital Twins for Buildings},
	url = {https://doi.org/10.1145/3611314.3616066},
	year = {2023},
}

@inproceedings{10.1145/3611314.3616067,
	abstract = {Forests are among the most widespread and diverse ecosystems on Earth, providing essential ecosystem services at local and global scales. However, they are facing major challenges due to climate change, economic pressures and human population growth. Digital twins of forests could help address these challenges by enabling comprehensive forest monitoring and supporting management decisions. In this publication, we describe how digital twins differ from other digital tools in the forest domain and explore concepts and technologies that can serve as the basis for implementing forest digital twins. We outline the underlying data model of the digital twins, which includes trees as the core forest elements, as well as their environment. We explain how a wide range of data collection approaches can be combined for comprehensive data collection and how the data can be integrated into a spatio-temporal forest data space. We describe data processing approaches to enrich raw data with semantic information and address how digital twins can support decision making through modeling and simulation. We explain the role of web-based visualization in interacting with forest digital twins. Overall, our concept lays the foundation for the technical implementation of forest digital twins that integrate, process, analyze and visualize forest data from a variety of sources. The implementation of forest digital twins in practice would enrich our understanding of forest ecosystems and enable targeted management of forests and their ecosystem services.},
	address = {New York, NY, USA},
	articleno = {30},
	author = {D\"{o}llner, J\"{u}rgen and de Amicis, Raffaele and Burmeister, Josafat-Mattias and Richter, Rico},
	booktitle = {Proceedings of the 28th International ACM Conference on 3D Web Technology},
	doi = {10.1145/3611314.3616067},
	isbn = {9798400703249},
	keywords = {Data Analysis, Data Visualization, Digital Twins, Environmental Monitoring, Forests, Remote Sensing, Sensor Data},
	location = {San Sebastian, Spain},
	numpages = {12},
	publisher = {Association for Computing Machinery},
	series = {Web3D '23},
	title = {Forests in the Digital Age: Concepts and Technologies for Designing and Deploying Forest Digital Twins},
	url = {https://doi.org/10.1145/3611314.3616067},
	year = {2023},
}

@article{10.1145/3615359,
	abstract = {Orienteering or itinerary planning algorithms in tourism are used to optimize travel routes by considering user preference and other constraints, such as time budget or traffic conditions. For these algorithms, it is essential to explore the user preference to predict potential points of interest (POIs) or tourist routes. However, nowadays, user preference has been significantly affected by COVID-19, since health concern plays a key tradeoff role. For example, people may try to avoid crowdedness, even if there is a strong desire for social interaction. Thus, the orienteering or itinerary planning algorithms should optimize routes beyond user preference. Therefore, this article proposes a social sensing system that considers the tradeoff between user preference and various factors, such as crowdedness, personality, knowledge of COVID-19, POI features, and desire for socialization. The experiments are conducted on profiling user interests with a properly trained fastText neural network and a set of specialized Na\"{\i}ve Bayesian Classifiers based on the “Yelp!” dataset. Also, we demonstrate how to approach and integrate COVID-related factors via conversational agents. Furthermore, the proposed system is in a modular design and evaluated in a user study; thus, it can be efficiently adapted to different algorithms for COVID-19-aware itinerary planning.},
	address = {New York, NY, USA},
	articleno = {31},
	author = {Pilato, Giovanni and Persia, Fabio and Ge, Mouzhi and Chondrogiannis, Theodoros and D’Auria, Daniela},
	doi = {10.1145/3615359},
	issn = {2158-656X},
	issue_date = {December 2023},
	journal = {ACM Trans. Manage. Inf. Syst.},
	keywords = {COVID-19, orienteering, social sensing, personalization, itinerary planning},
	month = {oct},
	number = {4},
	numpages = {26},
	publisher = {Association for Computing Machinery},
	title = {A Modular Social Sensing System for Personalized Orienteering in the COVID-19 Era},
	url = {https://doi.org/10.1145/3615359},
	volume = {14},
	year = {2023},
}

@inproceedings{10.1145/3615522.3615527,
	abstract = {Recently, controlling air pollution has become increasingly significant due to its impact on our health and daily lives. To prevent and control pollution, it is crucial to trace its source. Many researches have been developed for tracing the source of pollution. However, traditional methods using large-scale simulations need a large number of computation resources and time-consuming. In addition, traditional traceability algorithms do not consider topographic factors, which can cause a certain amount of errors. To resolve above problems, an interactive visual analytics system for pollutant traceability is proposed. In our method, instead of three-dimensional field data, only two-dimensional grid data is enough to track pollution sources in real time. Furthermore, our method can further improve precision through considering topographic factors, which are usually ignored by existing methods. Finally, the possible pollution sources are also identified in our method. This is achieved through analysis of changes in pollutant concentration and the distribution of man-made emission sources. In order to verify the effectiveness of this method, we propose a series of application examples to comprehensively analyze the sources of pollutants.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Hao, Yan and Bi, Chongke and Yang, Lu and Qiu, Xiaobin and Li, Yunlong and Yu, Ce},
	booktitle = {Proceedings of the 16th International Symposium on Visual Information Communication and Interaction},
	doi = {10.1145/3615522.3615527},
	isbn = {9798400707513},
	keywords = {Pollution source, propagation path, visual analytics},
	location = {<conf-loc>, <city>Guangzhou</city>, <country>China</country>, </conf-loc>},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {VINCI '23},
	title = {Visual Analytics of Air Pollutant Propagation Path and Pollution Source},
	url = {https://doi.org/10.1145/3615522.3615527},
	year = {2023},
}

@inproceedings{10.1145/3615833.3628594,
	abstract = {Road attributes play a pivotal role in digital maps, providing critical information for various routing and planning applications that aim to create a safe and efficient traffic environment. While some road attributes are available in existing map data such as OpenStreetMap [3], these sources may not cover all regions, meet high-quality standards, or include specific attributes required for specialized applications using these. To address these challenges, we propose a novel framework that leverages multi-task deep learning to learn road attributes from remote sensing imagery and GPS data. Our approach treats the task as a multi-task learning problem and incorporates convolutional and graph neural networks into an end-to-end learning framework. This enables efficient prediction of multiple road attributes for a set of input roads. To evaluate our system, we collect annotations and develop our model using public map sources. Our results demonstrate promising performance in predicting road type, road median, lane number, road directionality, and width in meters. By exploring different road attributes compared to previous works, our efforts open up new possibilities for novel applications in this domain. Overall, our research contributes to advancing the understanding and prediction of road attributes, enhancing the quality and completeness of digital maps, and enabling the development of innovative solutions for various applications.},
	address = {New York, NY, USA},
	author = {He, Yang and Eftelioglu, Emre and Moustafa, Mohamed and Chowdhury, Amber Roy},
	booktitle = {Proceedings of the 11th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data},
	doi = {10.1145/3615833.3628594},
	isbn = {9798400703454},
	keywords = {graph neural networks, multi-task deep learning, road attributes, satellite imagery},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	pages = {32–41},
	publisher = {Association for Computing Machinery},
	series = {BigSpatial '23},
	title = {A Highly Efficient and Effective Attribute Learning Framework for Road Graph from Aerial Imagery and GPS},
	url = {https://doi.org/10.1145/3615833.3628594},
	year = {2023},
}

@inproceedings{10.1145/3615885.3628008,
	abstract = {The growing prevalence of location-based devices has resulted in a significant abundance of location data from various tracking vendors. Nevertheless, a noticeable deficit exists regarding readily accessible, extensive, and publicly available datasets for research purposes, primarily due to privacy concerns and ownership constraints. There is a pressing need for expansive datasets to advance machine learning techniques in this domain. The absence of such resources currently represents a substantial hindrance to research progress in this field. Data augmentation is emerging as a popular technique to mitigate this issue in several domains. However, applying state-of-the-art techniques as-is proves challenging when dealing with trajectory data due to the intricate spatio-temporal dependencies inherent to such data. In this work, we propose a novel strategy for augmenting trajectory data that applies a geographical perturbation on trajectory points along a trajectory. Such a perturbation results in controlled changes in the raw trajectory and, consequently, causes changes in the trajectory feature space. We test our strategy in two trajectory datasets and show a performance improvement of approximately 20\% when contrasted with the baseline. We believe this strategy will pave the way for a more comprehensive framework for trajectory data augmentation that can be used in fields where few labeled trajectory data are available for training machine learning models.},
	address = {New York, NY, USA},
	author = {J. Haranwala, Yaksh and Spadon, Gabriel and Renso, Chiara and Soares, Amilcar},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Methods for Enriched Mobility Data: Emerging Issues and Ethical Perspectives 2023},
	doi = {10.1145/3615885.3628008},
	isbn = {9798400703478},
	keywords = {machine learning, trajectory data, data augmentation},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {5},
	pages = {25–29},
	publisher = {Association for Computing Machinery},
	series = {EMODE '23},
	title = {A Data Augmentation Algorithm for Trajectory Data},
	url = {https://doi.org/10.1145/3615885.3628008},
	year = {2023},
}

@inproceedings{10.1145/3615886.3627740,
	abstract = {Spatial Representations for Artificial Intelligence (srai) is a Python library for working with geospatial data. The library can download geospatial data, split a given area into micro-regions using multiple algorithms and train an embedding model using various architectures. It includes baseline models as well as more complex methods from published works. Those capabilities make it possible to use srai in a complete pipeline for geospatial task solving. The proposed library is the first step to standardize the geospatial AI domain toolset. It is fully open-source and published under Apache 2.0 licence.},
	address = {New York, NY, USA},
	author = {Gramacki, Piotr and Le\'{s}niara, Kacper and Raczycki, Kamil and Wo\'{z}niak, Szymon and Przymus, Marcin and Szyma\'{n}ski, Piotr},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3615886.3627740},
	isbn = {9798400703485},
	keywords = {geospatial data processing, openstreetmap embeddings, python library, spatial embeddings, spatial representation learning, standardization in geospatial domain, urban data embeddings},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	pages = {43–52},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '23},
	title = {SRAI: Towards Standardization of Geospatial AI},
	url = {https://doi.org/10.1145/3615886.3627740},
	year = {2023},
}

@inproceedings{10.1145/3615886.3627744,
	abstract = {Integrating Geospatial Artificial Intelligence (GeoAI) into our technological landscape has revolutionized our capacity to understand and engage with the world. However, the burgeoning adoption of GeoAI applications has emphasized the priority of data, format, and conveyance standardization and improving geospatial interoperability. This vision paper examines the intricacies of the evolving GeoAI environment, emphasizing the vital role of standardized practices and elevated interoperability. By synthesizing insights from geography, computer science, and data ethics, this contribution envisions a future characterized by the seamless synergy between AI systems and geospatial data, driving impactful decision-making and transformative innovation.},
	address = {New York, NY, USA},
	author = {Arundel, Samantha T. and Li, Wenwen and Campbell, Bryan B.},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3615886.3627744},
	isbn = {9798400703485},
	keywords = {Collaborative Innovation, Data Ethics, GeoAI, Geospatial Interoperability, Standardization},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {2},
	pages = {83–84},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '23},
	title = {Reimagining standardization and geospatial interoperability in today's GeoAI culture},
	url = {https://doi.org/10.1145/3615886.3627744},
	year = {2023},
}

@inproceedings{10.1145/3615886.3627750,
	abstract = {We introduce a new geospatial representation model called GeoVeX to learn global vectors for all geographical locations on Earth land cover. GeoVeX is built on a novel model architecture named Hexagonal Convolutional Autoencoders (HCAE) combined with a Zero-Inflated Poisson (ZIP) reconstruction layer, applied to a grid of Uber's H3 hexagons, each one described by the histogram of OpenStreetMap (OSM) geographical tags occurrences. GeoVeX is novel on two aspects: first, it produces pre-trained task-agnostic geospatial vectors with H3 and OSM that are, for the first time, contextualized on the neighboring hexagons features, by leveraging an hexagonal convolutional autoencoder applied on an H3/OSM grid centered on the location to embed; secondly, it introduces a zero-inflated Poisson autoencoder reconstruction layer, to adapt a standard autoencoder network to train on sparse geographical count data distributed on an hexagonal grid. Experiments demonstrate that GeoVeX embeddings improve upon two state-of-the-art geospatial location representations models, Hex2Vec and Space2Vec, on two different downstream tasks: worldwide listings price prediction in the travel industry, and hyperlocal interpolation of climate data from weather stations. The qualitative analysis of the latent representation structures learnt by GeoVeX showcases the higher quality of the geographical structures learnt by the geographically contextualized embeddings learnt by GeoVeX.},
	address = {New York, NY, USA},
	author = {Donghi, Daniele and Morvan, Anne},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on AI for Geographic Knowledge Discovery},
	doi = {10.1145/3615886.3627750},
	isbn = {9798400703485},
	keywords = {spatial representation learning, hexagonal convolutional autoencoders, embedding, convolutional neural networks, clustering, OpenStreetMap, H3, CNN},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {11},
	pages = {3–13},
	publisher = {Association for Computing Machinery},
	series = {GeoAI '23},
	title = {GeoVeX: Geospatial Vectors with Hexagonal Convolutional Autoencoders},
	url = {https://doi.org/10.1145/3615886.3627750},
	year = {2023},
}

@inproceedings{10.1145/3615887.3627758,
	abstract = {Being in natural places is known to influence well-being. Therefore, increasing attention is being paid to the diverse ways in which our emotions can be influenced by places, and this paper investigates this notion in a protected area -- the Cairngorms National Park in Scotland. We assume that positive sentiment correlates with pleasant (hedonic) experiences, and extract positive emotions and objects associated with them, from textual descriptions written about places in the park.To do so, we first apply sentiment analysis and select descriptions with positive sentiment. Second, we filter the initial dataset to reduce the impact of prolific users and ambiguously positive words. From an initial set of 33790 descriptions we are left with 3031 positive descriptions. Third, we apply transducers to find semantic sequences for patterns detection. Finally, these sequences are annotated and assigned to one of five classes -- activities, aesthetics, features, places and times. Features are by far the most common class, making up more than 50\% of all unique nouns associated with positive emotions.},
	address = {New York, NY, USA},
	author = {Mari\~{n}o, Daniela and Moncla, Ludovic and Purves, Ross S.},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3615887.3627758},
	isbn = {9798400703492},
	keywords = {Landscape perception, natural language processing, sentiment analysis, transducers, values of nature},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	pages = {44–47},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '23},
	title = {Extracting positive descriptions and exploring landscape value using text analysis in the Cairngorms National Park},
	url = {https://doi.org/10.1145/3615887.3627758},
	year = {2023},
}

@inproceedings{10.1145/3615887.3627759,
	abstract = {Maritime history archives contain a large number of toponyms whose meaning is sometimes difficult to grasp. They are lacunary and not quite homogeneous like most of historical datasets. The PORTIC project (https://anr.portic.fr/) intends to take into account the imperfection of these kind of data to answer questions on shipping activities in French ports and French foreign trade at the end of the eighteenth century. This paper focuses on the geographic dimension of data imperfection, and how it was handled to build datasets and tools useful for historian community. The first part shows how we have dealt with these problems to build and publish online a gazetteer of French port places of the 18th century, linked with GeoNames, by using the model LinkedPlaces defined by the LinkedPasts initiative [1]. The second part discusses the requirements for a vectorial background layer made of polygonal territorial entities valid for 1789, and how it has been addressed by the project, and challenges that remains to be overcome.},
	address = {New York, NY, USA},
	author = {Plumejeaud-Perreau, Christine},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3615887.3627759},
	isbn = {9798400703492},
	keywords = {Linked places, States, former administrative division, former borders, gazetteer, uncertainty},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	pages = {48–51},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '23},
	title = {Handling imperfection of spatial knowledge for the study of French maritime places dynamics along the 18th century},
	url = {https://doi.org/10.1145/3615887.3627759},
	year = {2023},
}

@inproceedings{10.1145/3615887.3627761,
	abstract = {Spatial narratives help us to organize experiences and give them meaning. Previous approaches to understanding geographies in textual sources focus on geoparsing to automatically identify place names and allocate them to coordinates. Those are highly quantitative, and are limited to named places with coordinates, and have little concept of time. Narratives of journeys indicate that human experiences of geography are often subjective and more suited to qualitative representation. Geography is not limited to named places but incorporates the vague, imprecise, and ambiguous, e.g "the camp", or "the hills in the distance", and relative locations such as "near to", "on the left", "north of" or "a few hours' journey from". Places are organized worlds of meaning, characterized by experience, emotion, and memory as well as by geography. In this paper, we discuss our approach to gaining more insight from textual data beyond the toponyms and introduce an extensible framework for extracting, analyzing, and visualizing spatial elements that define the 'locale' as well as the 'sense of place' referenced in text using two test corpora --the Corpus of the Lake District Writing and Holocaust Survivors' Testimonies.},
	address = {New York, NY, USA},
	author = {Ezeani, Ignatius and Rayson, Paul and Gregory, Ian and Haris, Erum and Cohn, Anthony and Stell, John and Cole, Tim and Taylor, Joanna and Bodenhamer, David and Devadasan, Neil and Steiner, Erik and Frank, Zephyr and Olson, Jackie},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL International Workshop on Geospatial Humanities},
	doi = {10.1145/3615887.3627761},
	isbn = {9798400703492},
	keywords = {corpus annotation, datasets, geographical feature nouns, locale, location, named entity recognition, ontology, place names, qualitative spatial representation, sense of place, spatial narratives, spatio-textual regions, toponyms},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	pages = {1–10},
	publisher = {Association for Computing Machinery},
	series = {GeoHumanities '23},
	title = {Towards an Extensible Framework for Understanding Spatial Narratives},
	url = {https://doi.org/10.1145/3615887.3627761},
	year = {2023},
}

@inproceedings{10.1145/3615888.3627812,
	abstract = {As people increasingly adopt a more flexible and mobile way of living, map applications and navigation services have gained increasingly in importance. In order for these services to be useful to their users, their providers need to ensure and maintain a high quality in terms of accuracy and reliability. Keeping track of the service quality is essential, especially in a dynamic domain like this, where the underlying road network data might change from one day to the next due for instance to temporal road closures or turn restrictions. Although the changes might be small and local, they can still have a big impact on the routing quality of direction services. The focus of this study lies on providing the means to avoid such high-impact scenarios in a cost-efficient manner. In particular, we focus on the legality aspect of routes that reflects the degree of prohibited by law maneuvers within a suggested route. First, we define a set of appropriate sample-based metrics to help us track a route's legality. Then, we introduce an automated pipeline based on OSM data and GPS traces that is able to support and reduce the load of our human judges to identify illegal maneuvers in a sample of candidate routes. Finally, we evaluate our pipeline using a random sample of 1,306 US routes while a local map data provider as well as human judgement via visual inspection serve as our ground truth. Our results show that our method is able to sufficiently cover most of the route sections in our candidate route samples and identify 90\% of all illegal maneuvers while reducing the load of manual human judgement by 86\%.},
	address = {New York, NY, USA},
	author = {Karatzoglou, Antonios and Bekic, Tijana and Agrawal, Vashutosh and Khanna, Mohit and Matejevic, Aleksandar and Kakkar, Varun and Evans, Michael R. and Samardzija, Aleksandar and Whitbeck, Jacob and Yankov, Dragomir and Perin, Nikola and Todic, Nikola and Predovic, Goran},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Spatial Big Data and AI for Industrial Applications},
	doi = {10.1145/3615888.3627812},
	isbn = {9798400703508},
	keywords = {Turn restrictions, Routing, One-Way restrictions, OSM, Navigability, Map services, Map inference, GPS traces, Direction services},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	pages = {22–31},
	publisher = {Association for Computing Machinery},
	series = {GeoIndustry '23},
	title = {Exploring The Use of OpenStreetMap Data (OSM) and GPS Traces for Validating Driving Routes and Identifying Prohibited Maneuvers in Direction Services},
	url = {https://doi.org/10.1145/3615888.3627812},
	year = {2023},
}

@inproceedings{10.1145/3615889.3628511,
	abstract = {The search for a missing person in a recent criminal case raised the question whether it is possible to reconstruct car-driven routes effectively by using data taken from a digital tachograph. Tachographs are mandatory in many vehicles across multiple countries. However, the recorded data comprises only speed measurements, without recorded GNSS-based tracks. Additionally, EU regulations limit the amount of GNSS-based locations recorded by smart tachographs. This article describes a semi-automated approach for reconstructing car-driven routes based on speed measurements. The method employs a linear reference system for locating speed measurements along route candidates for visual evaluation. This technique was successfully applied to find the corpse of the missing person. Therefore, revealing routes is an option that jeopardises drivers' privacy, increasing the risk of misuse.},
	address = {New York, NY, USA},
	author = {Werner, Tobias and Ahlers, J\"{o}rn and Brinkhoff, Thomas},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geo-Privacy and Data Utility for Smart Societies},
	doi = {10.1145/3615889.3628511},
	isbn = {9798400703515},
	keywords = {digital forensics, linear referencing system, geoprivacy, route reconstruction, tachograph},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {6},
	pages = {14–19},
	publisher = {Association for Computing Machinery},
	series = {GeoPrivacy '23},
	title = {About privacy on smart tachographs: Reconstructing car-driven routes based on speed measurements},
	url = {https://doi.org/10.1145/3615889.3628511},
	year = {2023},
}

@inproceedings{10.1145/3615890.3628535,
	abstract = {Geographic information systems (GIS) provide users with a means to efficiently search over spatial data given certain key pieces of information, like the coordinates or exact name of a location of interest. Current GIS capabilities do not enable users to search for locations using imperfect or incomplete information easily. In these cases, GIS tools help narrow down a region of interest, but users must conduct a manual last-mile search to find the exact location of interest within that region. This typically involves the user visually inspecting many remote sensing or street-view images to identify distinct landmarks or terrain features that match the partial information provided. This step of the search process is a bottleneck. Taking inspiration from the way humans recall and search for information, we present the Geospatially Enhanced Search with Terrain Augmented Location Targeting (GESTALT), an end-to-end pipeline for extracting geospatial data, transforming it into coherent spatial relations, storing those relations, and searching over them. We contribute a new Swan Valley Wineries dataset and a proof of concept architecture that includes multiple methods for querying spatial configurations of objects, handling uncertainty in the information known about a location or object, and accounting for the fuzzy boundaries between locations.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {O'Sullivan, Kent and Schneider, Nicole R. and Rasheed, Aleeza and Samet, Hanan},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
	doi = {10.1145/3615890.3628535},
	isbn = {9798400703522},
	keywords = {last-mile search, geospatial data, pictorial query, spatial pattern matching},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {GeoSearch '23},
	title = {GESTALT: Geospatially Enhanced Search with Terrain Augmented Location Targeting},
	url = {https://doi.org/10.1145/3615890.3628535},
	year = {2023},
}

@inproceedings{10.1145/3615890.3628537,
	abstract = {The Spatial Pattern Matching paradigm offers a promising direction for searching with incomplete or imperfect information, but it is badly constrained by dependence on graph-based representations and computationally intensive search algorithms like subgraph matching and constraint satisfaction. To address these limitations, we present COMPASS, a suite of data structures and algorithms that enable pattern-based search by encoding the directional relationships between objects in abstracted matrix representations rather than graph structures. We provide a series of recursive search algorithms that leverage our matrix representations to enable spatial search queries with directional constraints, which are typically too dense for previous graph-based approaches. Our search methods find matches even when the query pattern is not aligned to the global coordinate system, resulting in perfect recall in our evaluation. Computationally, our search methods scale with the number of query objects times the number of database objects squared in the worst case, which is significantly better than previous methods. Our empirical measurements show that the performance is typically even better, approaching logarithmic in the number of query terms.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {O'Sullivan, Kent and Schneider, Nicole R. and Samet, Hanan},
	booktitle = {Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Searching and Mining Large Collections of Geospatial Data},
	doi = {10.1145/3615890.3628537},
	isbn = {9798400703522},
	keywords = {searching geospatial objects, pictorial querying, spatial pattern matching},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {8},
	publisher = {Association for Computing Machinery},
	series = {GeoSearch '23},
	title = {COMPASS: Cardinal Orientation Manipulation and Pattern-Aware Spatial Search},
	url = {https://doi.org/10.1145/3615890.3628537},
	year = {2023},
}

@inproceedings{10.1145/3615891.3628016,
	abstract = {After a major natural disaster strikes a region, emergency response often lacks information about the post-disaster state of the road network. Conflicting information about the region from multiple sources creates confusion. Thus, it is imperative to obtain an updated map, which provides accurate information about currently navigable paths and identifies hazardous locations. However, due to possible damage to communication and computation infrastructure, existing centralized geospatial services may provide outdated information. One possible solution to this problem is using Internet-of-Things based devices that are deployable without relying on a centralized service, which will be able to both acquire and disseminate local data. In this paper, we introduce PhobosBC, a decentralized system which is able to provide a reliable mapping service using volunteer work, while relying on a blockchain backend. This solution utilizes the availability of modern smartphones with GPS receivers and processing capabilities to collect sequences of GPS locations and combine them into trajectories. These trajectory data are submitted as entries into a blockchain after processing them through a smart contract. PhobosBC relies on the inherent robustness and distributed nature of blockchains to make collating and assembling a map from these paths more accurate and less susceptible to disruption. Compared to a centralized system, PhobosBC is more resilient and is able to respond to individual agents going offline while still retaining consensus. As a performance incentive, we rank volunteers by the amount of correct data submitted and publicly recognize top performers. We develop an agent-based model to simulate PhobosBC under a hypothetical disaster scenario, and we compare our approach with a crowdsourcing system which deploys a simulated central control mechanism. We share the problems faced during creation of the agent-based model and the lessons we learned. Our results show that PhobosBC is able to recreate the ground truth faster while being more fault tolerant.},
	address = {New York, NY, USA},
	author = {Sarbajna, Raunak and Eick, Christoph F. and Laszka, Aron},
	booktitle = {Proceedings of the 6th ACM SIGSPATIAL International Workshop on GeoSpatial Simulation},
	doi = {10.1145/3615891.3628016},
	isbn = {9798400703539},
	keywords = {Agent Based Models for Spatial Simulation, Blockchain, Internet of Things, Spatial Analysis based on Simulation, crowdsourcing, disaster map building, disaster response},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {9},
	pages = {10–18},
	publisher = {Association for Computing Machinery},
	series = {GeoSim '23},
	title = {PhobosBC: A Blockchain-based Crowdsourced Post-disaster Mapping System and its Agent-based Simulation},
	url = {https://doi.org/10.1145/3615891.3628016},
	year = {2023},
}

@inproceedings{10.1145/3615892.3628476,
	abstract = {This study aims to discuss the spatial heterogeneity of accessibility of urban facilities and analyze the differences in socioeconomic characteristics in Thailand using the following indicators: walking and car accessibility, and intra-spatial inequality. The results of this study were that a certain percentage of local residents had difficulty accessing urban facilities, especially public transport hubs, even when using automobiles. Moreover, in some Mueang units, access to public transport hubs is generally poor due to an aging population. Although the elderly and low-income groups need to fulfill their needs within a walking distance, this study demonstrates this is difficult for them. Focusing on spatial inequalities in case of grocery stores, not only Mueang units in rural areas, but also ones in Bangkok metropolitan region experience spatial inequality. Despite technical and data limitations, these findings are essential for mitigating spatial inequality.},
	address = {New York, NY, USA},
	author = {Baba, Hiroki},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geocomputational Analysis of Socio-Economic Data},
	doi = {10.1145/3615892.3628476},
	isbn = {9798400703546},
	keywords = {Moran's I, point of interest, association of southeast asian nations, urban amenities, accessibility},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	pages = {1–4},
	publisher = {Association for Computing Machinery},
	series = {GeoSocial '23},
	title = {Measurement of spatial inequality using micro-spatial data in Thailand},
	url = {https://doi.org/10.1145/3615892.3628476},
	year = {2023},
}

@inproceedings{10.1145/3615892.3628477,
	abstract = { 'Volunteered Geographic Information' (VGI) has particular importance - in part - for its democratisation of geographic information. However, some recent research has suggested that despite being publicly open, several successful VGI platforms have under-representation of particular socio-demographic groups, which may lead to biases in the types of information contributed. This paper examines the relationship between demographic characteristics and user contributions to OpenStreetMap (OSM), one of the most successful examples of a project reliant on VGI. It demonstrates statistically significant differences in the information provided by users of different genders, ages, and education-levels. Differences between the demographic characteristics of OSM contributors and the underlying population are therefore likely to be reflected in the VGI contained in OSM.},
	address = {New York, NY, USA},
	author = {Sutton, Dominick and Solomon, Guy and Yuan, Xinyi and Polat Kayali, Merve and Gardner, Zoe and Basiri, Ana},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geocomputational Analysis of Socio-Economic Data},
	doi = {10.1145/3615892.3628477},
	isbn = {9798400703546},
	keywords = {OpenStreetMap, gini-simpson index, volunteered geographic information, crowdsourcing, contributor bias},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {7},
	pages = {5–11},
	publisher = {Association for Computing Machinery},
	series = {GeoSocial '23},
	title = {Assessing the relationship between socio-demographic characteristics and OpenStreetMap contributor behaviours},
	url = {https://doi.org/10.1145/3615892.3628477},
	year = {2023},
}

@inproceedings{10.1145/3615892.3628478,
	abstract = {Poverty alleviation is an important goal set by the UN which aims to improve the lives of a large number of people around the world, especially in developing countries. One of the challenges in achieving this goal is being able to measure wealth level frequently, inexpensively, and with high spatial granularity. There are surveys that measure wealth level, however these are conducted only every 4 to 5 years, require high costs in terms of logistics and manpower, or are only accurate at the provincial level. One alternative is to use deep learning models to estimate wealth levels using satellite images. However, this is cost-prohibitive due to the requirement of significant compute resources to train these models. We build upon an existing approach of measuring wealth level in the Philippines using machine learning and tabular data and we obtained an R2 of 0.68, MAPE of 22.71\%, and MAE of 7.8, which is better by 0.05 in R2 than the results of a previous study utilizing deep learning and satellite images. For urban areas, our model can achieve an even better MAPE of 13.9\%. The performance improvement can be attributed to the addition of new data (e.g. environmental, demographics, remote sensing indices) which provide additional context to the model. Additionally, we discover that the use of automatically collected data such as remote sensing data and environmental data provides comparable results (R2 of 0.66) and thus can be used as an alternative data source when manually created points of interest data are unavailable. Furthermore, we observe that environmental data and points of interest data contribute the most to the prediction of wealth level for urban and rural clusters. For future research directions, we recommend to explore the differences between urban and rural clusters, and to use deep learning as a supplement, especially for areas with limited data sources.},
	address = {New York, NY, USA},
	author = {Reyes, Dustin and Antonio, Roger Jr. and Orden, Ardie and Heinrich, Adrienne and Phoa, Randy and Bilal, Sara and Bonganay, Gerando and Singson, Maria},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geocomputational Analysis of Socio-Economic Data},
	doi = {10.1145/3615892.3628478},
	isbn = {9798400703546},
	keywords = {OpenStreetMap, wealth index, demographic and health survey (DHS), remote sensing},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {8},
	pages = {12–19},
	publisher = {Association for Computing Machinery},
	series = {GeoSocial '23},
	title = {Wealth Index Estimation using Machine Learning with Environmental, Demographics, Remote Sensing, and Points of Interest Data},
	url = {https://doi.org/10.1145/3615892.3628478},
	year = {2023},
}

@inproceedings{10.1145/3615892.3628482,
	abstract = {This research examined the relationship between greenery and the risk of dementia. When analyzing the association with the risk of dementia, we considered both greenery and open greenery(greenery with a clear view of the sky) within 500 meters of patients' homes using street view imagery. Also, we compared their associations with the relationship between the risk and greenery index of traditional satellite images. In the overall study area, a higher level of greenery was associated with a lower risk of dementia. However, high levels of both open greenery and EVI from satellite images were associated with a higher risk of dementia. The association patterns varied depending on geographic locations. In the rural area (Wonju-si, Gangwon-do), the correlation between overall greenery and the risk of dementia appeared similar to what was observed in the entire study area. However, in Seoul the relationship between greenery from street imagery and the risk of dementia was not statistically significant. Only in Eunpyoung-gu, Seoul, higher EVI was related to a lower risk of dementia, which differs from the patterns observed in the overall study area and rural areas. In rural areas, the relationship between overall greenery and the risk of dementia appeared stronger than urban areas. The open greenery indices derived from street imagery exhibited a stronger correlation with the risk of dementia in rural areas and the entire study area compared to those from satellite images.},
	address = {New York, NY, USA},
	author = {Heo, Joon and Joo, Yoohyung and Park, Sangyoon and Jung, Jaeyoung and Hong, Jiwan and Ko, Juyeon and Cho, Jaelim and Kim, Changsoo},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Geocomputational Analysis of Socio-Economic Data},
	doi = {10.1145/3615892.3628482},
	isbn = {9798400703546},
	keywords = {risk of dementia, cognitive health, street imagery, greenery},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {7},
	pages = {28–34},
	publisher = {Association for Computing Machinery},
	series = {GeoSocial '23},
	title = {Exploring the Relationship between Greenery in Patients’ Living Spaces and Cognitive Health: A Study of Urban versus Rural Areas},
	url = {https://doi.org/10.1145/3615892.3628482},
	year = {2023},
}

@inproceedings{10.1145/3615894.3628502,
	abstract = {Accurately predicting human mobility is crucial for understanding human dynamics, addressing urban challenges, and improving urban planning. However, it still faces challenges due to the dynamic and complex nature of human mobility. Traditional methods rely on manual feature design and statistical analysis, limiting the accuracy and generalizability of predictions. In recent years, the development of deep learning techniques has provided new solutions. In this study, we propose a Multi-perspective Spatiotemporal Context-aware Neural Network for accurately predicting the next location in human mobility. Our model considers individual and collective behaviors, as well as explicit representation of spatiotemporal characteristics. Experimental results demonstrate competitive performance on the large-scale human mobility dataset.},
	address = {New York, NY, USA},
	author = {Wang, Chenglong and Deng, Zhicheng},
	booktitle = {Proceedings of the 1st International Workshop on the Human Mobility Prediction Challenge},
	doi = {10.1145/3615894.3628502},
	isbn = {9798400703560},
	keywords = {human mobility prediction, spatiotemporally explicit artificial intelligence, individual-collective modeling},
	location = {Hamburg, Germany},
	numpages = {5},
	pages = {32–36},
	publisher = {Association for Computing Machinery},
	series = {HuMob-Challenge '23},
	title = {Multi-perspective Spatiotemporal Context-aware Neural Networks for Human Mobility Prediction},
	url = {https://doi.org/10.1145/3615894.3628502},
	year = {2023},
}

@inproceedings{10.1145/3615894.3628503,
	abstract = {Predicting human mobility is essential in various domains and applications (e.g., urban planning) and is one of the fundamental research topics in GIS. In recent years, there has been a lot of research on mobility (and trajectory) prediction using deep learning models. However, how to utilize irregular temporal information (i.e., the sampling time of coordinates is not regular) has not been studied enough. This paper describes the framework used in the Human Mobility Prediction Challenge (HuMob Challenge) 2023, which received the top-10 performance. Our framework consists of preprocessing cell-level trajectory data, then trains sequence encoder-decoder model using the processed spatio-temporal data, and predicts cell-level trajectories using the trained model. We also propose a model that can learn and infer even if there is a time gap by using embedded vectors that divide the time and day information. Finally, we conducted extensive experiments using 100K individual trajectory data of 90 days in an urban scale domain provided by HuMob Challenge 2023, and excellent results have been achieved.},
	address = {New York, NY, USA},
	author = {Kim, Taehoon and Kim, Kyoung-Sook and Matono, Akiyoshi},
	booktitle = {Proceedings of the 1st International Workshop on the Human Mobility Prediction Challenge},
	doi = {10.1145/3615894.3628503},
	isbn = {9798400703560},
	keywords = {cell-level trajectory, trajectory prediction, Seq2Seq, encoder-decoder network},
	location = {Hamburg, Germany},
	numpages = {4},
	pages = {37–40},
	publisher = {Association for Computing Machinery},
	series = {HuMob-Challenge '23},
	title = {Cell-Level Trajectory Prediction Using Time-embedded Encoder-Decoder Network},
	url = {https://doi.org/10.1145/3615894.3628503},
	year = {2023},
}

@inproceedings{10.1145/3615896.3628340,
	abstract = {The proliferation of location-based services and social networks have given rise to geosocial networks, which model not only the social interactions between users but also their spatial activities. Examples include traditional social networks extended with geo-annotated posts such as Twitter and Facebook, and networks such as Foursquare and Yelp that directly offer geosocial services. Despite the ubiquity of such networks in everyday life and the strong interest by the research community, a limited number of datasets are in fact publicly available. In view of this, we investigate the generation of realistic geosocial networks which find application in benchmarking and testing of analysis tasks, "what-if" scenarios and simulations. The contributions of our work are twofold. We first identify three types of synthetic geosocial networks which mimic the characteristics of real ones and second, we develop a prototype which combines graph and spatial generators, to construct such networks.},
	address = {New York, NY, USA},
	author = {Al Rhman Sarsour, Abed and Bouros, Panagiotis and Chondrogiannis, Theodoros},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3615896.3628340},
	isbn = {9798400703584},
	keywords = {generator, spatial data, graph, geosocial network},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	pages = {25–28},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '23},
	title = {Towards Generating Realistic Geosocial Networks},
	url = {https://doi.org/10.1145/3615896.3628340},
	year = {2023},
}

@inproceedings{10.1145/3615896.3628342,
	abstract = {In the data-driven era, recommendations have become indispensable across various systems. Graphs, as versatile data structures, shine at abstracting complex systems. Many real-world scenarios effortlessly translate into graphs, representing individuals and their relationships as nodes and edges. Link prediction, a cornerstone of recommendations, excels in forecasting future network connections based on current structures. Its applications span diverse domains, including social networks, biological networks, and network security. Previous studies have leveraged classification algorithms like logistic regression and random forest, often complemented by node embedding techniques, yielding impressive results in addressing the challenge of link prediction. Today's dynamic networks continually reshape connections, introducing new links and nodes while removing others. Furthermore, the inclusion of location information associated with nodes provides a new opportunity. Adapting models to this dynamism necessitates capturing spatial and temporal dependencies for sustained effectiveness. In this paper, we undertake a comprehensive evaluation of various algorithms for link prediction. Subsequently, we further enriched the continuous-time dynamic graph networks by incorporating essential location information. This strategic enhancement results in a remarkable performance improvement, highlighting the crucial role of location-based temporal data in improving recommendations. It emphasizes the untapped potential of location and temporal information in refining user recommendations within interconnected networks.},
	address = {New York, NY, USA},
	author = {Zhang, Ziyi and Li, Diya and Song, Zhenlei and Duffield, Nick and Zhang, Zhe},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3615896.3628342},
	isbn = {9798400703584},
	keywords = {geographic information system, dynamic graph, recommendation system, link prediction},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	pages = {58–61},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '23},
	title = {Location-Aware Social Network Recommendation via Temporal Graph Networks},
	url = {https://doi.org/10.1145/3615896.3628342},
	year = {2023},
}

@inproceedings{10.1145/3615896.3628344,
	abstract = {Trip planning services are employed extensively by users to compute paths between locations for many different use cases, including commuting to work, transportation of goods, and itinerary planning for tourists. In many scenarios, such as planning for a hiking trip, running training, or mountain cycling, it is desirable to provide users with personalized trips according to their preferences. Existing route planning systems for mountain activities recommend user-posted trips, along with ratings w.r.t. the route's difficulty, condition, or enjoyment it provides. However, users often want to define a specific trip by choosing the segments/trails they want to follow. Existing systems do not provide a rating for such trips, thus suffering from the cold-start problem. Also, the efforts to automatically infer such a rating have been limited. In this paper, we study the problem of inferring ratings for custom trips. We propose a machine-learning framework that encodes various rated trip features and employs random forest classifiers to infer ratings. We conduct feature engineering to encode information regarding a) trip location, b) trip elevation profile, c) closeness to points of interest, and d) closeness to locations of geotagged photos. Finally, we present the results of an ablation study on two real-world data sets and five different ratings. We evaluate the efficiency of our proposed approach and the effect each feature has on the rating inference accuracy.},
	address = {New York, NY, USA},
	author = {Chondrogiannis, Theodoros and Ge, Mouzhi},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3615896.3628344},
	isbn = {9798400703584},
	keywords = {random forests, mapping services, trip recommendation},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {8},
	pages = {50–57},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '23},
	title = {Rating Inference for Custom Trips from Enriched GPS Traces using Random Forests},
	url = {https://doi.org/10.1145/3615896.3628344},
	year = {2023},
}

@inproceedings{10.1145/3615896.3628345,
	abstract = {Generating synthetic social networks is an important task for many problems that study humans, their behavior, and their interactions. Geosocial networks enrich social networks with location information. Commonly used models to generate synthetic social networks include the classical Erd\H{o}s-R\'{e}nyi, Barab\'{a}si-Albert, and Watts-Strogatz models. However, these classic social network models do not consider the location of individuals. Real-world geosocial networks do exhibit a strong spatial autocorrelation, thus having a higher likelihood of a social connection between agents that are spatially close. As such, recent variants of the three classical models have been proposed to consider location information. Yet, these existing solutions assume that individuals are located on a uniform lattice and exhibit certain limitations when applied to real-world data that exhibits clusters. In this work, we discuss these limitations and propose new approaches to extend the three classic social network generation models to geosocial networks. Our experiments show that our generated synthetic geosocial networks address the shortcomings of the state-of-the-art models and generate realistic geosocial networks that exhibit high similarity to real-world geosocial networks.},
	address = {New York, NY, USA},
	author = {Gallagher, Ketevan and Anderson, Taylor and Crooks, Andrew and Z\"{u}fle, Andreas},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3615896.3628345},
	isbn = {9798400703584},
	keywords = {barabasi-albert, watts-strogatz, erdos-renyi, synthetic social networks, network generation, geosocial networks},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	pages = {15–24},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '23},
	title = {Synthetic Geosocial Network Generation},
	url = {https://doi.org/10.1145/3615896.3628345},
	year = {2023},
}

@inproceedings{10.1145/3615896.3628346,
	abstract = {Tourism has evolved from a simple pastime into a pivotal industry with significant economic implications. Modern travelers frequently venture into unfamiliar locales and seek guidance in crafting enjoyable itineraries. Creating a satisfying trip involves comprehending local attractions, environmental considerations, and personal preferences. In contrast to previous research that primarily focused on fixed Points of Interest (PoIs), this study introduces a diverse range of attractions including extracted events from social media, and proposes routes that encompass these variations. A pioneering aspect of this research is the utilization of social media events for travel route planning. It involves extracting and mapping those events, whenever possible, to existing spatial attractions. The results of our experimental evaluation demonstrate the effectiveness of this approach in efficiently recommending personalized travel itineraries that are driven by events.},
	address = {New York, NY, USA},
	author = {Orabi, Mariam and Afyouni, Imad and Al Aghbari, Zaher},
	booktitle = {Proceedings of the 7th ACM SIGSPATIAL Workshop on Location-Based Recommendations, Geosocial Networks and Geoadvertising},
	doi = {10.1145/3615896.3628346},
	isbn = {9798400703584},
	keywords = {geo-spatial data, social media events, tourist trip design, travel sequences, location-based recommendations, location-based services},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {6},
	pages = {44–49},
	publisher = {Association for Computing Machinery},
	series = {LocalRec '23},
	title = {Harnessing Extracted Social Media Events for Personalized Travel Sequences},
	url = {https://doi.org/10.1145/3615896.3628346},
	year = {2023},
}

@inproceedings{10.1145/3615898.3628258,
	abstract = {This paper introduces the EPIPOL disease simulation model, constructed upon the Patterns-of-life simulation, designed to produce human trajectory data. Over recent years, a surge in disease simulation models has been observed, each distinctive in its design and functionality. The primary objective of these models is to predict infection patterns for specified diseases in hypothetical settings, based on all available disease characteristics. A challenge that EPIPOL addresses is the typical rigidity of these models, which are often tailored for a specific disease. Thus they demand profound software expertise for modification. In our demonstration, participants will experience the user-friendliness and clarity of EPIPOL by: (1) selecting disease presets to initialize variables; (2) dynamically adjusting these variables during the simulation for enhanced precision; and (3) visualizing disease propagation in any globally mapped location.},
	address = {New York, NY, USA},
	author = {Kohn, Will and Amiri, Hossein and Z\"{u}fle, Andreas},
	booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on Spatial Computing for Epidemiology},
	doi = {10.1145/3615898.3628258},
	isbn = {9798400703607},
	keywords = {Customization, Epidemic prediction, Geospatial simulation, Human behavior, Infectious disease simulation},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {4},
	pages = {13–16},
	publisher = {Association for Computing Machinery},
	series = {SpatialEpi '23},
	title = {EPIPOL: An Epidemiological Patterns of Life Simulation (Demonstration Paper)},
	url = {https://doi.org/10.1145/3615898.3628258},
	year = {2023},
}

@inproceedings{10.1145/3615899.3627933,
	abstract = {MaaS (stands for Mobility as a Service) is a concept that aims to integrate different transportation services into a unified and seamless mobility solution. It encourages a shift away from personally owned modes of transportation, such as private cars, towards a more comprehensive approach that combines public transportation, such as ride-sharing, bike-sharing, carpooling, and other modes of transport into a single and user-centric service. One of the services offered within MaaS is a ride-sharing, which provides several advantages such as cost savings, reduced traffic congestion, or environmental benefits. However, to make a ride-sharing efficient, a sophisticated route finding (i.e., planning) is required to avoid redundantly long routes when picking up or dropping off passengers.In this paper, we consider an efficient ride-sharing route finding for large-vehicles such as buses that starts at the determined location and returns after visiting all the locations, when a set of the locations of the passengers is given. Moreover, we allow passengers to move within a short distance to find more efficient traversal plan. Note that this problem can be reduced to the traveling salesperson problem (with some constraints) which is a well-known NP-hard problem. Hence, we employ a 1.5-approximation algorithm to find an efficient route within a reasonable computational time, moreover, use the Viterbi algorithm to improve the efficiency of the route when allowing each passenger to move.},
	address = {New York, NY, USA},
	author = {Kim, Yonghwan and Amano, Masato and Yamamoto, Daisuke},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Sustainable Mobility},
	doi = {10.1145/3615899.3627933},
	isbn = {9798400703614},
	keywords = {TSP, approximation algorithm, route finding, ride-sharing},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {11},
	pages = {59–69},
	publisher = {Association for Computing Machinery},
	series = {SuMob '23},
	title = {A 1.5-Approximation Route Finding for a Ride-sharing considering Movement of Passengers},
	url = {https://doi.org/10.1145/3615899.3627933},
	year = {2023},
}

@inproceedings{10.1145/3615900.3628790,
	abstract = {Footprints of buildings can provide cues about architectural styles and functional types. Learning such latent thematic information from geometry is relevant for various applications, such as urban planning and map generalization. A common task in this context is to cluster a set of building footprints based on their shape characteristics. In this paper, we present a novel method for this task which is based on concepts of graph similarity. We use a graph similarity measure that combines ideas from the Weisfeiler-Lehman-method and optimal transport theory. For the final clustering, we use spectral clustering. To obtain a meaningful graph representation, we propose two algorithms that transform the medial axis of a building footprint into a skeleton graph. We tested our algorithm on a data set from Boston and performed a small user study, where we also compared the results to an existing feature-based clustering method. The study gives a first hint that the results of our algorithm are in line with human similarity perception. Future work is needed to improve the stability of the proposed similarity measure and to confirm our findings with more extensive experiments.},
	address = {New York, NY, USA},
	author = {Duong, Sophie and Rottmann, Peter and Haunert, Jan-Henrik and Mutzel, Petra},
	booktitle = {Proceedings of the 1st ACM SIGSPATIAL International Workshop on Advances in Urban-AI},
	doi = {10.1145/3615900.3628790},
	isbn = {9798400703621},
	keywords = {spectral clustering, graph similarity, graph kernels, medial axis, building footprints},
	location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
	numpages = {10},
	pages = {22–31},
	publisher = {Association for Computing Machinery},
	series = {UrbanAI '23},
	title = {Clustering Building Footprint Polygons Based on Graph Similarity Measures},
	url = {https://doi.org/10.1145/3615900.3628790},
	year = {2023},
}

@article{10.1145/3625819,
	abstract = {A vast amount of location information exists in unstructured texts, such as social media posts, news stories, scientific articles, web pages, travel blogs, and historical archives. Geoparsing refers to recognizing location references from texts and identifying their geospatial representations. While geoparsing can benefit many domains, a summary of its specific applications is still missing. Further, there is a lack of a comprehensive review and comparison of existing approaches for location reference recognition, which is the first and core step of geoparsing. To fill these research gaps, this review first summarizes seven typical application domains of geoparsing: geographic information retrieval, disaster management, disease surveillance, traffic management, spatial humanities, tourism management, and crime management. We then review existing approaches for location reference recognition by categorizing these approaches into four groups based on their underlying functional principle: rule-based, gazetteer matching–based, statistical learning-–based, and hybrid approaches. Next, we thoroughly evaluate the correctness and computational efficiency of the 27&nbsp;most widely used approaches for location reference recognition based on 26 public datasets with different types of texts (e.g., social media posts and news stories) containing 39,736 location references worldwide. Results from this thorough evaluation can help inform future methodological developments and can help guide the selection of proper approaches based on application needs.},
	address = {New York, NY, USA},
	articleno = {112},
	author = {Hu, Xuke and Zhou, Zhiyong and Li, Hao and Hu, Yingjie and Gu, Fuqiang and Kersten, Jens and Fan, Hongchao and Klan, Friederike},
	doi = {10.1145/3625819},
	issn = {0360-0300},
	issue_date = {May 2024},
	journal = {ACM Comput. Surv.},
	keywords = {comparative review, machine learning, location reference recognition, Geoparsing},
	month = {nov},
	number = {5},
	numpages = {37},
	publisher = {Association for Computing Machinery},
	title = {Location Reference Recognition from Texts: A Survey and Comparison},
	url = {https://doi.org/10.1145/3625819},
	volume = {56},
	year = {2023},
}

@inproceedings{10.1145/3626641.3626941,
	abstract = {Smartphones and their mobile applications have become an inseparable part of modern daily life. One of them is a public transportation app that helps people commute with public transportation. Many public transportation service providers have mobile applications as a companion for their customers to improve their quality of service. The application provides useful information and/or important features regarding their service, such as timetables or route finding for public transit. Such features run a slow algorithm to provide real-time or time-based information to the users. Dijkstra is one example of an algorithm that allows users to find the shortest path from one location to another in a public transit network. However, Dijkstra is categorized as a greedy algorithm because it looks at all possible solutions. It takes longer for Dijkstra to find the optimum solution when the number of points in a network graph is high. This study attempted to improve the overall route-finding processing time and reduce the memory footprint while keeping the visual cue of a route path reasonably consistent by utilizing Douglas-Peucker line simplification algorithm to simplify the path and reduce the number of points that Dijkstra algorithm processes. The result suggested that Douglas-Peucker simplification process took 0.5\% additional memory footprint and put 5.13\% additional processing time into the overall route-finding processing time. However, it reduces the overall memory footprint and processing time by 72.56\% and 80.83\%, respectively. The result conveyed a significant improvement in the practical implementation of Douglas-Peucker algorithm to reduce the input for the Dijkstra algorithm in finding the optimal route, thereby improving the overall route-finding performance.},
	address = {New York, NY, USA},
	author = {Pinandito, Aryo and Kharisma, Agi Putra and Akbar, Muhammad Aminul},
	booktitle = {Proceedings of the 8th International Conference on Sustainable Information Engineering and Technology},
	doi = {10.1145/3626641.3626941},
	isbn = {9798400708503},
	keywords = {transportation, simplification, route, path, Douglas-Peucker, Dijkstra},
	location = {<conf-loc>, <city>Badung, Bali</city>, <country>Indonesia</country>, </conf-loc>},
	numpages = {8},
	pages = {401–408},
	publisher = {Association for Computing Machinery},
	series = {SIET '23},
	title = {Improving Route-Finding Performance of Dijkstra Algorithm and Maintaining Path Visual Cue Using Douglas-Peucker Algorithm},
	url = {https://doi.org/10.1145/3626641.3626941},
	year = {2023},
}

@article{10.1145/3627160,
	abstract = {The rapid advancements in sensing techniques, networking, and artificial intelligence (AI) algorithms in recent years have brought autonomous driving vehicles closer to common use in vehicular transportation. One of the fundamental components to enable autonomous driving functionalities are High-Definition (HD) maps – a type of map that carries highly accurate and much richer information than conventional maps. The creation and use of HD maps rely on advances in multiple disciplines, such as computer vision/object perception, geographic information systems, sensing, simultaneous localization and mapping, machine learning, etc. To date, several survey papers have been published describing the literature related to HD maps and their use in specialized contexts. In this survey, we aim to provide (1) a comprehensive overview of the issues and solutions related to HD maps and their use without attachment to a particular context; (2) a detailed coverage of the important domain knowledge of HD map furniture, from acquisition techniques and extraction approaches, through HD map–related datasets, to furniture quality assessment metrics, for the purpose of providing a comprehensive understanding of the entire workflow of HD map furniture generation, as well as its use.},
	address = {New York, NY, USA},
	articleno = {5},
	author = {Zang, Andi and Xu, Runsheng and Trajcevski, Goce and Zhou, Fan},
	doi = {10.1145/3627160},
	issn = {2374-0353},
	issue_date = {March 2024},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {high-definition maps, heterogeneous datasets, autonomous driving},
	month = {jan},
	number = {1},
	numpages = {37},
	publisher = {Association for Computing Machinery},
	title = {Data Issues in High-Definition Maps Furniture – A Survey},
	url = {https://doi.org/10.1145/3627160},
	volume = {10},
	year = {2024},
}

@article{10.1145/3627987,
	abstract = {Human and natural processes such as navigation and natural calamities are intrinsically linked to the geographic space and described using place names. Extraction and subsequent geocoding of place names from text are critical for understanding the onset, progression, and end of these processes. Geocoding place names extracted from text requires using an external knowledge base such as a gazetteer. However, a standard gazetteer is typically incomplete. Additionally, widely used place name geocoding—also known as toponym resolution—approaches generally focus on geocoding ambiguous but known gazetteer place names. Hence, there is a need for an approach to automatically geocode non-gazetteer place names. In this research, we demonstrate that patterns in place names are not spatially random. Places are often named based on people, geography, and history of the area and thus exhibit a degree of similarity. Similarly, places that co-occur in text are likely to be spatially proximate as they provide geographic reference to common events. We propose a novel data-driven spatially-aware algorithm, Bhugol, that leverages the spatial patterns and the spatial context of place names to automatically geocode the non-gazetteer place names. The efficacy of Bhugol is demonstrated using two diverse geographic areas – USA and India. The results show that Bhugol outperforms well-known state-of-the-art geocoders.},
	address = {New York, NY, USA},
	articleno = {1},
	author = {Sharma, Praval and Samal, Ashok and Soh, Leen-Kiat and Joshi, Deepti},
	doi = {10.1145/3627987},
	issn = {2374-0353},
	issue_date = {March 2024},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {spatial place name pattern analysis, place name patterns, spatial-awareness, geocoding, Non-gazetteer place names},
	month = {dec},
	number = {1},
	numpages = {34},
	publisher = {Association for Computing Machinery},
	title = {A Spatially-Aware Data-Driven Approach to Automatically Geocoding Non-Gazetteer Place Names},
	url = {https://doi.org/10.1145/3627987},
	volume = {10},
	year = {2023},
}

@article{10.1145/3632404,
	abstract = {Three-dimensional (3D) maps with many millions to billions of points are now used in an increasing number of applications, with processing rates in the hundreds of thousands to millions of points per second. In mobile applications, power and energy consumption for managing such data and extracting useful information thereof are critical concerns. We have developed structures and methodologies with the purpose of minimizing memory usage and associated energy consumption for indexing and serialization of voxelized point-clouds. The primary source of points in our case is airborne laser scanning, but our methodology is not restricted to only this setting. Our emulated results show a memory usage reduction factor of roughly up to 200\texttimes{} that of Octree/Octomap and a file size reduction factor of up to 1.65\texttimes{} compared with the predominating compression scheme for airborne Lidar data, LASzip. In addition, our structures enable significantly more efficient processing since they are included in a hierarchical structure that captures geometric aspects.},
	address = {New York, NY, USA},
	articleno = {2},
	author = {Rifai, Mouad and Johnsson, Lennart},
	doi = {10.1145/3632404},
	issn = {2374-0353},
	issue_date = {March 2024},
	journal = {ACM Trans. Spatial Algorithms Syst.},
	keywords = {custom architecture, parametrizable representation of data, resource efficiency, 3D point-cloud, Voxel management system},
	month = {dec},
	number = {1},
	numpages = {34},
	publisher = {Association for Computing Machinery},
	title = {VxH: A Systematic Determination of Efficient Hierarchical Voxel Structures},
	url = {https://doi.org/10.1145/3632404},
	volume = {10},
	year = {2023},
}

@inproceedings{10.5555/3091125.3091449,
	abstract = {Environmental, regulatory and resource constraints affects the safety and efficiency of vessels navigating in and out of the ports. Movement of vessels under such constraints must be coordinated for improving safety and efficiency. Thus, we frame the vessel coordination problem as a multi-agent path-finding (MAPF) problem. We solve this MAPF problem using a Coordinated Path-Finding (CPF) algorithm. Based on the local search paradigm, the CPF algorithm improves on the aggregated path quality of the vessels iteratively. Outputs of the CPF algorithm are the coordinated trajectories. The Vessel Coordination Module (VCM) described here is the module encapsulating our MAPF-based approach for coordinating vessel traffic. Our demonstration of VCM is conducted using two maritime scenarios of vessel traffic at two geographical regions of Singapore Waters.},
	address = {Richland, SC},
	author = {Teng, Teck-Hou and Lau, Hoong Chuin and Kumar, Akshat},
	booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
	keywords = {vessel traffic management, multi-agent path finding, coordination},
	location = {S\~{a}o Paulo, Brazil},
	numpages = {3},
	pages = {1814–1816},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	series = {AAMAS '17},
	title = {A Multi-Agent System for Coordinating Vessel Traffic},
	year = {2017},
}

@inproceedings{10.5555/3091125.3091450,
	abstract = {Designing a building to be conducive to physical activity can significantly contribute to reducing sedentary behavior in a workplace. We aim to develop a game-based simulation platform that provides an intuitive, interactive tool to assess physical activity levels of an office building. The platform simulates workplace physical activity in a 3D virtual building. The assessors can freely manipulate spatial metrics characterizing the building's layouts and investigate their potential associations to physical activity levels. The evaluation may facilitate stakeholders to assess a building plan on public health awareness. One of the key challenges in developing such a platform lies in simulating realistic personal behavior in a complex interaction environment. In this paper, we present our initial development of the simulation platform where we focus on tackling the challenge of incorporating intention recognition in human interactions, demonstrating how this incorporation realistically affects their behaviours. Furthermore, we demonstrate the utility of collective intention recognition techniques in improving recognition reliability as well as efficiency of the simulation platform.},
	address = {Richland, SC},
	author = {Zeng, Yifeng and Zhang, Zhang and Han, The Anh and Spears, Iain R. and Qin, Shengchao},
	booktitle = {Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems},
	keywords = {simulation, intention recognition, computer games},
	location = {S\~{a}o Paulo, Brazil},
	numpages = {3},
	pages = {1817–1819},
	publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	series = {AAMAS '17},
	title = {Using Intention Recognition in a Simulation Platform to Assess Physical Activity Levels of an Office Building},
	year = {2017},
}
